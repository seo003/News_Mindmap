# -*- coding: utf-8 -*-

import logging
import time
import numpy as np
from collections import defaultdict, Counter
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer
from database.news_fetcher import fetch_news_from_db
from analysis.news_analyzer import NewsAnalyzer

# ë¡œê¹… ì„¤ì •
def setup_logging():
    """ë¡œê¹… ì„¤ì • - ì½˜ì†”ê³¼ íŒŒì¼ ëª¨ë‘ì— ì¶œë ¥"""
    import os
    from datetime import datetime
    
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "logs")
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%m-%d_%H%M")
    log_file = os.path.join(log_dir, f"accuracy_{timestamp}.log")
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),  # ì½˜ì†” ì¶œë ¥
            logging.FileHandler(log_file, encoding='utf-8')  # íŒŒì¼ ì¶œë ¥
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {log_file}")
    return logger

logger = setup_logging()


class AccuracyEvaluator:
    """
    ë‰´ìŠ¤ ë¶„ì„ ì •í™•ë„ í‰ê°€ í´ë˜ìŠ¤
    
    í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ, í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„, ëŒ€í•™êµ ë¶„ë¥˜ ì •í™•ë„ ë“±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """ì •í™•ë„ í‰ê°€ê¸° ì´ˆê¸°í™”"""
        logger.info("ğŸ”§ AccuracyEvaluator ì´ˆê¸°í™” ì‹œì‘...")
        
        # NewsAnalyzer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì´ë¯¸ ëª¨ë¸ì´ ë¡œë”©ë˜ì–´ ìˆìŒ)
        self.news_analyzer = NewsAnalyzer()
        
        # NewsAnalyzerì˜ ëª¨ë¸ì„ ì¬ì‚¬ìš© (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
        self.embedding_model = self.news_analyzer.embedding_model
        self.keybert_model = self.news_analyzer.kw_model  # KeyBERT ëª¨ë¸ ì¬ì‚¬ìš©
        logger.info("âœ… SentenceTransformer ëª¨ë¸ ì¬ì‚¬ìš© (NewsAnalyzerì—ì„œ ë¡œë”©ëœ ëª¨ë¸)")
        logger.info("âœ… KeyBERT ëª¨ë¸ ì¬ì‚¬ìš© (NewsAnalyzerì—ì„œ ë¡œë”©ëœ ëª¨ë¸)")
        
        # í‰ê°€ìš© ê¸°ì¤€ ë°ì´í„°
        self.university_keywords = {
            "ì¸í•˜ê³µì „", "ì¸í•˜ëŒ€", "í•­ê³µëŒ€", "KAIST", "ì„œìš¸ëŒ€", "ì—°ì„¸ëŒ€", "ê³ ë ¤ëŒ€",
            "ì„±ê· ê´€ëŒ€", "í•œì–‘ëŒ€", "ì¤‘ì•™ëŒ€", "ê²½í¬ëŒ€", "ë™êµ­ëŒ€", "í™ìµëŒ€", "êµ­ë¯¼ëŒ€"
        }
        
        self.category_keywords = {
            "ì •ì¹˜": ["ëŒ€í†µë ¹", "ì •ë¶€", "êµ­íšŒ", "ì •ì¹˜", "ì„ ê±°", "ì—¬ì•¼", "ì •ì±…", "êµ­ì •", "ì •ë‹¹"],
            "ê²½ì œ": ["ê²½ì œ", "íˆ¬ì", "ê¸°ì—…", "ê¸ˆìœµ", "ì£¼ì‹", "ì‹œì¥", "ìˆ˜ì¶œ", "ìˆ˜ì…", "GDP", "ê¸ˆë¦¬"],
            "ì‚¬íšŒ": ["ì‚¬íšŒ", "êµìœ¡", "ë³µì§€", "ë³´ê±´", "í™˜ê²½", "êµí†µ", "ì£¼íƒ", "ë…¸ë™", "ê³ ìš©"],
            "êµ­ì œ": ["êµ­ì œ", "ì™¸êµ", "ë¯¸êµ­", "ì¤‘êµ­", "ì¼ë³¸", "ëŸ¬ì‹œì•„", "ìœ ëŸ½", "íŠ¸ëŸ¼í”„", "í‘¸í‹´"],
            "ë²•ë¬´": ["ë²•ë¬´", "ë²•ì›", "ê²€ì°°", "ê²½ì°°", "ì¬íŒ", "í˜•ì‚¬", "ë¯¼ì‚¬", "ë²•ë¥ ", "ì‚¬ë²•"],
            "ë¬¸í™”": ["ë¬¸í™”", "ì˜ˆìˆ ", "ìŠ¤í¬ì¸ ", "ì—°ì˜ˆ", "ì˜í™”", "ìŒì•…", "ì¶•ì œ", "ì „ì‹œ"],
            "ê¸°ìˆ ": ["ê¸°ìˆ ", "AI", "ì¸ê³µì§€ëŠ¥", "ë””ì§€í„¸", "ìŠ¤ë§ˆíŠ¸", "IT", "ì†Œí”„íŠ¸ì›¨ì–´", "í•˜ë“œì›¨ì–´"],
            "êµìœ¡": ["êµìœ¡", "ëŒ€í•™", "í•™êµ", "í•™ìƒ", "êµìˆ˜", "ì—°êµ¬", "í•™ìˆ ", "ì…ì‹œ"],
            "ì˜ë£Œ": ["ì˜ë£Œ", "ë³‘ì›", "ì˜ì‚¬", "ì¹˜ë£Œ", "ê±´ê°•", "ì§ˆë³‘", "ì˜ì•½", "ë³´ê±´"],
            "í™˜ê²½": ["í™˜ê²½", "ê¸°í›„", "ì—ë„ˆì§€", "ì¬ìƒ", "ì¹œí™˜ê²½", "ëŒ€ê¸°", "ìˆ˜ì§ˆ", "íê¸°ë¬¼"]
        }
        
        logger.info(f"ğŸ“š í‰ê°€ ê¸°ì¤€ ë°ì´í„° ì„¤ì • ì™„ë£Œ (ëŒ€í•™êµ: {len(self.university_keywords)}ê°œ, ì¹´í…Œê³ ë¦¬: {len(self.category_keywords)}ê°œ)")
        logger.info("ğŸ‰ AccuracyEvaluator ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def evaluate_clustering_quality(self, news_data, limit=1000, embeddings=None, clusterer=None):
        """
        í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€
        
        Args:
            news_data (list): ë‰´ìŠ¤ ë°ì´í„°
            limit (int): ë¶„ì„í•  ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜
            embeddings: ë¯¸ë¦¬ ìƒì„±ëœ ì„ë² ë”© (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
            
        Returns:
            dict: í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ ì§€í‘œ
        """
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ğŸ“° ì…ë ¥ ë‰´ìŠ¤ ë°ì´í„°: {len(news_data)}ê°œ")
            logger.info(f"ğŸ”¢ ë¶„ì„ ì œí•œ: {limit}ê°œ")
            
            # ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
            logger.info("ğŸ“ ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ ì¤‘...")
            titles = [item["title"] for item in news_data[:limit]]
            logger.info(f"âœ… ì œëª© ì¶”ì¶œ ì™„ë£Œ: {len(titles)}ê°œ")
            
            if len(titles) < 10:
                logger.warning(f"âš ï¸ ë¶„ì„í•  ë‰´ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(titles)}ê°œ (ìµœì†Œ 10ê°œ í•„ìš”)")
                return {"error": "ë¶„ì„í•  ë‰´ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 10ê°œ í•„ìš”)"}
            
            # ì„ë² ë”© ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
            if embeddings is None:
                logger.info("ğŸ¤– SentenceTransformerë¡œ ì„ë² ë”© ìƒì„± ì¤‘...")
                start_time = time.time()
                embeddings = self.embedding_model.encode(titles, normalize_embeddings=True)
                embedding_time = time.time() - start_time
                logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape} (ì†Œìš”ì‹œê°„: {embedding_time:.2f}ì´ˆ)")
            else:
                logger.info(f"â™»ï¸ ê¸°ì¡´ ì„ë² ë”© ì¬ì‚¬ìš©: {embeddings.shape}")
            
            # K-Means í´ëŸ¬ìŠ¤í„°ë§ (ì¤‘ë³µ í´ëŸ¬ìŠ¤í„°ë§ ë°©ì§€ë¥¼ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬)
            # logger.info("ğŸ” í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
            # from sklearn.cluster import KMeans
            
            # í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚° (NewsAnalyzerì™€ ë™ì¼í•œ ë°©ì‹) - ì£¼ì„ ì²˜ë¦¬
            # n_clusters = self.news_analyzer.calculate_kmeans_clusters(len(titles))
            
            # if n_clusters < 2:
            #     logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {n_clusters}ê°œ (ìµœì†Œ 2ê°œ í•„ìš”)")
            #     return {"error": "í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 2ê°œ í•„ìš”)"}
            
            # kmeans = KMeans(
            #     n_clusters=n_clusters, 
            #     random_state=self.news_analyzer.KMEANS_RANDOM_STATE, 
            #     n_init=self.news_analyzer.KMEANS_N_INIT
            # )
            
            # clustering_start = time.time()
            # cluster_labels = kmeans.fit_predict(embeddings)
            # clustering_time = time.time() - clustering_start
            # logger.info(f"âœ… K-Means í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {clustering_time:.2f}ì´ˆ)")
            
            # NewsAnalyzer ë¶„ì„ ê²°ê³¼ì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì •ë³´ ì¶”ì¶œ
            logger.info("â™»ï¸ NewsAnalyzer ë¶„ì„ ê²°ê³¼ì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì •ë³´ ì¶”ì¶œ...")
            
            # í’ˆì§ˆ ì§€í‘œ ì´ˆê¸°í™”
            quality_metrics = {}
            
            # ì„ íƒí•œ í´ëŸ¬ìŠ¤í„°ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰ (ì„ë² ë”© ì¬ì‚¬ìš©)
            if clusterer is None:
                clusterer = self.news_analyzer
            
            analysis_start_time = time.time()
            try:
                logger.info("â™»ï¸ ê¸°ì¡´ ì„ë² ë”©ì„ ì¬ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ë¶„ì„ ìˆ˜í–‰...")
                # í´ëŸ¬ìŠ¤í„°ëŸ¬ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ë©”ì„œë“œ í˜¸ì¶œ
                if hasattr(clusterer, 'analyze_from_db'):
                    # NewsAnalyzer
                    analysis_result = clusterer.analyze_from_db(news_data[:limit], embeddings)
                elif hasattr(clusterer, 'analyze_news'):
                    # SimpleClusterer, TfidfClusterer, FastTextClusterer
                    analysis_result = clusterer.analyze_news(news_data[:limit])
                else:
                    logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í´ëŸ¬ìŠ¤í„°ëŸ¬ íƒ€ì…: {type(clusterer)}")
                    return {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í´ëŸ¬ìŠ¤í„°ëŸ¬ íƒ€ì…: {type(clusterer)}"}
                analysis_time = time.time() - analysis_start_time
                if analysis_result:
                    logger.info(f"âœ… ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {analysis_time:.2f}ì´ˆ) - ì„ë² ë”© ì¬ì‚¬ìš©ìœ¼ë¡œ ì‹œê°„ ë‹¨ì¶•")
                    # ë¶„ì„ ì‹œê°„ì„ ë©”íŠ¸ë¦­ì— ì¶”ê°€
                    quality_metrics["analysis_time"] = analysis_time
                else:
                    logger.warning("âš ï¸ ë‰´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨")
            except Exception as e:
                logger.error(f"âŒ ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ë¶„ì„ ê²°ê³¼ì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ í†µê³„ ì¶”ì¶œ
            if analysis_result:
                n_clusters = len(analysis_result)
                n_noise = 0  # NewsAnalyzerëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹
                
                # ì‹¤ì œë¡œ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹ëœ ë‰´ìŠ¤ ìˆ˜ ê³„ì‚°
                clustered_news_count = 0
                for major_category in analysis_result:
                    middle_keywords = major_category.get('middleKeywords', [])
                    other_news = major_category.get('otherNews', [])
                    # ì¤‘ë¶„ë¥˜ì— í¬í•¨ëœ ë‰´ìŠ¤ ìˆ˜
                    for middle_cat in middle_keywords:
                        related_news = middle_cat.get('relatedNews', [])
                        clustered_news_count += len(related_news)
                    # ê¸°íƒ€ ë‰´ìŠ¤ ìˆ˜
                    clustered_news_count += len(other_news)
                
                logger.info(f"ğŸ”¢ í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}ê°œ")
                logger.info(f"ğŸ”‡ ë…¸ì´ì¦ˆ ìˆ˜: {n_noise}ê°œ")
                logger.info(f"ğŸ“ˆ ë…¸ì´ì¦ˆ ë¹„ìœ¨: {(n_noise / len(titles) * 100):.1f}%")
                logger.info(f"ğŸ“Š ì‹¤ì œ í´ëŸ¬ìŠ¤í„°ëœ ë‰´ìŠ¤: {clustered_news_count}ê°œ")
                
                quality_metrics.update({
                    "total_news": len(titles),
                    "n_clusters": n_clusters,
                    "n_noise": n_noise,
                    "noise_ratio": n_noise / len(titles) if len(titles) > 0 else 0,
                    "avg_cluster_size": clustered_news_count / n_clusters if n_clusters > 0 else 0
                })
                
                logger.info(f"ğŸ“Š í‰ê·  í´ëŸ¬ìŠ¤í„° í¬ê¸°: {quality_metrics['avg_cluster_size']:.1f}")
                
                # ì‹¤ë£¨ì—£ ì ìˆ˜ëŠ” NewsAnalyzerì˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
                if n_clusters > 1:
                    logger.info("ğŸ“ ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚° ì¤‘...")
                    try:
                        # ë¶„ì„ ê²°ê³¼ì—ì„œ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì¬êµ¬ì„± (ê¸°íƒ€ ë‰´ìŠ¤ë§Œ ëŒ€ìƒ)
                        cluster_labels = []
                        total_news_count = 0
                        
                        # NewsAnalyzerê°€ ì²˜ë¦¬í•œ ê¸°íƒ€ ë‰´ìŠ¤ ìˆ˜ ê³„ì‚°
                        other_news_count = 0
                        for major_category in analysis_result:
                            middle_keywords = major_category.get('middleKeywords', [])
                            other_news = major_category.get('otherNews', [])
                            other_news_count += len(other_news)
                            for middle_cat in middle_keywords:
                                related_news = middle_cat.get('relatedNews', [])
                                other_news_count += len(related_news)
                        
                        logger.info(f"ğŸ” í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì¬êµ¬ì„± ì‹œì‘: ì „ì²´ {len(titles)}ê°œ ì œëª© ì¤‘ ê¸°íƒ€ ë‰´ìŠ¤ {other_news_count}ê°œ")
                        
                        for i, major_category in enumerate(analysis_result):
                            major_name = major_category.get('majorKeyword', f'cluster_{i}')
                            middle_keywords = major_category.get('middleKeywords', [])
                            other_news = major_category.get('otherNews', [])
                            
                            logger.info(f"   ëŒ€ë¶„ë¥˜ {i}: {major_name} (ì¤‘ë¶„ë¥˜ {len(middle_keywords)}ê°œ, ê¸°íƒ€ {len(other_news)}ê°œ)")
                            
                            # ì¤‘ë¶„ë¥˜ë³„ë¡œ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ í• ë‹¹
                            for j, middle_cat in enumerate(middle_keywords):
                                related_news = middle_cat.get('relatedNews', [])
                                cluster_labels.extend([i] * len(related_news))
                                total_news_count += len(related_news)
                                logger.info(f"     ì¤‘ë¶„ë¥˜ {j}: {len(related_news)}ê°œ ë‰´ìŠ¤")
                            
                            # ê¸°íƒ€ ë‰´ìŠ¤ë„ ê°™ì€ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹
                            cluster_labels.extend([i] * len(other_news))
                            total_news_count += len(other_news)
                        
                        logger.info(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì¬êµ¬ì„± ì™„ë£Œ: {len(cluster_labels)}ê°œ ë¼ë²¨, {total_news_count}ê°œ ë‰´ìŠ¤")
                        logger.info(f"ğŸ“Š ì„ë² ë”© ìˆ˜: {len(embeddings)}ê°œ")
                        logger.info(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë¶„í¬: {dict(zip(*np.unique(cluster_labels, return_counts=True)))}")
                        
                        # ê¸°íƒ€ ë‰´ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ì„ë² ë”©ë§Œ ì‚¬ìš© (ëŒ€í•™êµ ë‰´ìŠ¤ ì œì™¸)
                        # NewsAnalyzerëŠ” ëŒ€í•™êµ ë‰´ìŠ¤ë¥¼ ë¨¼ì € ë¶„ë¦¬í•˜ë¯€ë¡œ, ê¸°íƒ€ ë‰´ìŠ¤ëŠ” ë’¤ìª½ì— ìœ„ì¹˜
                        university_news_count = len(titles) - other_news_count
                        other_embeddings = embeddings[university_news_count:]
                        
                        logger.info(f"ğŸ“Š ëŒ€í•™êµ ë‰´ìŠ¤: {university_news_count}ê°œ, ê¸°íƒ€ ë‰´ìŠ¤: {len(other_embeddings)}ê°œ")
                        logger.info(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(set(cluster_labels))}ê°œ")
                        
                        if len(cluster_labels) == len(other_embeddings) and len(set(cluster_labels)) > 1:
                            from sklearn.metrics import silhouette_score
                            silhouette_avg = silhouette_score(other_embeddings, cluster_labels)
                            quality_metrics["silhouette_score"] = silhouette_avg
                            logger.info(f"âœ… ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_avg:.4f}")
                        else:
                            logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ê³¼ ê¸°íƒ€ ë‰´ìŠ¤ ì„ë² ë”© ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ: {len(cluster_labels)} vs {len(other_embeddings)}")
                            logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(set(cluster_labels))}ê°œ")
                            quality_metrics["silhouette_score"] = None
                    except Exception as e:
                        logger.warning(f"âš ï¸ ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
                        quality_metrics["silhouette_score"] = None
                else:
                    logger.warning("âš ï¸ í´ëŸ¬ìŠ¤í„°ê°€ 1ê°œ ì´í•˜ë¡œ ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚° ë¶ˆê°€")
                    quality_metrics["silhouette_score"] = None
                
                # Davies-Bouldin Index ê³„ì‚°
                if n_clusters > 1 and len(cluster_labels) == len(other_embeddings):
                    logger.info("ğŸ“ Davies-Bouldin Index ê³„ì‚° ì¤‘...")
                    try:
                        db_index = davies_bouldin_score(other_embeddings, cluster_labels)
                        quality_metrics["davies_bouldin_index"] = db_index
                        logger.info(f"âœ… Davies-Bouldin Index: {db_index:.4f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Davies-Bouldin Index ê³„ì‚° ì‹¤íŒ¨: {e}")
                        quality_metrics["davies_bouldin_index"] = None
                else:
                    logger.warning("âš ï¸ í´ëŸ¬ìŠ¤í„°ê°€ 1ê°œ ì´í•˜ì´ê±°ë‚˜ ë°ì´í„° ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•Šì•„ Davies-Bouldin Index ê³„ì‚° ë¶ˆê°€")
                    quality_metrics["davies_bouldin_index"] = None
                
                # Calinski-Harabasz Index ê³„ì‚° (ì¶”ê°€ëœ ë‚´ë¶€ í‰ê°€ ì§€í‘œ)
                if n_clusters > 1 and len(cluster_labels) == len(other_embeddings):
                    logger.info("ğŸ“ Calinski-Harabasz Index ê³„ì‚° ì¤‘...")
                    try:
                        ch_index = calinski_harabasz_score(other_embeddings, cluster_labels)
                        quality_metrics["calinski_harabasz_index"] = ch_index
                        logger.info(f"âœ… Calinski-Harabasz Index: {ch_index:.4f} (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Calinski-Harabasz Index ê³„ì‚° ì‹¤íŒ¨: {e}")
                        quality_metrics["calinski_harabasz_index"] = None
                else:
                    logger.warning("âš ï¸ í´ëŸ¬ìŠ¤í„°ê°€ 1ê°œ ì´í•˜ì´ê±°ë‚˜ ë°ì´í„° ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•Šì•„ Calinski-Harabasz Index ê³„ì‚° ë¶ˆê°€")
                    quality_metrics["calinski_harabasz_index"] = None
                
                # í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„
                logger.info("ğŸ“‹ í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„ ìƒì„± ì¤‘...")
                cluster_stats = {}
                for i, major_category in enumerate(analysis_result):
                    major_name = major_category.get('majorKeyword', f'cluster_{i}')
                    middle_keywords = major_category.get('middleKeywords', [])
                    other_news = major_category.get('otherNews', [])
                    
                    cluster_size = len(other_news) + sum(len(middle.get('relatedNews', [])) for middle in middle_keywords)
                    
                    cluster_stats[f"cluster_{i}"] = {
                        "name": major_name,
                        "size": cluster_size,
                        "middle_categories": len(middle_keywords)
                    }
                
                quality_metrics["cluster_details"] = cluster_stats
                quality_metrics["analysis_result"] = analysis_result  # ë¶„ì„ ê²°ê³¼ë„ í•¨ê»˜ ë°˜í™˜
            else:
                logger.error("âŒ ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {"error": "ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
            
            logger.info("ğŸ‰ í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ ì™„ë£Œ!")
            logger.info("=" * 60)
            return quality_metrics
            
        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def evaluate_keyword_extraction(self, news_data, limit=1000, embeddings=None, clusterer=None):
        """
        í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€
        
        Args:
            news_data (list): ë‰´ìŠ¤ ë°ì´í„°
            limit (int): ë¶„ì„í•  ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜
            embeddings: ë¯¸ë¦¬ ìƒì„±ëœ ì„ë² ë”© (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
            
        Returns:
            dict: í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ ì§€í‘œ
        """
        try:
            logger.info("=" * 60)
            logger.info("ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€ ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ğŸ“° ì…ë ¥ ë‰´ìŠ¤ ë°ì´í„°: {len(news_data)}ê°œ")
            logger.info(f"ğŸ”¢ ë¶„ì„ ì œí•œ: {limit}ê°œ")
            
            # ì‚¬ìš©í•  í´ëŸ¬ìŠ¤í„°ëŸ¬ ê²°ì •
            if clusterer is None:
                clusterer = self.news_analyzer
            
            # ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰
            # ì „ì²˜ë¦¬ëŠ” NewsAnalyzerì˜ ë©”ì„œë“œë¥¼ ì‚¬ìš© (ëª¨ë“  í´ëŸ¬ìŠ¤í„°ëŸ¬ê°€ ë™ì¼í•œ ì „ì²˜ë¦¬ ë¡œì§ ì‚¬ìš©)
            logger.info("ğŸ“Š ë‰´ìŠ¤ ì „ì²˜ë¦¬ ì¤‘...")
            if hasattr(clusterer, 'preprocess_titles'):
                processed_data = clusterer.preprocess_titles(news_data[:limit])
            elif hasattr(self.news_analyzer, 'preprocess_titles'):
                processed_data = self.news_analyzer.preprocess_titles(news_data[:limit])
            else:
                processed_data = news_data[:limit]
            logger.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_data)}ê°œ")
            
            if len(processed_data) < 5:
                logger.warning(f"âš ï¸ ë¶„ì„í•  ë‰´ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(processed_data)}ê°œ (ìµœì†Œ 5ê°œ í•„ìš”)")
                return {"error": "ë¶„ì„í•  ë‰´ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 5ê°œ í•„ìš”)"}
            
            # ëŒ€í•™êµë³„ ë¶„ë¥˜ (NewsAnalyzerì˜ ë©”ì„œë“œ ì‚¬ìš©)
            logger.info("ğŸ« ëŒ€í•™êµë³„ ë‰´ìŠ¤ ë¶„ë¥˜ ì¤‘...")
            if hasattr(clusterer, 'split_news_by_uni_name'):
                university_news, other_news = clusterer.split_news_by_uni_name(processed_data)
            elif hasattr(self.news_analyzer, 'split_news_by_uni_name'):
                university_news, other_news = self.news_analyzer.split_news_by_uni_name(processed_data)
            else:
                # ëŒ€í•™êµ ë¶„ë¥˜ê°€ ì—†ëŠ” ê²½ìš° ëª¨ë“  ë‰´ìŠ¤ë¥¼ ê¸°íƒ€ ë‰´ìŠ¤ë¡œ ì²˜ë¦¬
                university_news = {}
                other_news = processed_data
            logger.info(f"âœ… ëŒ€í•™êµ ë‰´ìŠ¤: {len(university_news)}ê°œ ê·¸ë£¹")
            logger.info(f"âœ… ê¸°íƒ€ ë‰´ìŠ¤: {len(other_news)}ê°œ")
            
            # í´ëŸ¬ìŠ¤í„°ë§ - ì„ íƒí•œ í´ëŸ¬ìŠ¤í„°ëŸ¬ë¡œ ì „ì²´ ë¶„ì„ ìˆ˜í–‰
            if other_news:
                logger.info("ğŸ” ì„ íƒí•œ í´ëŸ¬ìŠ¤í„°ëŸ¬ë¡œ ì „ì²´ ë¶„ì„ ìˆ˜í–‰...")
                # ì „ì²´ ë‰´ìŠ¤ ë°ì´í„°ë¡œ ë¶„ì„ ìˆ˜í–‰ (í´ëŸ¬ìŠ¤í„°ëŸ¬ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ë©”ì„œë“œ í˜¸ì¶œ)
                if embeddings is not None and hasattr(clusterer, 'analyze_from_db'):
                    logger.info("â™»ï¸ ê¸°ì¡´ ì„ë² ë”©ì„ ì¬ì‚¬ìš©í•˜ì—¬ ë¶„ì„...")
                    analysis_result = clusterer.analyze_from_db(news_data[:limit], embeddings)
                elif hasattr(clusterer, 'analyze_news'):
                    logger.info("â™»ï¸ ì„ íƒí•œ í´ëŸ¬ìŠ¤í„°ëŸ¬ë¡œ ë¶„ì„...")
                    analysis_result = clusterer.analyze_news(news_data[:limit])
                else:
                    logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í´ëŸ¬ìŠ¤í„°ëŸ¬ íƒ€ì…: {type(clusterer)}")
                    return {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í´ëŸ¬ìŠ¤í„°ëŸ¬ íƒ€ì…: {type(clusterer)}"}
                
                # ë¶„ì„ ê²°ê³¼ì—ì„œ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ì¶œ
                if analysis_result:
                    clusters = {}
                    noise_news = []
                    # ë¶„ì„ ê²°ê³¼ë¥¼ í´ëŸ¬ìŠ¤í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    for major_category in analysis_result:
                        middle_keywords = major_category.get('middleKeywords', [])
                        other_news_list = major_category.get('otherNews', [])
                        # ì¤‘ë¶„ë¥˜ë¥¼ í´ëŸ¬ìŠ¤í„°ë¡œ ë³€í™˜
                        for middle_cat in middle_keywords:
                            related_news = middle_cat.get('relatedNews', [])
                            if related_news:
                                cluster_id = len(clusters)
                                clusters[cluster_id] = related_news
                        # ê¸°íƒ€ ë‰´ìŠ¤ë„ í´ëŸ¬ìŠ¤í„°ë¡œ ì¶”ê°€
                        if other_news_list:
                            cluster_id = len(clusters)
                            clusters[cluster_id] = other_news_list
                    logger.info(f"âœ… í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(clusters)}ê°œ")
                    logger.info(f"âœ… ë…¸ì´ì¦ˆ ë‰´ìŠ¤: {len(noise_news)}ê°œ")
                else:
                    clusters, noise_news = {}, []
            else:
                logger.info("â„¹ï¸ ê¸°íƒ€ ë‰´ìŠ¤ê°€ ì—†ì–´ í´ëŸ¬ìŠ¤í„°ë§ ê±´ë„ˆëœ€")
                clusters, noise_news = {}, []
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€
            logger.info("ğŸ“Š í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ ì§€í‘œ ê³„ì‚° ì¤‘...")
            keyword_metrics = {
                "total_processed_news": len(processed_data),
                "university_news_count": len(university_news),
                "clustered_news_count": sum(len(cluster) for cluster in clusters.values()),
                "noise_news_count": len(noise_news)
            }
            
            logger.info(f"ğŸ“ˆ ì²˜ë¦¬ëœ ë‰´ìŠ¤: {keyword_metrics['total_processed_news']}ê°œ")
            logger.info(f"ğŸ« ëŒ€í•™êµ ë‰´ìŠ¤: {keyword_metrics['university_news_count']}ê°œ")
            logger.info(f"ğŸ” í´ëŸ¬ìŠ¤í„°ëœ ë‰´ìŠ¤: {keyword_metrics['clustered_news_count']}ê°œ")
            logger.info(f"ğŸ”‡ ë…¸ì´ì¦ˆ ë‰´ìŠ¤: {keyword_metrics['noise_news_count']}ê°œ")
            
            # ëŒ€í•™êµ í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„
            logger.info("ğŸ« ëŒ€í•™êµ í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€ ì¤‘...")
            university_accuracy = self._evaluate_university_keywords(university_news)
            keyword_metrics.update(university_accuracy)
            logger.info(f"âœ… ëŒ€í•™êµ í‚¤ì›Œë“œ ì •í™•ë„: {university_accuracy.get('university_keyword_accuracy', 0):.1%}")
            
            # í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„
            logger.info("ğŸ” í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€ ì¤‘...")
            cluster_accuracy = self._evaluate_cluster_keywords(clusters)
            keyword_metrics.update(cluster_accuracy)
            logger.info(f"âœ… í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì •í™•ë„: {cluster_accuracy.get('cluster_keyword_accuracy', 0):.1%}")
            
            logger.info("ğŸ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€ ì™„ë£Œ!")
            logger.info("=" * 60)
            return keyword_metrics
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def evaluate_topic_consistency(self, news_data, analysis_result, limit=1000):
        """
        Topic Consistency í‰ê°€ (ChatGPT ì œì•ˆ)
        
        ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ í‚¤ì›Œë“œì™€ ë‰´ìŠ¤ ë³¸ë¬¸ KeyBERT í‚¤ì›Œë“œ ê°„ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
        
        Args:
            news_data (list): ë‰´ìŠ¤ ë°ì´í„°
            analysis_result (list): ë¶„ì„ ê²°ê³¼
            limit (int): ë¶„ì„í•  ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜
            
        Returns:
            dict: Topic Consistency ì§€í‘œ
        """
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“Š Topic Consistency í‰ê°€ ì‹œì‘")
            logger.info("=" * 60)
            
            if not analysis_result:
                logger.warning("âš ï¸ ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ Topic Consistency í‰ê°€ ë¶ˆê°€")
                return {"error": "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
            
            consistency_scores = []
            cluster_details = []
            
            # ê° í´ëŸ¬ìŠ¤í„°ë³„ë¡œ í‰ê°€
            for major_idx, major_category in enumerate(analysis_result):
                major_keyword = major_category.get('majorKeyword', '')
                middle_keywords = major_category.get('middleKeywords', [])
                other_news = major_category.get('otherNews', [])
                
                # ì¤‘ë¶„ë¥˜ë³„ë¡œ í‰ê°€
                for middle_cat in middle_keywords:
                    middle_keyword = middle_cat.get('middleKeyword', '')
                    related_news = middle_cat.get('relatedNews', [])
                    
                    if not related_news:
                        continue
                    
                    # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ í‚¤ì›Œë“œ (ì¤‘ë¶„ë¥˜ í‚¤ì›Œë“œ ì‚¬ìš©)
                    cluster_keyword = middle_keyword if middle_keyword else major_keyword
                    
                    # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘
                    cluster_texts = []
                    for news in related_news:
                        # ì›ë³¸ ë‰´ìŠ¤ ë°ì´í„°ì—ì„œ ë³¸ë¬¸ ì°¾ê¸°
                        news_id = news.get('id') or news.get('title', '')
                        original_news = next(
                            (item for item in news_data[:limit] 
                             if item.get('id') == news_id or item.get('title') == news.get('title', '')),
                            None
                        )
                        if original_news:
                            # ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì œëª© ì‚¬ìš©
                            text = original_news.get('content', '') or original_news.get('title', '')
                            if text:
                                cluster_texts.append(text)
                    
                    if not cluster_texts:
                        continue
                    
                    # KeyBERTë¡œ í´ëŸ¬ìŠ¤í„° ì „ì²´ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
                    combined_text = ' '.join(cluster_texts[:20])  # ìµœëŒ€ 20ê°œ ë‰´ìŠ¤ë§Œ ì‚¬ìš©
                    try:
                        keybert_keywords = self.keybert_model.extract_keywords(
                            combined_text,
                            keyphrase_ngram_range=(1, 3),
                            top_n=5,
                            use_mmr=True,
                            diversity=0.5
                        )
                        keybert_keyword_list = [kw for kw, score in keybert_keywords]
                    except Exception as e:
                        logger.warning(f"âš ï¸ KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        keybert_keyword_list = []
                    
                    # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ í‚¤ì›Œë“œì™€ KeyBERT í‚¤ì›Œë“œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
                    if keybert_keyword_list and cluster_keyword:
                        try:
                            # í‚¤ì›Œë“œë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
                            keywords_to_compare = [cluster_keyword] + keybert_keyword_list[:3]  # ìƒìœ„ 3ê°œë§Œ
                            keyword_embeddings = self.embedding_model.encode(
                                keywords_to_compare, 
                                normalize_embeddings=True
                            )
                            
                            # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ í‚¤ì›Œë“œì™€ KeyBERT í‚¤ì›Œë“œë“¤ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                            cluster_keyword_emb = keyword_embeddings[0]
                            keybert_embs = keyword_embeddings[1:]
                            
                            similarities = cosine_similarity(
                                [cluster_keyword_emb], 
                                keybert_embs
                            )[0]
                            
                            avg_similarity = float(np.mean(similarities)) if len(similarities) > 0 else 0.0
                            consistency_scores.append(avg_similarity)
                            
                            cluster_details.append({
                                "cluster_id": f"major_{major_idx}_middle_{len(cluster_details)}",
                                "major_keyword": major_keyword,
                                "middle_keyword": middle_keyword,
                                "cluster_keyword": cluster_keyword,
                                "keybert_keywords": keybert_keyword_list[:3],
                                "similarity": avg_similarity,
                                "news_count": len(related_news)
                            })
                            
                        except Exception as e:
                            logger.warning(f"âš ï¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                            continue
                
                # ê¸°íƒ€ ë‰´ìŠ¤ë„ í‰ê°€
                if other_news and major_keyword:
                    # ê¸°íƒ€ ë‰´ìŠ¤ì˜ ë³¸ë¬¸ ìˆ˜ì§‘
                    other_texts = []
                    for news in other_news[:10]:  # ìµœëŒ€ 10ê°œë§Œ
                        news_id = news.get('id') or news.get('title', '')
                        original_news = next(
                            (item for item in news_data[:limit] 
                             if item.get('id') == news_id or item.get('title') == news.get('title', '')),
                            None
                        )
                        if original_news:
                            text = original_news.get('content', '') or original_news.get('title', '')
                            if text:
                                other_texts.append(text)
                    
                    if other_texts:
                        combined_text = ' '.join(other_texts)
                        try:
                            keybert_keywords = self.keybert_model.extract_keywords(
                                combined_text,
                                keyphrase_ngram_range=(1, 3),
                                top_n=5,
                                use_mmr=True,
                                diversity=0.5
                            )
                            keybert_keyword_list = [kw for kw, score in keybert_keywords]
                        except Exception as e:
                            logger.warning(f"âš ï¸ KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                            keybert_keyword_list = []
                        
                        if keybert_keyword_list and major_keyword:
                            try:
                                keywords_to_compare = [major_keyword] + keybert_keyword_list[:3]
                                keyword_embeddings = self.embedding_model.encode(
                                    keywords_to_compare,
                                    normalize_embeddings=True
                                )
                                
                                cluster_keyword_emb = keyword_embeddings[0]
                                keybert_embs = keyword_embeddings[1:]
                                
                                similarities = cosine_similarity(
                                    [cluster_keyword_emb],
                                    keybert_embs
                                )[0]
                                
                                avg_similarity = float(np.mean(similarities)) if len(similarities) > 0 else 0.0
                                consistency_scores.append(avg_similarity)
                                
                                cluster_details.append({
                                    "cluster_id": f"major_{major_idx}_other",
                                    "major_keyword": major_keyword,
                                    "middle_keyword": "",
                                    "cluster_keyword": major_keyword,
                                    "keybert_keywords": keybert_keyword_list[:3],
                                    "similarity": avg_similarity,
                                    "news_count": len(other_news)
                                })
                            except Exception as e:
                                logger.warning(f"âš ï¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            
            # ì „ì²´ í‰ê·  ê³„ì‚°
            avg_consistency = float(np.mean(consistency_scores)) if consistency_scores else 0.0
            
            logger.info(f"âœ… Topic Consistency í‰ê°€ ì™„ë£Œ")
            logger.info(f"ğŸ“Š í‰ê°€ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(consistency_scores)}ê°œ")
            logger.info(f"ğŸ“Š í‰ê·  Topic Consistency: {avg_consistency:.4f}")
            logger.info("=" * 60)
            
            return {
                "topic_consistency_score": avg_consistency,
                "evaluated_clusters": len(consistency_scores),
                "cluster_details": cluster_details,
                "all_scores": consistency_scores
            }
            
        except Exception as e:
            logger.error(f"Topic Consistency í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def _evaluate_university_keywords(self, university_news):
        """ëŒ€í•™êµ í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€"""
        if not university_news:
            return {"university_keyword_accuracy": 0, "university_keyword_details": {}}
        
        correct_classifications = 0
        total_classifications = 0
        details = {}
        
        for univ_name, news_list in university_news.items():
            total_classifications += len(news_list)
            
            # ì‹¤ì œ ëŒ€í•™êµëª…ì´ ì œëª©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            correct_count = 0
            for news in news_list:
                title = news['cleaned_title']
                if univ_name in title or any(keyword in title for keyword in self.university_keywords):
                    correct_count += 1
                    correct_classifications += 1
            
            details[univ_name] = {
                "total_news": len(news_list),
                "correct_classifications": correct_count,
                "accuracy": correct_count / len(news_list) if len(news_list) > 0 else 0
            }
        
        return {
            "university_keyword_accuracy": correct_classifications / total_classifications if total_classifications > 0 else 0,
            "university_keyword_details": details
        }
    
    def _evaluate_cluster_keywords(self, clusters):
        """í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€"""
        if not clusters:
            return {"cluster_keyword_accuracy": 0, "cluster_keyword_details": {}}
        
        # í´ëŸ¬ìŠ¤í„° ë°ì´í„° í˜•ì‹ í™•ì¸ ë° ë³€í™˜
        # TF-IDF í´ëŸ¬ìŠ¤í„°ëŸ¬ëŠ” _format_news_itemìœ¼ë¡œ í¬ë§·íŒ…ëœ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ
        # generate_cluster_labelsë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì— ì›ë³¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ í•„ìš”
        converted_clusters = {}
        for cluster_id, news_list in clusters.items():
            converted_news = []
            for news in news_list:
                # ì´ë¯¸ í¬ë§·íŒ…ëœ ë°ì´í„°ì¸ ê²½ìš° (title, linkë§Œ ìˆëŠ” ê²½ìš°)
                if "title" in news and "cleaned_title" not in news:
                    # titleì„ cleaned_titleë¡œ ì‚¬ìš©
                    converted_news.append({
                        "cleaned_title": news.get("title", ""),
                        "original": news  # ì›ë³¸ ë°ì´í„° ë³´ì¡´
                    })
                # ì›ë³¸ í˜•ì‹ ë°ì´í„°ì¸ ê²½ìš°
                elif "cleaned_title" in news:
                    converted_news.append(news)
                else:
                    # titleë„ ì—†ëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
                    logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° {cluster_id}ì˜ ë‰´ìŠ¤ì— title ë˜ëŠ” cleaned_titleì´ ì—†ìŠµë‹ˆë‹¤: {news}")
                    continue
            if converted_news:
                converted_clusters[cluster_id] = converted_news
        
        if not converted_clusters:
            logger.warning("âš ï¸ ë³€í™˜ëœ í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {"cluster_keyword_accuracy": 0, "cluster_keyword_details": {}}
        
        try:
            cluster_labels = self.news_analyzer.generate_cluster_labels(converted_clusters)
        except Exception as e:
            logger.error(f"âŒ generate_cluster_labels ì‹¤íŒ¨: {e}")
            # Fallback: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
            cluster_labels = {}
            for cluster_id, news_list in converted_clusters.items():
                titles = [item.get("cleaned_title", item.get("title", "")) for item in news_list]
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì œëª©ì˜ ì²« ë‹¨ì–´ ì‚¬ìš©)
                major_category = titles[0].split()[0] if titles else "Unknown"
                cluster_labels[cluster_id] = {
                    "major_category": major_category,
                    "keywords": []
                }
        
        total_clusters = len(clusters)
        meaningful_clusters = 0
        details = {}
        
        for cluster_id, cluster_info in cluster_labels.items():
            major_category = cluster_info.get("major_category", "")
            keywords = cluster_info.get("keywords", [])
            
            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì •í™•ë„ í™•ì¸
            category_match = False
            for category, category_words in self.category_keywords.items():
                if any(word in major_category for word in category_words):
                    category_match = True
                    break
            
            if category_match:
                meaningful_clusters += 1
            
            details[f"cluster_{cluster_id}"] = {
                "major_category": major_category,
                "keywords": keywords,
                "news_count": len(clusters.get(cluster_id, [])),
                "category_match": category_match
            }
        
        return {
            "cluster_keyword_accuracy": meaningful_clusters / total_clusters if total_clusters > 0 else 0,
            "cluster_keyword_details": details
        }
    
    def evaluate_performance(self, news_data, limit=1000, total_time=None):
        """
        ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í‰ê°€
        
        Args:
            news_data (list): ë‰´ìŠ¤ ë°ì´í„°
            limit (int): ë¶„ì„í•  ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜
            
        Returns:
            dict: ì„±ëŠ¥ ì§€í‘œ
        """
        try:
            logger.info("=" * 60)
            logger.info("âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ğŸ“° ì…ë ¥ ë‰´ìŠ¤ ë°ì´í„°: {len(news_data)}ê°œ")
            logger.info(f"ğŸ”¢ ë¶„ì„ ì œí•œ: {limit}ê°œ")
            
            performance_metrics = {}
            
            # ì „ì²´ ì‹œê°„ì´ ì œê³µë˜ë©´ ì¬ì‚¬ìš©, ì•„ë‹ˆë©´ ì¸¡ì •
            if total_time is not None:
                processing_time = total_time
                logger.info(f"â±ï¸ ì „ì²´ í‰ê°€ ì‹œê°„ ì¬ì‚¬ìš©: {processing_time:.2f}ì´ˆ")
            else:
                # ì „ì²´ ë¶„ì„ ì‹œê°„ ì¸¡ì • (fallback)
                logger.info("ğŸš€ ì „ì²´ ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                start_time = time.time()
                result = self.news_analyzer.analyze_from_db(news_data[:limit])
                end_time = time.time()
                
                processing_time = end_time - start_time
                logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ")
            
            performance_metrics.update({
                "total_processing_time": processing_time,
                "news_count": len(news_data[:limit]),
                "throughput": len(news_data[:limit]) / processing_time if processing_time > 0 else 0,
                "analysis_success": True
            })
            
            logger.info(f"ğŸ“Š ì²˜ë¦¬ëŸ‰: {performance_metrics['throughput']:.1f} ë‰´ìŠ¤/ì´ˆ")
            logger.info("ğŸ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (ê°„ë‹¨í•œ ë°©ë²•)
            import sys
            memory_usage = sys.getsizeof(news_data[:limit]) / (1024 * 1024)
            performance_metrics["estimated_memory_usage_mb"] = memory_usage
            logger.info(f"ğŸ’¾ ì¶”ì • ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.1f}MB")
            
            logger.info("ğŸ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
            logger.info("=" * 60)
            return performance_metrics
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def comprehensive_evaluation(self, limit=1000, use_json_file=True, json_file_path="test_news_1000.json", method='news_analyzer', clusterer=None):
        """
        ì¢…í•© ì •í™•ë„ í‰ê°€
        
        Args:
            limit (int): ë¶„ì„í•  ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜
            use_json_file (bool): JSON íŒŒì¼ ì‚¬ìš© ì—¬ë¶€
            json_file_path (str): JSON íŒŒì¼ ê²½ë¡œ
            method (str): í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²• ID
            clusterer: ì‚¬ìš©í•  í´ëŸ¬ìŠ¤í„°ëŸ¬ ê°ì²´ (Noneì´ë©´ self.news_analyzer ì‚¬ìš©)
            
        Returns:
            dict: ì¢…í•© í‰ê°€ ê²°ê³¼
        """
        try:
            logger.info("ğŸ¯" * 30)
            logger.info("ğŸ¯ ì¢…í•© ì •í™•ë„ í‰ê°€ ì‹œì‘")
            logger.info("ğŸ¯" * 30)
            logger.info(f"ğŸ“Š í‰ê°€ ì œí•œ: {limit}ê°œ ë‰´ìŠ¤")
            logger.info(f"â° ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            # ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            if use_json_file and json_file_path:
                # JSON íŒŒì¼ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                import os
                if not os.path.isabs(json_file_path):
                    # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° backend/data í´ë”ì—ì„œ ì°¾ê¸°
                    backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                    data_dir = os.path.join(backend_dir, "data")
                    
                    # íŒŒì¼ëª…ë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: "test_news_1000.json")
                    if os.path.dirname(json_file_path) == "" or os.path.dirname(json_file_path) == ".":
                        json_file_path = os.path.join(data_dir, os.path.basename(json_file_path))
                    # "data/"ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
                    elif json_file_path.startswith("data/"):
                        json_file_path = os.path.join(backend_dir, json_file_path)
                    # ê·¸ ì™¸ì˜ ê²½ìš°ëŠ” backend_dir ê¸°ì¤€
                    else:
                        json_file_path = os.path.join(backend_dir, json_file_path)
                
                logger.info(f"ğŸ“ JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘: {json_file_path}")
                news_data = self._load_news_from_json(json_file_path, limit)
            else:
                logger.info("ğŸ“° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
                news_data = fetch_news_from_db(limit=limit)
            
            if not news_data:
                logger.error("âŒ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {"error": "ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
            
            logger.info(f"âœ… ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(news_data)}ê°œ")
            
            # ì „ì²´ í‰ê°€ ì‹œê°„ ì¸¡ì • ì‹œì‘
            evaluation_start_time = time.time()
            
            # ì„ë² ë”© í•œ ë²ˆë§Œ ìƒì„± (ëª¨ë“  í‰ê°€ì—ì„œ ì¬ì‚¬ìš©)
            logger.info("ğŸ¤– ì „ì²´ í‰ê°€ìš© ì„ë² ë”© ìƒì„± ì¤‘...")
            titles = [item["title"] for item in news_data[:limit]]
            embeddings_start_time = time.time()
            embeddings = self.embedding_model.encode(titles, normalize_embeddings=True)
            embeddings_time = time.time() - embeddings_start_time
            logger.info(f"âœ… ì „ì²´ í‰ê°€ìš© ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape} (ì†Œìš”ì‹œê°„: {embeddings_time:.2f}ì´ˆ)")
            
            evaluation_results = {
                "evaluation_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "data_info": {
                    "total_news": len(news_data),
                    "limit": limit,
                    "embeddings_shape": embeddings.shape,
                    "embeddings_time": embeddings_time
                }
            }
            
            # ì‚¬ìš©í•  í´ëŸ¬ìŠ¤í„°ëŸ¬ ê²°ì •
            if clusterer is None:
                clusterer = self.news_analyzer
                logger.info(f"ğŸ“Œ ê¸°ë³¸ í´ëŸ¬ìŠ¤í„°ëŸ¬ ì‚¬ìš©: NewsAnalyzer")
            else:
                logger.info(f"ğŸ“Œ ì„ íƒí•œ í´ëŸ¬ìŠ¤í„°ëŸ¬ ì‚¬ìš©: {method}")
            
            # 1. í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ (ì„ë² ë”© ì¬ì‚¬ìš©)
            logger.info("\n" + "ğŸ“Š" * 20)
            logger.info("1ï¸âƒ£ í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ ì‹œì‘")
            clustering_results = self.evaluate_clustering_quality(news_data, limit, embeddings, clusterer=clusterer)
            evaluation_results["clustering_quality"] = clustering_results
            
            # ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ (ì¤‘ë³µ ë¶„ì„ ë°©ì§€ìš©)
            analysis_result = clustering_results.get("analysis_result")
            
            # 2. í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€ (ì„ë² ë”© ì¬ì‚¬ìš©)
            logger.info("\n" + "ğŸ”‘" * 20)
            logger.info("2ï¸âƒ£ í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€ ì‹œì‘")
            keyword_results = self.evaluate_keyword_extraction(news_data, limit, embeddings, clusterer=clusterer)
            evaluation_results["keyword_extraction"] = keyword_results
            
            # 2-1. Topic Consistency í‰ê°€ (ChatGPT ì œì•ˆ - ìƒˆë¡œìš´ í‰ê°€ ì§€í‘œ)
            logger.info("\n" + "ğŸ“Š" * 20)
            logger.info("2ï¸âƒ£-1ï¸âƒ£ Topic Consistency í‰ê°€ ì‹œì‘")
            topic_consistency_results = self.evaluate_topic_consistency(news_data, analysis_result, limit)
            evaluation_results["topic_consistency"] = topic_consistency_results
            
            # 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ì˜ ë¶„ì„ ì‹œê°„ ì‚¬ìš©)
            logger.info("\n" + "âš¡" * 20)
            logger.info("3ï¸âƒ£ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
            
            # í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ì—ì„œ ë¶„ì„ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
            analysis_time = clustering_results.get("analysis_time", None)
            if analysis_time:
                logger.info(f"â±ï¸ í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ì˜ ë¶„ì„ ì‹œê°„ ì¬ì‚¬ìš©: {analysis_time:.2f}ì´ˆ")
                performance_results = self.evaluate_performance(news_data, limit, analysis_time)
            else:
                logger.warning("âš ï¸ ë¶„ì„ ì‹œê°„ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ í‰ê°€ ì‹œê°„ ì‚¬ìš©")
                total_evaluation_time = time.time() - evaluation_start_time
                performance_results = self.evaluate_performance(news_data, limit, total_evaluation_time)
            
            evaluation_results["performance"] = performance_results
            
            # 4. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            logger.info("\n" + "ğŸ†" * 20)
            logger.info("4ï¸âƒ£ ì¢…í•© ì ìˆ˜ ê³„ì‚° ì‹œì‘")
            overall_score = self._calculate_overall_score(
                clustering_results, keyword_results, performance_results, topic_consistency_results
            )
            evaluation_results["overall_score"] = overall_score
            
            logger.info(f"ğŸ‰ ì¢…í•© ì •í™•ë„ í‰ê°€ ì™„ë£Œ!")
            logger.info(f"ğŸ† ìµœì¢… ì ìˆ˜: {overall_score['score']:.2f}/100 ({overall_score['grade']})")
            logger.info("ğŸ¯" * 30)
            
            # í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ë¡œê·¸ íŒŒì¼ì— ì €ì¥ (ë¶„ì„ ê²°ê³¼ ì „ë‹¬)
            self._save_evaluation_summary(evaluation_results, overall_score, analysis_result)
            
            # JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ float32ë¥¼ floatë¡œ ë³€í™˜
            return self._convert_to_json_serializable(evaluation_results)
            
        except Exception as e:
            logger.error(f"ì¢…í•© ì •í™•ë„ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def _save_evaluation_summary(self, evaluation_results, overall_score, analysis_result=None):
        """
        í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ë³„ë„ ë¡œê·¸ íŒŒì¼ì— ì €ì¥ (ìƒì„¸ ì •ë³´ í¬í•¨)
        
        Args:
            evaluation_results (dict): ì „ì²´ í‰ê°€ ê²°ê³¼
            overall_score (dict): ì¢…í•© ì ìˆ˜
        """
        try:
            import os
            from datetime import datetime
            
            # ìš”ì•½ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
            log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "logs")
            os.makedirs(log_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%m-%d_%H%M")
            summary_file = os.path.join(log_dir, f"summary_{timestamp}.txt")
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("ğŸ“Š ì •í™•ë„ í‰ê°€ ê²°ê³¼ ìƒì„¸ ë³´ê³ ì„œ\n")
                f.write("=" * 80 + "\n")
                f.write(f"í‰ê°€ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ë¶„ì„ ë‰´ìŠ¤ ìˆ˜: {evaluation_results.get('data_info', {}).get('total_news', 'N/A')}ê°œ\n")
                f.write(f"ì œí•œ: {evaluation_results.get('data_info', {}).get('limit', 'N/A')}ê°œ\n\n")
                
                # ì¢…í•© ì ìˆ˜
                f.write("ğŸ† ì¢…í•© ì ìˆ˜\n")
                f.write("-" * 40 + "\n")
                f.write(f"ì´ì : {overall_score.get('score', 0):.2f}/100\n")
                f.write(f"ë“±ê¸‰: {overall_score.get('grade', 'N/A')}\n\n")
                
                # ì„¸ë¶€ ì ìˆ˜
                components = overall_score.get('components', {})
                f.write("ğŸ“Š ì„¸ë¶€ ì ìˆ˜\n")
                f.write("-" * 40 + "\n")
                f.write(f"í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ: {components.get('clustering', 0):.1f}/30\n")
                f.write(f"í‚¤ì›Œë“œ ì¶”ì¶œ: {components.get('keyword_extraction', 0):.1f}/40\n")
                f.write(f"ì„±ëŠ¥: {components.get('performance', 0):.1f}/30\n\n")
                
                # í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ ìƒì„¸
                clustering = evaluation_results.get('clustering_quality', {})
                f.write("ğŸ” í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ ìƒì„¸\n")
                f.write("-" * 40 + "\n")
                f.write(f"í´ëŸ¬ìŠ¤í„° ìˆ˜: {clustering.get('n_clusters', 0)}ê°œ\n")
                f.write(f"ë…¸ì´ì¦ˆ ìˆ˜: {clustering.get('n_noise', 0)}ê°œ\n")
                f.write(f"ë…¸ì´ì¦ˆ ë¹„ìœ¨: {clustering.get('noise_ratio', 0):.1%}\n")
                f.write(f"í‰ê·  í´ëŸ¬ìŠ¤í„° í¬ê¸°: {clustering.get('avg_cluster_size', 0):.1f}\n")
                if clustering.get('silhouette_score'):
                    f.write(f"ì‹¤ë£¨ì—£ ì ìˆ˜: {clustering['silhouette_score']:.4f}\n")
                if clustering.get('davies_bouldin_index'):
                    f.write(f"Davies-Bouldin Index: {clustering['davies_bouldin_index']:.4f}\n\n")
                
                # HDBSCAN ì„¤ì •ê°’ ì €ì¥
                f.write("âš™ï¸ HDBSCAN ì„¤ì •ê°’\n")
                f.write("-" * 40 + "\n")
                f.write(f"MIN_CLUSTER_SIZE: {self.news_analyzer.HDBSCAN_MIN_CLUSTER_SIZE}\n")
                f.write(f"MIN_SAMPLES: {self.news_analyzer.HDBSCAN_MIN_SAMPLES}\n")
                f.write(f"EPSILON: {self.news_analyzer.HDBSCAN_EPSILON}\n\n")
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ ìƒì„¸
                keyword = evaluation_results.get('keyword_extraction', {})
                f.write("ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ìƒì„¸\n")
                f.write("-" * 40 + "\n")
                f.write(f"ëŒ€í•™êµ í‚¤ì›Œë“œ ì •í™•ë„: {keyword.get('university_keyword_accuracy', 0):.1%}\n")
                f.write(f"í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì •í™•ë„: {keyword.get('cluster_keyword_accuracy', 0):.1%}\n\n")
                
                # ì„±ëŠ¥ ìƒì„¸
                performance = evaluation_results.get('performance', {})
                f.write("âš¡ ì„±ëŠ¥ ìƒì„¸\n")
                f.write("-" * 40 + "\n")
                f.write(f"ì´ ì²˜ë¦¬ ì‹œê°„: {performance.get('total_processing_time', 0):.2f}ì´ˆ\n")
                f.write(f"ì²˜ë¦¬ëŸ‰: {performance.get('throughput', 0):.1f} ë‰´ìŠ¤/ì´ˆ\n\n")
                
                # ì‹¤ì œ ë¶„ì„ ê²°ê³¼ ì €ì¥
                f.write("ğŸ“‹ ì‹¤ì œ ë¶„ì„ ê²°ê³¼\n")
                f.write("=" * 80 + "\n")
                
                # í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì • ë¡œê·¸ ì €ì¥
                f.write("ğŸ” í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì • ìƒì„¸\n")
                f.write("-" * 60 + "\n")
                
                # NewsAnalyzerë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥
                try:
                    # JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ (backend/data í´ë”ì—ì„œ ì°¾ê¸°)
                    backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                    data_dir = os.path.join(backend_dir, "data")
                    json_file_path = os.path.join(data_dir, "test_news_1000.json")
                    news_data = self._load_news_from_json(json_file_path, evaluation_results.get('data_info', {}).get('limit', 1000))
                    
                    if news_data:
                        # í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì •ì„ ë¡œê·¸ë¡œ ìº¡ì²˜í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
                        import logging
                        original_level = logging.getLogger().level
                        logging.getLogger().setLevel(logging.INFO)
                        
                        # ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰ (ì¤‘ë³µ ë¶„ì„ ë°©ì§€ë¥¼ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬)
                        # analysis_result = self.news_analyzer.analyze_from_db(news_data)
                        
                        # ë¡œê·¸ ë ˆë²¨ ë³µì›
                        logging.getLogger().setLevel(original_level)
                        
                        if analysis_result:
                            f.write("ğŸ« ëŒ€ë¶„ë¥˜/ì¤‘ë¶„ë¥˜ ë¶„ì„ ê²°ê³¼\n")
                            f.write("-" * 60 + "\n")
                            
                            # ëŒ€ë¶„ë¥˜ë³„ í†µê³„
                            total_major_categories = len(analysis_result)
                            f.write(f"ğŸ“Š ì´ ëŒ€ë¶„ë¥˜ ìˆ˜: {total_major_categories}ê°œ\n\n")
                            
                            for major_idx, major_category in enumerate(analysis_result, 1):
                                major_name = major_category.get('majorKeyword', 'Unknown')
                                middle_keywords = major_category.get('middleKeywords', [])
                                other_news = major_category.get('otherNews', [])
                                
                                # ëŒ€ë¶„ë¥˜ë³„ ë‰´ìŠ¤ ìˆ˜ ê³„ì‚°
                                total_news_in_major = len(other_news)
                                for middle_cat in middle_keywords:
                                    total_news_in_major += len(middle_cat.get('relatedNews', []))
                                
                                f.write(f"ğŸ“ ëŒ€ë¶„ë¥˜ {major_idx}: {major_name} (ì´ {total_news_in_major}ê°œ ë‰´ìŠ¤)\n")
                                
                                # ì¤‘ë¶„ë¥˜ ì¶œë ¥
                                if middle_keywords:
                                    f.write(f"   ì¤‘ë¶„ë¥˜ ìˆ˜: {len(middle_keywords)}ê°œ\n")
                                    for middle_idx, middle_cat in enumerate(middle_keywords, 1):
                                        middle_name = middle_cat.get('middleKeyword', 'Unknown')
                                        related_news = middle_cat.get('relatedNews', [])
                                        
                                        f.write(f"   â”œâ”€ ì¤‘ë¶„ë¥˜ {middle_idx}: {middle_name} ({len(related_news)}ê°œ ë‰´ìŠ¤)\n")
                                        
                                        # ë‰´ìŠ¤ ì œëª© ì¶œë ¥ (ìµœëŒ€ 5ê°œ)
                                        for news_idx, news in enumerate(related_news[:5], 1):
                                            news_title = news.get('title', 'Unknown')
                                            # ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                                            if len(news_title) > 80:
                                                news_title = news_title[:80] + "..."
                                            f.write(f"   â”‚  â””â”€ {news_idx}. {news_title}\n")
                                        
                                        # ë” ë§ì€ ë‰´ìŠ¤ê°€ ìˆìœ¼ë©´ í‘œì‹œ
                                        if len(related_news) > 5:
                                            f.write(f"   â”‚     ... ì™¸ {len(related_news) - 5}ê°œ\n")
                                
                                # ê¸°íƒ€ ë‰´ìŠ¤ ì¶œë ¥
                                if other_news:
                                    f.write(f"   â””â”€ ê¸°íƒ€ ë‰´ìŠ¤: {len(other_news)}ê°œ\n")
                                    for news_idx, news in enumerate(other_news[:3], 1):
                                        news_title = news.get('title', 'Unknown')
                                        if len(news_title) > 80:
                                            news_title = news_title[:80] + "..."
                                        f.write(f"      â””â”€ {news_idx}. {news_title}\n")
                                    if len(other_news) > 3:
                                        f.write(f"         ... ì™¸ {len(other_news) - 3}ê°œ\n")
                                
                                f.write("\n")
                            
                            # ì „ì²´ í†µê³„ ìš”ì•½
                            f.write("ğŸ“Š ì „ì²´ í†µê³„ ìš”ì•½\n")
                            f.write("-" * 60 + "\n")
                            f.write(f"ì´ ëŒ€ë¶„ë¥˜: {total_major_categories}ê°œ\n")
                            
                            total_middle_categories = sum(len(major.get('middleKeywords', [])) for major in analysis_result)
                            f.write(f"ì´ ì¤‘ë¶„ë¥˜: {total_middle_categories}ê°œ\n")
                            
                            total_news_count = sum(
                                len(major.get('otherNews', [])) + 
                                sum(len(middle.get('relatedNews', [])) for middle in major.get('middleKeywords', []))
                                for major in analysis_result
                            )
                            f.write(f"ì´ ë‰´ìŠ¤ ìˆ˜: {total_news_count}ê°œ\n")
                            
                        else:
                            f.write("âŒ ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")
                    
                except Exception as e:
                    f.write(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}\n")
                
                f.write("\n" + "=" * 80 + "\n")
                f.write("ğŸ“ ë³´ê³ ì„œ ë\n")
                f.write("=" * 80 + "\n")
            
            logger.info(f"ğŸ“ ìƒì„¸ í‰ê°€ ê²°ê³¼ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {summary_file}")
            
        except Exception as e:
            logger.error(f"í‰ê°€ ê²°ê³¼ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _load_news_from_json(self, json_file_path, limit):
        """
        JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
        
        Args:
            json_file_path (str): JSON íŒŒì¼ ê²½ë¡œ
            limit (int): ë¡œë“œí•  ë‰´ìŠ¤ ê°œìˆ˜
            
        Returns:
            list: ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        import json
        import os
        
        try:
            # íŒŒì¼ ê²½ë¡œ í™•ì¸
            if not os.path.exists(json_file_path):
                logger.error(f"JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file_path}")
                return []
            
            # JSON íŒŒì¼ ë¡œë“œ
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë‰´ìŠ¤ ë°ì´í„° ì¶”ì¶œ
            news_data = data.get('news_data', [])
            metadata = data.get('metadata', {})
            
            logger.info(f"ğŸ“Š JSON íŒŒì¼ ë©”íƒ€ë°ì´í„°:")
            logger.info(f"   - ì¶”ì¶œ ì‹œê°„: {metadata.get('extraction_time', 'N/A')}")
            logger.info(f"   - ì´ ë‰´ìŠ¤ ìˆ˜: {metadata.get('total_news_count', len(news_data))}")
            logger.info(f"   - ì„¤ëª…: {metadata.get('description', 'N/A')}")
            
            # limit ì ìš©
            if limit and limit < len(news_data):
                news_data = news_data[:limit]
                logger.info(f"ğŸ“ limit ì ìš©: {len(news_data)}ê°œ ë‰´ìŠ¤")
            
            logger.info(f"âœ… JSON íŒŒì¼ì—ì„œ {len(news_data)}ê°œ ë‰´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            return news_data
            
        except Exception as e:
            logger.error(f"JSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _convert_to_json_serializable(self, obj):
        """
        JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ float32, numpy íƒ€ì… ë“±ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        
        Args:
            obj: ë³€í™˜í•  ê°ì²´
            
        Returns:
            JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê°ì²´
        """
        import numpy as np
        
        if isinstance(obj, dict):
            return {key: self._convert_to_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif hasattr(obj, 'item'):  # numpy scalar
            return obj.item()
        else:
            return obj
    
    def _calculate_overall_score(self, clustering_results, keyword_results, performance_results, topic_consistency_results=None):
        """
        ì¢…í•© ì ìˆ˜ ê³„ì‚° (ChatGPT ì œì•ˆ ë°˜ì˜)
        
        Args:
            clustering_results: í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ ê²°ê³¼
            keyword_results: í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ í‰ê°€ ê²°ê³¼
            performance_results: ì„±ëŠ¥ í‰ê°€ ê²°ê³¼
            topic_consistency_results: Topic Consistency í‰ê°€ ê²°ê³¼ (ì„ íƒ)
        """
        try:
            logger.info("ğŸ§® ì¢…í•© ì ìˆ˜ ê³„ì‚° ì‹œì‘...")
            score_components = {}
            total_score = 0
            max_score = 0
            
            # í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ ì ìˆ˜ (30ì  ë§Œì ) - ë‚´ë¶€ í‰ê°€ ì§€í‘œ ê°œì„ 
            logger.info("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì¤‘...")
            if "error" not in clustering_results:
                clustering_score = 0
                
                # ì‹¤ë£¨ì—£ ì ìˆ˜ (10ì ) - ê°€ì¤‘ì¹˜ ì¡°ì •
                if clustering_results.get("silhouette_score") is not None:
                    silhouette = clustering_results["silhouette_score"]
                    # ì‹¤ë£¨ì—£ ì ìˆ˜ëŠ” -1~1 ë²”ìœ„ì´ë¯€ë¡œ 0~1ë¡œ ì •ê·œí™” í›„ ì ìˆ˜í™”
                    normalized_silhouette = (silhouette + 1) / 2  # -1~1 -> 0~1
                    silhouette_points = normalized_silhouette * 10
                    clustering_score += silhouette_points
                    logger.info(f"   ğŸ“ ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette:.4f} â†’ {silhouette_points:.1f}ì ")
                else:
                    logger.warning("   âš ï¸ ì‹¤ë£¨ì—£ ì ìˆ˜ ì—†ìŒ")
                
                # Calinski-Harabasz Index (10ì ) - ì¶”ê°€ëœ ì§€í‘œ
                if clustering_results.get("calinski_harabasz_index") is not None:
                    ch_index = clustering_results["calinski_harabasz_index"]
                    # CH IndexëŠ” ê°’ì´ í´ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ ì •ê·œí™” í•„ìš”
                    # ì¼ë°˜ì ìœ¼ë¡œ 100~10000 ë²”ìœ„ì´ë¯€ë¡œ ë¡œê·¸ ìŠ¤ì¼€ì¼ ì‚¬ìš©
                    if ch_index > 0:
                        normalized_ch = min(1.0, np.log10(ch_index + 1) / 4)  # ëŒ€ëµ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
                        ch_points = normalized_ch * 10
                        clustering_score += ch_points
                        logger.info(f"   ğŸ“Š Calinski-Harabasz Index: {ch_index:.2f} â†’ {ch_points:.1f}ì ")
                else:
                    logger.warning("   âš ï¸ Calinski-Harabasz Index ì—†ìŒ")
                
                # Davies-Bouldin Index (10ì ) - ì¶”ê°€ëœ ì§€í‘œ
                if clustering_results.get("davies_bouldin_index") is not None:
                    db_index = clustering_results["davies_bouldin_index"]
                    # DB IndexëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ ì—­ìˆ˜ ì‚¬ìš©
                    # ì¼ë°˜ì ìœ¼ë¡œ 0~5 ë²”ìœ„ì´ë¯€ë¡œ ì •ê·œí™”
                    normalized_db = max(0, 1 - (db_index / 5))  # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
                    db_points = normalized_db * 10
                    clustering_score += db_points
                    logger.info(f"   ğŸ“ Davies-Bouldin Index: {db_index:.4f} â†’ {db_points:.1f}ì ")
                else:
                    logger.warning("   âš ï¸ Davies-Bouldin Index ì—†ìŒ")
                
                score_components["clustering"] = clustering_score
                total_score += clustering_score
                logger.info(f"   âœ… í´ëŸ¬ìŠ¤í„°ë§ ì´ì : {clustering_score:.1f}/30")
            else:
                logger.warning("   âŒ í´ëŸ¬ìŠ¤í„°ë§ í‰ê°€ ì‹¤íŒ¨")
            max_score += 30
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ ì ìˆ˜ (30ì  ë§Œì ) - ê°€ì¤‘ì¹˜ ì¡°ì •
            logger.info("ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ì •í™•ë„ ì ìˆ˜ ê³„ì‚° ì¤‘...")
            if "error" not in keyword_results:
                keyword_score = 0
                
                # ëŒ€í•™êµ í‚¤ì›Œë“œ ì •í™•ë„ (15ì )
                univ_accuracy = keyword_results.get("university_keyword_accuracy", 0)
                univ_points = univ_accuracy * 15
                keyword_score += univ_points
                logger.info(f"   ğŸ« ëŒ€í•™êµ í‚¤ì›Œë“œ ì •í™•ë„: {univ_accuracy:.1%} â†’ {univ_points:.1f}ì ")
                
                # í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì •í™•ë„ (15ì )
                cluster_accuracy = keyword_results.get("cluster_keyword_accuracy", 0)
                cluster_points = cluster_accuracy * 15
                keyword_score += cluster_points
                logger.info(f"   ğŸ” í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì •í™•ë„: {cluster_accuracy:.1%} â†’ {cluster_points:.1f}ì ")
                
                score_components["keyword_extraction"] = keyword_score
                total_score += keyword_score
                logger.info(f"   âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì´ì : {keyword_score:.1f}/30")
            else:
                logger.warning("   âŒ í‚¤ì›Œë“œ ì¶”ì¶œ í‰ê°€ ì‹¤íŒ¨")
            max_score += 30
            
            # Topic Consistency ì ìˆ˜ (20ì  ë§Œì ) - ChatGPT ì œì•ˆ ì¶”ê°€
            logger.info("ğŸ“Š Topic Consistency ì ìˆ˜ ê³„ì‚° ì¤‘...")
            if topic_consistency_results and "error" not in topic_consistency_results:
                topic_consistency_score = 0
                consistency = topic_consistency_results.get("topic_consistency_score", 0)
                consistency_points = consistency * 20  # 0~1 ë²”ìœ„ë¥¼ 0~20ì ìœ¼ë¡œ ë³€í™˜
                topic_consistency_score += consistency_points
                logger.info(f"   ğŸ“Š Topic Consistency: {consistency:.4f} â†’ {consistency_points:.1f}ì ")
                
                score_components["topic_consistency"] = topic_consistency_score
                total_score += topic_consistency_score
                logger.info(f"   âœ… Topic Consistency ì´ì : {topic_consistency_score:.1f}/20")
            else:
                logger.warning("   âš ï¸ Topic Consistency í‰ê°€ ì—†ìŒ (ì„ íƒì  ì§€í‘œ)")
            max_score += 20
            
            # ì„±ëŠ¥ ì ìˆ˜ (30ì  ë§Œì )
            logger.info("âš¡ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° ì¤‘...")
            if "error" not in performance_results:
                performance_score = 0
                
                # ì²˜ë¦¬ ì‹œê°„ ì ìˆ˜ (15ì ) - 10ì´ˆ ì´ë‚´ë©´ ë§Œì 
                processing_time = performance_results.get("total_processing_time", 100)
                time_score = min(15, max(0, 15 - (processing_time - 10) * 0.5))
                performance_score += time_score
                logger.info(f"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ â†’ {time_score:.1f}ì ")
                
                # ì²˜ë¦¬ëŸ‰ ì ìˆ˜ (15ì ) - ë‰´ìŠ¤/ì´ˆ
                throughput = performance_results.get("throughput", 0)
                throughput_score = min(15, max(0, throughput * 0.1))
                performance_score += throughput_score
                logger.info(f"   ğŸ“Š ì²˜ë¦¬ëŸ‰: {throughput:.1f} ë‰´ìŠ¤/ì´ˆ â†’ {throughput_score:.1f}ì ")
                
                score_components["performance"] = performance_score
                total_score += performance_score
                logger.info(f"   âœ… ì„±ëŠ¥ ì´ì : {performance_score:.1f}/30")
            else:
                logger.warning("   âŒ ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨")
            max_score += 30
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚°
            final_score = (total_score / max_score * 100) if max_score > 0 else 0
            grade = self._get_grade(final_score)
            
            logger.info(f"ğŸ† ìµœì¢… ì ìˆ˜: {total_score:.1f}/{max_score} â†’ {final_score:.1f}/100 ({grade})")
            
            result = {
                "score": final_score,
                "max_possible_score": max_score,
                "components": score_components,
                "grade": grade
            }
            
            # JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ float32ë¥¼ floatë¡œ ë³€í™˜
            return self._convert_to_json_serializable(result)
            
        except Exception as e:
            logger.error(f"ì¢…í•© ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def _get_grade(self, score):
        """ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ë°˜í™˜"""
        if score >= 90:
            return "A+"
        elif score >= 80:
            return "A"
        elif score >= 70:
            return "B+"
        elif score >= 60:
            return "B"
        elif score >= 50:
            return "C+"
        elif score >= 40:
            return "C"
        else:
            return "D"


def run_accuracy_evaluation(limit=1000):
    """
    ì •í™•ë„ í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        limit (int): ë¶„ì„í•  ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜
        
    Returns:
        dict: í‰ê°€ ê²°ê³¼
    """
    evaluator = AccuracyEvaluator()
    return evaluator.comprehensive_evaluation(limit=limit)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("=" * 80)
    print("ğŸ“Š ë‰´ìŠ¤ ë¶„ì„ ì •í™•ë„ í‰ê°€")
    print("=" * 80)
    
    try:
        result = run_accuracy_evaluation(limit=1000)
        
        if "error" in result:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {result['error']}")
        else:
            print(f"âœ… í‰ê°€ ì™„ë£Œ!")
            print(f"ğŸ“… í‰ê°€ ì‹œê°„: {result['evaluation_timestamp']}")
            print(f"ğŸ“° ë¶„ì„ ë‰´ìŠ¤: {result['data_info']['total_news']}ê°œ")
            print(f"ğŸ† ì¢…í•© ì ìˆ˜: {result['overall_score']['score']:.1f}/100 ({result['overall_score']['grade']})")
            
            # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
            if "clustering_quality" in result and "error" not in result["clustering_quality"]:
                cq = result["clustering_quality"]
                print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ:")
                print(f"   â€¢ í´ëŸ¬ìŠ¤í„° ìˆ˜: {cq.get('n_clusters', 0)}ê°œ")
                print(f"   â€¢ ë…¸ì´ì¦ˆ ë¹„ìœ¨: {cq.get('noise_ratio', 0):.1%}")
                print(f"   â€¢ ì‹¤ë£¨ì—£ ì ìˆ˜: {cq.get('silhouette_score', 'N/A')}")
            
            if "keyword_extraction" in result and "error" not in result["keyword_extraction"]:
                ke = result["keyword_extraction"]
                print(f"\nğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ:")
                print(f"   â€¢ ëŒ€í•™êµ ë¶„ë¥˜ ì •í™•ë„: {ke.get('university_keyword_accuracy', 0):.1%}")
                print(f"   â€¢ í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì •í™•ë„: {ke.get('cluster_keyword_accuracy', 0):.1%}")
            
            if "performance" in result and "error" not in result["performance"]:
                perf = result["performance"]
                print(f"\nâš¡ ì„±ëŠ¥:")
                print(f"   â€¢ ì²˜ë¦¬ ì‹œê°„: {perf.get('total_processing_time', 0):.2f}ì´ˆ")
                print(f"   â€¢ ì²˜ë¦¬ëŸ‰: {perf.get('throughput', 0):.1f} ë‰´ìŠ¤/ì´ˆ")
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print("=" * 80)
