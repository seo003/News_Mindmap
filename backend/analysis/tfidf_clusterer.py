#!/usr/bin/env python3
"""
TF-IDF ê¸°ë°˜ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë“ˆ

ë‰´ìŠ¤ ë°ì´í„°ë¥¼ TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰
"""

import logging
from collections import Counter, defaultdict
from konlpy.tag import Okt
import re
import os
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

logger = logging.getLogger(__name__)

class TfidfClusterer:
    """
    TF-IDF ê¸°ë°˜ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ í´ë˜ìŠ¤
    """
    
    def __init__(self):
        """
        ì´ˆê¸°í™”
        """
        # KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.okt = Okt()
        
        # í´ëŸ¬ìŠ¤í„°ë§ íŒŒë¼ë¯¸í„° 
        self.MIN_TITLE_LENGTH = 10          # ìµœì†Œ ì œëª© ê¸¸ì´ 
        self.MIN_WORD_LENGTH = 2            # ìµœì†Œ ë‹¨ì–´(ëª…ì‚¬) ê¸¸ì´ 
        self.MIN_NEWS_COUNT = 5             # ë¶„ì„ì— í•„ìš”í•œ ìµœì†Œ ë‰´ìŠ¤ ê°œìˆ˜
        
        # í•„í„°ë§ ê¸°ì¤€
        self.MIN_UNIV_NEWS_COUNT = 2        # ëŒ€í•™êµë¡œ ë¶„ë¥˜ë˜ê¸° ìœ„í•œ ìµœì†Œ ë‰´ìŠ¤ ê°œìˆ˜
        self.MIN_CLUSTER_NEWS_COUNT = 3     # í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„ë¥˜ë˜ê¸° ìœ„í•œ ìµœì†Œ ë‰´ìŠ¤ ê°œìˆ˜
        self.MIN_MINOR_NEWS_COUNT = 2       # ì¤‘ë¶„ë¥˜ë¡œ ë¶„ë¥˜ë˜ê¸° ìœ„í•œ ìµœì†Œ ë‰´ìŠ¤ ê°œìˆ˜
        self.MAX_UNIV_DISPLAY = 5           # í‘œì‹œí•  ìµœëŒ€ ëŒ€í•™êµ ê°œìˆ˜
        self.MIN_MIDDLE_KEYWORDS_COUNT = 1  # ëŒ€ë¶„ë¥˜ë¡œ í‘œì‹œë˜ê¸° ìœ„í•œ ìµœì†Œ ì¤‘ë¶„ë¥˜ ê°œìˆ˜
        
        # ===== ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ (ì»´íŒŒì¼ëœ íŒ¨í„´ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”) =====
        self.uni_pattern = re.compile(r".+ëŒ€$")                    # ëŒ€í•™êµ íŒ¨í„´ (ì˜ˆ: ì„œìš¸ëŒ€, ì—°ì„¸ëŒ€)
        self.bracket_pattern = re.compile(r'\[.*?\]')              # ëŒ€ê´„í˜¸ ì œê±°ìš©
        self.parenthesis_pattern = re.compile(r'\(.*?\)')          # ì†Œê´„í˜¸ ì œê±°ìš©
        self.html_tag_pattern = re.compile(r'<.*?>')               # HTML íƒœê·¸ ì œê±°ìš©
        self.special_char_pattern = re.compile(r'[^\w\sê°€-í£]')     # íŠ¹ìˆ˜ë¬¸ì ì œê±°ìš©
        self.whitespace_pattern = re.compile(r'\s+')               # ì—°ì† ê³µë°± ì •ë¦¬ìš©
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        config_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "config")
        STOPWORDS_PATH = os.path.join(config_dir, "stopwords.txt")
        NON_UNIV_WORD_PATH = os.path.join(config_dir, "non_university_words.txt")
        
        # ë¶ˆìš©ì–´ì™€ ì œì™¸ ë‹¨ì–´ ë¡œë“œ
        self.stopwords = self._load_text_file_as_set(STOPWORDS_PATH, "ë¶ˆìš©ì–´")
        self.exclude_words = self._load_text_file_as_set(NON_UNIV_WORD_PATH, "ì œì™¸ ë‹¨ì–´")
        
        logger.info("âœ… TfidfClusterer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_text_file_as_set(self, file_path, file_description):
        """
        í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ setìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ê³µí†µ ë©”ì„œë“œ
        
        Args:
            file_path (str): ì½ì„ íŒŒì¼ ê²½ë¡œ
            file_description (str): íŒŒì¼ ì„¤ëª… (ì—ëŸ¬ ë©”ì‹œì§€ìš©)
            
        Returns:
            set: íŒŒì¼ ë‚´ìš©ì„ ì¤„ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ì§‘í•©
            
        Raises:
            FileNotFoundError: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œ
            PermissionError: íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ì„ ë•Œ
            UnicodeDecodeError: íŒŒì¼ ì¸ì½”ë”© ì˜¤ë¥˜
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return {line.strip() for line in f if line.strip()}
        except FileNotFoundError:
            logger.error(f"{file_description} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return set()
        except PermissionError:
            logger.error(f"{file_description} íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return set()
        except UnicodeDecodeError as e:
            logger.error(f"{file_description} íŒŒì¼ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
            return set()
        except Exception as e:
            logger.error(f"{file_description} ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return set()
    
    def extract_nouns(self, text):
        """
        KoNLPyì˜ Oktë¥¼ ì‚¬ìš©í•˜ì—¬ í˜•íƒœì†Œ ë¶„ì„ í›„ ëª…ì‚¬ ì¶”ì¶œ 
        """
        tokens = self.okt.pos(text, stem=True)
        nouns = [word for word, tag in tokens 
                if tag == "Noun" and word not in self.stopwords and len(word) >= self.MIN_WORD_LENGTH]
        return nouns
    
    def _extract_university_keyword(self, nouns):
        """
        ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ëŒ€í•™êµ í‚¤ì›Œë“œ ì¶”ì¶œ
        """
        university_keyword = next(
            (kw for kw in nouns if self.uni_pattern.match(kw) and kw not in self.exclude_words), 
            None
        )
        
        if not university_keyword and "KAIST" in nouns:
            return "KAIST"
        
        return university_keyword
    
    def preprocess_titles(self, news_data):
        """
        ë‰´ìŠ¤ ì œëª© ì „ì²˜ë¦¬
        """
        processed_titles = []
        
        for item in news_data:
            title = item["title"]
            
            title = self.bracket_pattern.sub('', title)
            title = self.parenthesis_pattern.sub('', title)
            title = self.html_tag_pattern.sub('', title)
            title = self.special_char_pattern.sub(' ', title)
            title = self.whitespace_pattern.sub(' ', title).strip()
            
            if len(title) > self.MIN_TITLE_LENGTH:
                processed_titles.append({
                    "original": item,
                    "cleaned_title": title
                })
        
        unique_titles = []
        seen_titles = set()
        
        for item in processed_titles:
            title = item["cleaned_title"]
            if title not in seen_titles:
                seen_titles.add(title)
                unique_titles.append(item)
        
        return unique_titles
    
    def split_news_by_uni_name(self, processed_data):
        """
        ëŒ€í•™êµ ì´ë¦„ìœ¼ë¡œ ë‰´ìŠ¤ ë¶„ë¥˜ 
        """
        university_news = defaultdict(list)
        other_news = []
        
        for item in processed_data:
            title = item["cleaned_title"]
            nouns = self.extract_nouns(title)
            university_keyword = self._extract_university_keyword(nouns)
            
            news_info = {
                "original": item["original"],
                "cleaned_title": title,
                "nouns": nouns
            }
            
            if university_keyword:
                university_news[university_keyword].append(news_info)
            else:
                other_news.append(news_info)
        
        return university_news, other_news
    
    def cluster_by_tfidf_cosine(self, news_data, min_keyword_count=5, score_threshold=0.2):
        """
        TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
        
        ì´ ë©”ì„œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
        1. ëª¨ë“  ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ê³„ì‚°
        2. ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ (min_keyword_count ì´ìƒ)
        3. ê° í‚¤ì›Œë“œë³„ í† í”½ ë²¡í„° ìƒì„± (TF-IDF)
        4. ê° ë‰´ìŠ¤ì™€ í† í”½ ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        5. ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ í† í”½ì— ë‹¨ì¼ í• ë‹¹ (score_threshold ì´ìƒ)
        """
        logger.info(f"ğŸ¯ TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {len(news_data)}ê°œ ë‰´ìŠ¤")
        
        # 1ë‹¨ê³„: ëª¨ë“  ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        all_keywords = []
        for item in news_data:
            nouns = self.extract_nouns(item.get("cleaned_title", ""))
            all_keywords.extend(nouns)
        
        # 2ë‹¨ê³„: í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚° ë° ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        keyword_counts = Counter(all_keywords)
        top_keywords = [kw for kw, count in keyword_counts.most_common() if count >= min_keyword_count]
        
        logger.info(f"ğŸ“Š ìƒìœ„ í‚¤ì›Œë“œ: {len(top_keywords)}ê°œ")
        
        if not top_keywords:
            logger.warning("ìƒìœ„ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
            return {}
        
        # 3ë‹¨ê³„: ê° í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” ë‰´ìŠ¤ë“¤ ìˆ˜ì§‘ (ì„±ëŠ¥ ê°œì„ )
        logger.info("ğŸ“Š í† í”½ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        topic_news = {}
        
        # ë¯¸ë¦¬ ëª¨ë“  ë‰´ìŠ¤ì˜ ëª…ì‚¬ë¥¼ ì¶”ì¶œ (í•œ ë²ˆë§Œ)
        news_nouns = []
        for item in news_data:
            nouns = self.extract_nouns(item.get("cleaned_title", ""))
            news_nouns.append(nouns)
        
        # ê° í‚¤ì›Œë“œì— ëŒ€í•´ ë¹ ë¥´ê²Œ ë§¤ì¹­
        for keyword in top_keywords:
            topic_news[keyword] = []
            for i, nouns in enumerate(news_nouns):
                if keyword in nouns:
                    topic_news[keyword].append(news_data[i])
            
            logger.info(f"   '{keyword}': {len(topic_news[keyword])}ê°œ ë‰´ìŠ¤")
        
        # 4ë‹¨ê³„: TF-IDF ë²¡í„°í™”
        logger.info("ğŸ“Š TF-IDF ë²¡í„°í™” ì¤‘...")
        all_texts = [item.get("cleaned_title", "") for item in news_data]
        
        try:
            # TF-IDF ë²¡í„°í™” (ëª…ì‚¬ ê¸°ë°˜)
            tfidf = TfidfVectorizer(
                max_features=500,  # 1000 â†’ 500ìœ¼ë¡œ ì¶•ì†Œ
                tokenizer=lambda x: self.extract_nouns(x),
                token_pattern=None,
                min_df=3,  # 2 â†’ 3ìœ¼ë¡œ ì¦ê°€ (ë” ì—„ê²©)
                max_df=0.7  # 0.8 â†’ 0.7ë¡œ ê°ì†Œ (ë” ì—„ê²©)
            )
            tfidf_matrix = tfidf.fit_transform(all_texts)
            logger.info(f"âœ… TF-IDF ë²¡í„°í™” ì™„ë£Œ: {tfidf_matrix.shape}")
            
            # 5ë‹¨ê³„: ê° í† í”½ì— ëŒ€í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ìµœì í™”)
            logger.info("ğŸ“Š ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
            clusters = defaultdict(list)
            assigned_count = 0
            
            # í† í”½ë³„ TF-IDF ë²¡í„°ë¥¼ ë¯¸ë¦¬ ê³„ì‚° (í•œ ë²ˆë§Œ)
            topic_vectors = {}
            for keyword in top_keywords:
                if keyword not in topic_news or len(topic_news[keyword]) < min_keyword_count:
                    logger.info(f"   '{keyword}' ì œì™¸: ë‰´ìŠ¤ ìˆ˜ ë¶€ì¡± ({len(topic_news.get(keyword, []))}ê°œ)")
                    continue
                
                # í† í”½ ë¬¸ì„œ ìƒì„±
                topic_texts = [news_item.get("cleaned_title", "") for news_item in topic_news[keyword]]
                topic_doc = " ".join(topic_texts)
                
                # í† í”½ ë²¡í„° ë¯¸ë¦¬ ê³„ì‚°
                topic_vector = tfidf.transform([topic_doc])
                if topic_vector.nnz > 0:
                    topic_vectors[keyword] = topic_vector
                    logger.info(f"   '{keyword}' í† í”½ ë²¡í„° ìƒì„± ì„±ê³µ (nnz: {topic_vector.nnz})")
                else:
                    logger.info(f"   '{keyword}' í† í”½ ë²¡í„° ìƒì„± ì‹¤íŒ¨ (nnz: 0)")
            
            logger.info(f"âœ… í† í”½ ë²¡í„° ê³„ì‚° ì™„ë£Œ: {len(topic_vectors)}ê°œ")
            
            # ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ ì¹´ìš´í„°
            processed_count = 0
            total_news = len(news_data)
            
            for i, item in enumerate(news_data):
                best_topic = None
                best_score = 0.0
                
                news_vector = tfidf_matrix[i:i+1]
                if news_vector.nnz == 0:
                    continue
                
                for keyword, topic_vector in topic_vectors.items():
                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    similarity = cosine_similarity(news_vector, topic_vector)[0][0]
                    
                    if similarity > best_score:
                        best_score = similarity
                        best_topic = keyword
                
                # ì„ê³„ê°’ ì´ìƒì´ë©´ í• ë‹¹
                if best_topic and best_score >= score_threshold:
                    clusters[best_topic].append(item)
                    assigned_count += 1
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ
                processed_count += 1
                if processed_count % 50 == 0:  # 100 â†’ 50ìœ¼ë¡œ ë³€ê²½ (ë” ìì£¼ í‘œì‹œ)
                    logger.info(f"   ì§„í–‰ë¥ : {processed_count}/{total_news} ({processed_count/total_news*100:.1f}%)")
            
            # 6ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° ì •ë¦¬
            final_clusters = {}
            cluster_id = 0
            
            for keyword, cluster_news in clusters.items():
                if len(cluster_news) >= min_keyword_count:
                    final_clusters[cluster_id] = {
                        "keyword": keyword,
                        "news": cluster_news,
                        "size": len(cluster_news)
                    }
                    logger.info(f"   í´ëŸ¬ìŠ¤í„° {cluster_id}: '{keyword}' ({len(cluster_news)}ê°œ ë‰´ìŠ¤)")
                    cluster_id += 1
            
            unassigned_count = len(news_data) - assigned_count
            logger.info(f"ğŸ“Š í• ë‹¹ ì™„ë£Œ: {assigned_count}ê°œ, ë¯¸í• ë‹¹: {unassigned_count}ê°œ")
            logger.info(f"ğŸ‰ TF-IDF ì½”ì‚¬ì¸ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(final_clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
            
            return final_clusters
            
        except Exception as e:
            logger.error(f"âŒ TF-IDF ì½”ì‚¬ì¸ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            # Fallback: ë¹ˆë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
            return self.cluster_by_keyword_frequency(news_data, min_keyword_count)
    
    def cluster_by_keyword_frequency(self, news_data, min_keyword_count=5):
        logger.info(f"ğŸ”¢ ë¹ˆë„ìˆ˜ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {len(news_data)}ê°œ ë‰´ìŠ¤")
        
        # 1ë‹¨ê³„: ëª¨ë“  ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        all_keywords = []
        for item in news_data:
            nouns = self.extract_nouns(item.get("cleaned_title", ""))
            all_keywords.extend(nouns)
        
        # 2ë‹¨ê³„: í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keyword_counts = Counter(all_keywords)
        
        # 3ë‹¨ê³„: ë¹ˆë„ê°€ ë†’ì€ í‚¤ì›Œë“œë¡œ ëŒ€ë¶„ë¥˜ ìƒì„±
        clusters = {}
        used_news_indices = set()
        cluster_id = 0
        
        for keyword, count in keyword_counts.most_common():
            if count < min_keyword_count:  # ìµœì†Œ ë¹ˆë„ ì´í•˜ëŠ” ì œì™¸
                break
            
            # ì´ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë‰´ìŠ¤ë“¤ ì°¾ê¸°
            cluster_news = []
            for i, item in enumerate(news_data):
                if i in used_news_indices:
                    continue
                
                nouns = self.extract_nouns(item.get("cleaned_title", ""))
                if keyword in nouns:
                    cluster_news.append(item)
                    used_news_indices.add(i)
            
            if len(cluster_news) >= min_keyword_count:
                clusters[cluster_id] = {
                    "keyword": keyword,
                    "news": cluster_news,
                    "size": len(cluster_news)
                }
                logger.info(f"   í´ëŸ¬ìŠ¤í„° {cluster_id}: '{keyword}' ({len(cluster_news)}ê°œ ë‰´ìŠ¤)")
                cluster_id += 1
        
        # 4ë‹¨ê³„: ì‚¬ìš©ë˜ì§€ ì•Šì€ ë‰´ìŠ¤ë“¤ì€ ì œì™¸ (ê¸°íƒ€ í´ëŸ¬ìŠ¤í„° ìƒì„± ì•ˆí•¨)
        unused_news = [news_data[i] for i in range(len(news_data)) if i not in used_news_indices]
        if unused_news:
            logger.info(f"   ë¯¸ë¶„ë¥˜ ë‰´ìŠ¤: {len(unused_news)}ê°œ (ì œì™¸)")
        
        logger.info(f"ğŸ‰ ë¹ˆë„ìˆ˜ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
        
        return clusters
    
    def create_subcategories_tfidf(self, cluster_news, max_subcategories=5):
        """
        TF-IDF ê¸°ë°˜ ì¤‘ë¶„ë¥˜ ìƒì„±
        - í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë‰´ìŠ¤ë§Œ ì¤‘ë¶„ë¥˜ì— í¬í•¨
        - TF-IDFë¡œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        """
        if len(cluster_news) < 6:  # ë‰´ìŠ¤ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì¤‘ë¶„ë¥˜ ìƒì„± ì•ˆí•¨
            return []
        
        # ì‘ì€ í´ëŸ¬ìŠ¤í„°(10ê°œ ì´í•˜)ëŠ” ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ë°”ë¡œ ì „í™˜
        if len(cluster_news) <= 10:
            return self.create_subcategories(cluster_news, max_subcategories)
        
        # TF-IDFë¡œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        texts = [item.get("cleaned_title", "") for item in cluster_news]
        
        try:
            # í´ëŸ¬ìŠ¤í„° í¬ê¸°ì— ë”°ë¼ min_df ë™ì  ì¡°ì •
            # ì‘ì€ í´ëŸ¬ìŠ¤í„°ëŠ” min_df=1, í° í´ëŸ¬ìŠ¤í„°ëŠ” min_df=2
            min_df_value = 1 if len(cluster_news) <= 15 else 2
            
            # TF-IDF ë²¡í„°í™”
            tfidf = TfidfVectorizer(
                max_features=50,
                tokenizer=lambda x: self.extract_nouns(x),
                token_pattern=None,
                min_df=min_df_value,  # ë™ì  ì¡°ì •
                max_df=0.9  # ë” ê´€ëŒ€í•˜ê²Œ (0.8 -> 0.9)
            )
            tfidf_matrix = tfidf.fit_transform(texts)
            feature_names = tfidf.get_feature_names_out()
            
            if len(feature_names) == 0:
                # íŠ¹ì„±ì´ ì—†ìœ¼ë©´ ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜
                return self.create_subcategories(cluster_news, max_subcategories)
            
            # TF-IDF ì ìˆ˜ í•©ì‚°
            tfidf_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()
            top_indices = tfidf_scores.argsort()[::-1][:max_subcategories]
            
            # ìƒìœ„ í‚¤ì›Œë“œë¡œ ì¤‘ë¶„ë¥˜ ìƒì„±
            subcategories = []
            used_news_indices = set()
            
            for idx in top_indices:
                keyword = feature_names[idx]
                
                # ì´ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë‰´ìŠ¤ë“¤ë§Œ ì°¾ê¸°
                subcategory_news = []
                for i, item in enumerate(cluster_news):
                    if i in used_news_indices:
                        continue
                    
                    nouns = self.extract_nouns(item.get("cleaned_title", ""))
                    if keyword in nouns:
                        subcategory_news.append(item)
                        used_news_indices.add(i)
                
                if len(subcategory_news) >= 2:
                    subcategories.append({
                        "keyword": keyword,
                        "news": subcategory_news,
                        "size": len(subcategory_news)
                    })
            
            # ì¤‘ë¶„ë¥˜ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ì¬ì‹œë„
            if len(subcategories) < 2:
                return self.create_subcategories(cluster_news, max_subcategories)
            
            return subcategories
            
        except Exception as e:
            # TF-IDF ì‹¤íŒ¨ ì‹œ ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜ (ê²½ê³  ë¡œê·¸ ì œê±°)
            return self.create_subcategories(cluster_news, max_subcategories)
    
    def create_subcategories(self, cluster_news, max_subcategories=5):
        """
        í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ì¤‘ë¶„ë¥˜ ìƒì„± (í‚¤ì›Œë“œ ë¹ˆë„ ê¸°ë°˜, fallbackìš©)
        """
        if len(cluster_news) < 6:  # ë‰´ìŠ¤ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì¤‘ë¶„ë¥˜ ìƒì„± ì•ˆí•¨
            return []
        
        # í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘
        all_keywords = []
        for item in cluster_news:
            nouns = self.extract_nouns(item.get("cleaned_title", ""))
            all_keywords.extend(nouns)
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keyword_counts = Counter(all_keywords)
        
        # ìƒìœ„ í‚¤ì›Œë“œë¡œ ì¤‘ë¶„ë¥˜ ìƒì„±
        subcategories = []
        used_news_indices = set()
        
        for keyword, count in keyword_counts.most_common(max_subcategories):
            if count < 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë‰´ìŠ¤ê°€ ìˆì–´ì•¼ ì¤‘ë¶„ë¥˜ë¡œ ì¸ì •
                break
            
            # ì´ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë‰´ìŠ¤ë“¤ ì°¾ê¸°
            subcategory_news = []
            for i, item in enumerate(cluster_news):
                if i in used_news_indices:
                    continue
                
                nouns = self.extract_nouns(item.get("cleaned_title", ""))
                
                if keyword in nouns:
                    subcategory_news.append(item)
                    used_news_indices.add(i)
            
            if len(subcategory_news) >= 2:
                subcategories.append({
                    "keyword": keyword,
                    "news": subcategory_news,
                    "size": len(subcategory_news)
                })
        
        return subcategories
    
    def normalize_keyword(self, keyword):
        """
        í‚¤ì›Œë“œì˜ ê³µë°±ì„ í•˜ì´í”ˆìœ¼ë¡œ ë³€ê²½ (news_analyzer.pyì™€ ë™ì¼)
        """
        return keyword.replace(" ", "-")
    
    def _format_news_item(self, news):
        """
        ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (news_analyzer.pyì™€ ë™ì¼)
        """
        return {
            "title": news["original"].get("title", news.get("cleaned_title", "Unknown")), 
            "link": news["original"].get("link", "")
        }
    
    def analyze_news(self, news_data):
        """
        ë‰´ìŠ¤ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜
        
        1. ë‰´ìŠ¤ ì œëª© ì „ì²˜ë¦¬ (ê´„í˜¸ ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬)
        2. ëŒ€í•™êµ ë‰´ìŠ¤ ë¶„ë¦¬ (ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜)
        3. ê¸°íƒ€ ë‰´ìŠ¤ TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í´ëŸ¬ìŠ¤í„°ë§
        4. ê° í´ëŸ¬ìŠ¤í„° ë‚´ TF-IDF ê¸°ë°˜ ì¤‘ë¶„ë¥˜ ìƒì„±
        5. í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ë³€í™˜
        """
        logger.info(f"ğŸš€ TF-IDF ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘: {len(news_data)}ê°œ ë‰´ìŠ¤")
        
        # ë‰´ìŠ¤ ì œëª© ì „ì²˜ë¦¬
        processed_data = self.preprocess_titles(news_data)
        
        if len(processed_data) < 10:  # ìµœì†Œ ë‰´ìŠ¤ ìˆ˜ ì²´í¬
            logger.warning("ë¶„ì„ ê°€ëŠ¥í•œ ë‰´ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")
            return None
        
        # 2ë‹¨ê³„: ëŒ€í•™êµ ë‰´ìŠ¤ ë¶„ë¦¬
        university_news, other_news = self.split_news_by_uni_name(processed_data)
        
        logger.info(f"ğŸ“Š ë¶„ë¥˜ ì™„ë£Œ: ëŒ€í•™êµ {len(university_news)}ê°œ ê·¸ë£¹, ê¸°íƒ€ {len(other_news)}ê°œ")
        
        # 3ë‹¨ê³„: ê¸°íƒ€ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ (TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)
        clusters = self.cluster_by_tfidf_cosine(other_news, min_keyword_count=5, score_threshold=0.4)
        
        # 4ë‹¨ê³„: í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        result = []
        
        # ëŒ€í•™êµ ë‰´ìŠ¤ ì²˜ë¦¬
        if university_news:
            # ëŒ€í•™êµ ë°ì´í„° í•„í„°ë§ ë° ì •ë ¬ (ë‰´ìŠ¤ ìˆ˜ ê¸°ì¤€)
            filtered_universities = {
                university_name: news_list 
                for university_name, news_list in university_news.items() 
                if len(news_list) >= self.MIN_UNIV_NEWS_COUNT
            }
            
            # ë‰´ìŠ¤ ìˆ˜ê°€ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œë§Œ ì„ íƒ
            sorted_items = sorted(filtered_universities.items(), key=lambda x: len(x[1]), reverse=True)
            sorted_universities = dict(sorted_items[:self.MAX_UNIV_DISPLAY])
            
            if sorted_universities:
                # ëŒ€í•™êµë³„ ì¤‘ë¶„ë¥˜ ìƒì„±
                univ_middle_keywords = [
                    {
                        "middleKeyword": self.normalize_keyword(uni_name),
                        "relatedNews": [self._format_news_item(news) for news in news_list]
                    }
                    for uni_name, news_list in sorted_universities.items()
                ]
                
                # ìµœì†Œ ì¤‘ë¶„ë¥˜ ìˆ˜ í™•ì¸ í›„ ëŒ€ë¶„ë¥˜ ì¶”ê°€
                if len(univ_middle_keywords) >= self.MIN_MIDDLE_KEYWORDS_COUNT:
                    result.append({
                        "majorKeyword": self.normalize_keyword("ëŒ€í•™êµ"),
                        "middleKeywords": univ_middle_keywords,
                        "otherNews": []
                    })
        
        # ì¼ë°˜ í´ëŸ¬ìŠ¤í„° ì²˜ë¦¬
        for cluster_id, cluster_data in clusters.items():
            keyword = cluster_data["keyword"]
            news = cluster_data["news"]
            
            # ì¤‘ë¶„ë¥˜ ìƒì„± (TF-IDF ê¸°ë°˜)
            subcategories = self.create_subcategories_tfidf(news)
            
            # ì¤‘ë¶„ë¥˜ê°€ ìˆìœ¼ë©´ ì¤‘ë¶„ë¥˜ë¡œ, ì—†ìœ¼ë©´ ê¸°íƒ€ ë‰´ìŠ¤ë¡œ
            if subcategories and len(subcategories) > 1:  # ì¤‘ë¶„ë¥˜ê°€ 2ê°œ ì´ìƒì´ì–´ì•¼ í•¨
                middle_keywords = []
                other_news_in_cluster = []
                
                for subcat in subcategories:
                    if subcat["keyword"] == "ê¸°íƒ€":
                        other_news_in_cluster = [self._format_news_item(news) for news in subcat["news"]]
                    else:
                        middle_keywords.append({
                            "middleKeyword": subcat["keyword"],
                            "relatedNews": [self._format_news_item(news) for news in subcat["news"]]
                        })
                
                # ì¤‘ë¶„ë¥˜ê°€ ìˆê³  ê¸°íƒ€ ë‰´ìŠ¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if middle_keywords or other_news_in_cluster:
                    result.append({
                        "majorKeyword": keyword,
                        "middleKeywords": middle_keywords,
                        "otherNews": other_news_in_cluster
                    })
            else:
                # ì¤‘ë¶„ë¥˜ê°€ ì—†ê±°ë‚˜ 1ê°œë¿ì´ë©´ í•´ë‹¹ ëŒ€ë¶„ë¥˜ëŠ” ì œì™¸
                logger.info(f"   ëŒ€ë¶„ë¥˜ '{keyword}' ì œì™¸: ì¤‘ë¶„ë¥˜ {len(subcategories) if subcategories else 0}ê°œ (ìµœì†Œ 2ê°œ í•„ìš”)")
        
        logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {len(result)}ê°œ ëŒ€ë¶„ë¥˜ ìƒì„±")
        
        return result

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    test_tfidf_clusterer()
