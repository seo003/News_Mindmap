#!/usr/bin/env python3
"""
FastText + K-Means ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë“ˆ

TF-IDF ëŒ€ì‹  FastText ì„ë² ë”©ê³¼ K-Meansë¥¼ ì‚¬ìš©í•œ ì•ˆì •ì ì¸ í´ëŸ¬ìŠ¤í„°ë§
"""

import logging
from collections import Counter, defaultdict
from konlpy.tag import Okt
import re
import os
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import fasttext
import tempfile

logger = logging.getLogger(__name__)

class FastTextClusterer:
    """
    FastText + K-Means ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ í´ë˜ìŠ¤
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.okt = Okt()
        
        # ê¸°ì¡´ NewsAnalyzerì™€ ë™ì¼í•œ ì„¤ì •
        self.MIN_WORD_LENGTH = 2
        self.MIN_TITLE_LENGTH = 10
        
        # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        self.bracket_pattern = re.compile(r'\[.*?\]')
        self.parenthesis_pattern = re.compile(r'\(.*?\)')
        self.html_tag_pattern = re.compile(r'<[^>]+>')
        self.special_char_pattern = re.compile(r'[^\w\sê°€-í£]')
        self.whitespace_pattern = re.compile(r'\s+')
        
        # ëŒ€í•™êµ íŒ¨í„´ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        self.uni_pattern = re.compile(r'.*ëŒ€(í•™êµ|í•™ì›)?$')
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì • (backend/config í´ë”)
        # __file__: backend/analysis/fasttext_clusterer.py
        # dirname(__file__): backend/analysis
        # dirname(dirname(__file__)): backend
        backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        config_dir = os.path.join(backend_dir, "config")
        STOPWORDS_PATH = os.path.join(config_dir, "stopwords.txt")
        NON_UNIV_WORD_PATH = os.path.join(config_dir, "non_university_words.txt")
        
        # ê²½ë¡œ í™•ì¸ ë¡œê·¸
        logger.debug(f"Config ë””ë ‰í† ë¦¬: {config_dir}")
        logger.debug(f"Stopwords ê²½ë¡œ: {STOPWORDS_PATH}")
        logger.debug(f"Non-univ words ê²½ë¡œ: {NON_UNIV_WORD_PATH}")
        
        # ë¶ˆìš©ì–´ì™€ ì œì™¸ ë‹¨ì–´ ë¡œë“œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        self.stopwords = self._load_text_file_as_set(STOPWORDS_PATH, "ë¶ˆìš©ì–´")
        self.exclude_words = self._load_text_file_as_set(NON_UNIV_WORD_PATH, "ì œì™¸ ë‹¨ì–´")
        
        # FastText ëª¨ë¸ ì´ˆê¸°í™”
        self.fasttext_model = None
        self._init_fasttext()
        
        logger.info("âœ… FastTextClusterer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_text_file_as_set(self, file_path, file_description):
        """
        í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ setìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ê³µí†µ ë©”ì„œë“œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return {line.strip() for line in f if line.strip()}
        except FileNotFoundError:
            logger.error(f"{file_description} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return set()
        except PermissionError:
            logger.error(f"{file_description} íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return set()
        except UnicodeDecodeError as e:
            logger.error(f"{file_description} íŒŒì¼ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
            return set()
        except Exception as e:
            logger.error(f"{file_description} ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return set()
    
    def _init_fasttext(self):
        """FastText ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # í•œêµ­ì–´ FastText ëª¨ë¸ ë¡œë“œ
            # models í´ë”ì—ì„œ ë¨¼ì € ì°¾ê³ , ì—†ìœ¼ë©´ í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
            import os
            model_paths = [
                'models/cc.ko.300.bin',
                'cc.ko.300.bin',
                os.path.join(os.path.dirname(os.path.dirname(__file__)), 'models', 'cc.ko.300.bin')
            ]
            
            model_path = None
            for path in model_paths:
                if os.path.exists(path):
                    model_path = path
                    break
            
            if model_path:
                self.fasttext_model = fasttext.load_model(model_path)
                logger.info(f"âœ… í•œêµ­ì–´ FastText ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            else:
                raise FileNotFoundError("FastText ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. models/cc.ko.300.bin ë˜ëŠ” cc.ko.300.bin íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        except Exception as e:
            logger.warning(f"âš ï¸ FastText ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ”„ ê°„ë‹¨í•œ ì„ë² ë”© ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´...")
            self.fasttext_model = None
    
    def extract_nouns(self, text):
        """
        KoNLPyì˜ Oktë¥¼ ì‚¬ìš©í•˜ì—¬ í˜•íƒœì†Œ ë¶„ì„ í›„ ëª…ì‚¬ ì¶”ì¶œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        """
        tokens = self.okt.pos(text, stem=True)
        nouns = [word for word, tag in tokens 
                if tag == "Noun" and word not in self.stopwords and len(word) >= self.MIN_WORD_LENGTH]
        return nouns
    
    def _extract_university_keyword(self, nouns):
        """
        ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ëŒ€í•™êµ í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        """
        university_keyword = next(
            (kw for kw in nouns if self.uni_pattern.match(kw) and kw not in self.exclude_words), 
            None
        )
        
        if not university_keyword and "KAIST" in nouns:
            return "KAIST"
        
        return university_keyword
    
    def preprocess_titles(self, news_data):
        """
        ë‰´ìŠ¤ ì œëª© ì „ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        """
        processed_titles = []
        
        for item in news_data:
            title = item["title"]
            
            title = self.bracket_pattern.sub('', title)
            title = self.parenthesis_pattern.sub('', title)
            title = self.html_tag_pattern.sub('', title)
            title = self.special_char_pattern.sub(' ', title)
            title = self.whitespace_pattern.sub(' ', title).strip()
            
            if len(title) > self.MIN_TITLE_LENGTH:
                processed_titles.append({
                    "original": item,
                    "cleaned_title": title
                })
        
        unique_titles = []
        seen_titles = set()
        
        for item in processed_titles:
            title = item["cleaned_title"]
            if title not in seen_titles:
                seen_titles.add(title)
                unique_titles.append(item)
        
        return unique_titles
    
    def split_news_by_uni_name(self, processed_data):
        """
        ëŒ€í•™êµ ì´ë¦„ìœ¼ë¡œ ë‰´ìŠ¤ ë¶„ë¥˜ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        """
        university_news = defaultdict(list)
        other_news = []
        
        for item in processed_data:
            title = item["cleaned_title"]
            nouns = self.extract_nouns(title)
            university_keyword = self._extract_university_keyword(nouns)
            
            news_info = {
                "original": item["original"],
                "cleaned_title": title,
                "nouns": nouns
            }
            
            if university_keyword:
                university_news[university_keyword].append(news_info)
            else:
                other_news.append(news_info)
        
        return university_news, other_news
    
    def get_text_embedding(self, text):
        """
        í…ìŠ¤íŠ¸ì˜ FastText ì„ë² ë”© ìƒì„±
        
        Args:
            text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            np.array: ì„ë² ë”© ë²¡í„°
        """
        if self.fasttext_model:
            # FastText ëª¨ë¸ ì‚¬ìš©
            return self.fasttext_model.get_sentence_vector(text)
        else:
            # Fallback: ë‹¨ìˆœí•œ ë‹¨ì–´ ë²¡í„° í‰ê· 
            words = text.split()
            if not words:
                return np.zeros(300)
            
            # ê° ë‹¨ì–´ì˜ í‰ê·  ë²¡í„° (ëœë¤ ë²¡í„°ë¡œ ëŒ€ì²´)
            np.random.seed(hash(text) % 2**32)  # í…ìŠ¤íŠ¸ë³„ë¡œ ì¼ê´€ëœ ëœë¤
            return np.random.randn(300)
    
    def cluster_with_fasttext_kmeans(self, news_data, n_clusters=None):
        """
        FastText ì„ë² ë”© + K-Meansë¡œ í´ëŸ¬ìŠ¤í„°ë§
        
        Args:
            news_data (list): ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            n_clusters (int): í´ëŸ¬ìŠ¤í„° ìˆ˜ (Noneì´ë©´ ìë™ ê²°ì •)
            
        Returns:
            dict: í´ëŸ¬ìŠ¤í„° ê²°ê³¼
        """
        logger.info(f"ğŸš€ FastText + K-Means í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {len(news_data)}ê°œ ë‰´ìŠ¤")
        
        if len(news_data) < 10:
            logger.warning("ë‰´ìŠ¤ê°€ ë„ˆë¬´ ì ì–´ì„œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {0: {"keyword": "ê¸°íƒ€", "news": news_data, "size": len(news_data)}}
        
        # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        logger.info("ğŸ“Š FastText ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = []
        texts = []
        
        for item in news_data:
            text = item.get("cleaned_title", "")
            if text.strip():
                embedding = self.get_text_embedding(text)
                embeddings.append(embedding)
                texts.append(text)
        
        if not embeddings:
            logger.warning("ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {0: {"keyword": "ê¸°íƒ€", "news": news_data, "size": len(news_data)}}
        
        embeddings = np.array(embeddings)
        logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape}")
        
        # 2ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
        if n_clusters is None:
            # ë°ì´í„° í¬ê¸°ì— ë”°ë¼ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
            n_data = len(news_data)
            if n_data < 50:
                n_clusters = min(3, n_data // 10)
            elif n_data < 200:
                n_clusters = min(8, n_data // 20)
            else:
                n_clusters = min(15, n_data // 30)
            
            n_clusters = max(2, n_clusters)  # ìµœì†Œ 2ê°œ
        
        logger.info(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}ê°œ")
        
        # 3ë‹¨ê³„: K-Means í´ëŸ¬ìŠ¤í„°ë§
        logger.info("ğŸ”¢ K-Means í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ì¤‘...")
        try:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(embeddings)
            
            logger.info(f"âœ… K-Means ì™„ë£Œ: {len(set(cluster_labels))}ê°œ í´ëŸ¬ìŠ¤í„°")
            
        except Exception as e:
            logger.error(f"âŒ K-Means ì‹¤íŒ¨: {e}")
            return {0: {"keyword": "ê¸°íƒ€", "news": news_data, "size": len(news_data)}}
        
        # 4ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì •ë¦¬
        logger.info("ğŸ·ï¸ í´ëŸ¬ìŠ¤í„°ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        clusters = {}
        
        for cluster_id in range(n_clusters):
            # í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë‰´ìŠ¤ë“¤
            cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]
            cluster_news = [news_data[i] for i in cluster_indices]
            
            if len(cluster_news) < 3:  # ë„ˆë¬´ ì ì€ í´ëŸ¬ìŠ¤í„°ëŠ” ê±´ë„ˆë›°ê¸°
                continue
            
            # í´ëŸ¬ìŠ¤í„° ë‚´ í‚¤ì›Œë“œ ì¶”ì¶œ
            cluster_keywords = []
            for item in cluster_news:
                nouns = self.extract_nouns(item.get("cleaned_title", ""))
                cluster_keywords.extend(nouns)
            
            # ê°€ì¥ ë¹ˆë²ˆí•œ í‚¤ì›Œë“œ ì„ íƒ
            if cluster_keywords:
                keyword_counts = Counter(cluster_keywords)
                main_keyword = keyword_counts.most_common(1)[0][0]
            else:
                main_keyword = f"í´ëŸ¬ìŠ¤í„°_{cluster_id}"
            
            clusters[cluster_id] = {
                "keyword": main_keyword,
                "news": cluster_news,
                "size": len(cluster_news)
            }
            
            logger.info(f"   í´ëŸ¬ìŠ¤í„° {cluster_id}: '{main_keyword}' ({len(cluster_news)}ê°œ ë‰´ìŠ¤)")
        
        # 5ë‹¨ê³„: ì‚¬ìš©ë˜ì§€ ì•Šì€ ë‰´ìŠ¤ë“¤ì„ 'ê¸°íƒ€' í´ëŸ¬ìŠ¤í„°ë¡œ
        used_indices = set()
        for cluster_data in clusters.values():
            for item in cluster_data["news"]:
                if item in news_data:
                    used_indices.add(news_data.index(item))
        
        unused_news = [news_data[i] for i in range(len(news_data)) if i not in used_indices]
        if unused_news:
            clusters[len(clusters)] = {
                "keyword": "ê¸°íƒ€",
                "news": unused_news,
                "size": len(unused_news)
            }
            logger.info(f"   í´ëŸ¬ìŠ¤í„° {len(clusters)-1}: 'ê¸°íƒ€' ({len(unused_news)}ê°œ ë‰´ìŠ¤)")
        
        logger.info(f"ğŸ‰ FastText + K-Means í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
        
        return clusters
    
    def create_subcategories(self, cluster_news, max_subcategories=5):
        """
        í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ì¤‘ë¶„ë¥˜ ìƒì„± (FastText ê¸°ë°˜)
        """
        if len(cluster_news) < 6:  # ë‰´ìŠ¤ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì¤‘ë¶„ë¥˜ ìƒì„± ì•ˆí•¨
            return []
        
        # í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘
        all_keywords = []
        for item in cluster_news:
            nouns = self.extract_nouns(item.get("cleaned_title", ""))
            all_keywords.extend(nouns)
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keyword_counts = Counter(all_keywords)
        
        # ìƒìœ„ í‚¤ì›Œë“œë¡œ ì¤‘ë¶„ë¥˜ ìƒì„±
        subcategories = []
        used_news_indices = set()
        
        for keyword, count in keyword_counts.most_common(max_subcategories):
            if count < 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë‰´ìŠ¤ê°€ ìˆì–´ì•¼ ì¤‘ë¶„ë¥˜ë¡œ ì¸ì •
                break
            
            # ì´ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë‰´ìŠ¤ë“¤ ì°¾ê¸°
            subcategory_news = []
            for i, item in enumerate(cluster_news):
                if i in used_news_indices:
                    continue
                
                nouns = self.extract_nouns(item.get("cleaned_title", ""))
                
                if keyword in nouns:
                    subcategory_news.append(item)
                    used_news_indices.add(i)
            
            if len(subcategory_news) >= 2:
                subcategories.append({
                    "keyword": keyword,
                    "news": subcategory_news,
                    "size": len(subcategory_news)
                })
        
        # ì‚¬ìš©ë˜ì§€ ì•Šì€ ë‰´ìŠ¤ë“¤ì„ 'ê¸°íƒ€' ì¤‘ë¶„ë¥˜ë¡œ
        unused_news = [cluster_news[i] for i in range(len(cluster_news)) if i not in used_news_indices]
        if unused_news:
            subcategories.append({
                "keyword": "ê¸°íƒ€",
                "news": unused_news,
                "size": len(unused_news)
            })
        
        return subcategories
    
    def analyze_news(self, news_data):
        """
        ë‰´ìŠ¤ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜ (FastText + K-Means ê¸°ë°˜)
        """
        logger.info(f"ğŸš€ FastText + K-Means ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘: {len(news_data)}ê°œ ë‰´ìŠ¤")
        
        # 1ë‹¨ê³„: ì „ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        processed_data = self.preprocess_titles(news_data)
        
        if len(processed_data) < 10:  # ìµœì†Œ ë‰´ìŠ¤ ìˆ˜ ì²´í¬
            logger.warning("ë¶„ì„ ê°€ëŠ¥í•œ ë‰´ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")
            return None
        
        # 2ë‹¨ê³„: ëŒ€í•™êµ ë‰´ìŠ¤ ë¶„ë¦¬ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        university_news, other_news = self.split_news_by_uni_name(processed_data)
        
        logger.info(f"ğŸ“Š ë¶„ë¥˜ ì™„ë£Œ: ëŒ€í•™êµ {len(university_news)}ê°œ ê·¸ë£¹, ê¸°íƒ€ {len(other_news)}ê°œ")
        
        # 3ë‹¨ê³„: ê¸°íƒ€ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ (FastText + K-Means)
        clusters = self.cluster_with_fasttext_kmeans(other_news)
        
        # 4ë‹¨ê³„: í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        result = []
        
        # ëŒ€í•™êµ ë‰´ìŠ¤ ì¶”ê°€ (í•˜ë‚˜ì˜ "ëŒ€í•™êµ" ëŒ€ë¶„ë¥˜ë¡œ í†µí•©)
        if university_news:
            # ëª¨ë“  ëŒ€í•™êµ ë‰´ìŠ¤ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
            all_university_news = []
            for uni_name, uni_news_list in university_news.items():
                all_university_news.extend(uni_news_list)
            
            # "ëŒ€í•™êµ" ëŒ€ë¶„ë¥˜ë¡œ ì¶”ê°€
            result.append({
                "majorKeyword": "ëŒ€í•™êµ",
                "middleKeywords": [],
                "otherNews": all_university_news
            })
        
        # í´ëŸ¬ìŠ¤í„°ë“¤ ì¶”ê°€
        for cluster_id, cluster_data in clusters.items():
            keyword = cluster_data["keyword"]
            news = cluster_data["news"]
            
            # ì¤‘ë¶„ë¥˜ ìƒì„±
            subcategories = self.create_subcategories(news)
            
            # ì¤‘ë¶„ë¥˜ê°€ ìˆìœ¼ë©´ ì¤‘ë¶„ë¥˜ë¡œ, ì—†ìœ¼ë©´ ê¸°íƒ€ ë‰´ìŠ¤ë¡œ
            if subcategories:
                middle_keywords = []
                other_news_in_cluster = []
                
                for subcat in subcategories:
                    if subcat["keyword"] == "ê¸°íƒ€":
                        other_news_in_cluster = subcat["news"]
                    else:
                        middle_keywords.append({
                            "middleKeyword": subcat["keyword"],
                            "relatedNews": subcat["news"]
                        })
                
                result.append({
                    "majorKeyword": keyword,
                    "middleKeywords": middle_keywords,
                    "otherNews": other_news_in_cluster
                })
            else:
                result.append({
                    "majorKeyword": keyword,
                    "middleKeywords": [],
                    "otherNews": news
                })
        
        logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {len(result)}ê°œ ëŒ€ë¶„ë¥˜ ìƒì„±")
        
        return result


def test_fasttext_clusterer():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    import json
    import os
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    json_file_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "test_news_1000.json")
    
    if not os.path.exists(json_file_path):
        print(f"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file_path}")
        return
    
    with open(json_file_path, 'r', encoding='utf-8') as f:
        json_data = json.load(f)
    
    news_data = json_data.get('news_data', [])[:100]  # ì²˜ìŒ 100ê°œë§Œ í…ŒìŠ¤íŠ¸
    
    # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
    clusterer = FastTextClusterer()
    result = clusterer.analyze_news(news_data)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š FastText + K-Means í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")
    print("=" * 80)
    
    for major_idx, major_cat in enumerate(result, 1):
        major_name = major_cat.get('majorKeyword', 'Unknown')
        middle_keywords = major_cat.get('middleKeywords', [])
        other_news = major_cat.get('otherNews', [])
        
        total_news = sum(len(mid.get('relatedNews', [])) for mid in middle_keywords) + len(other_news)
        print(f"\nğŸ“ ëŒ€ë¶„ë¥˜ {major_idx}: {major_name} (ì´ {total_news}ê°œ ë‰´ìŠ¤)")
        
        if middle_keywords:
            for middle_idx, middle_cat in enumerate(middle_keywords, 1):
                middle_name = middle_cat.get('middleKeyword', 'Unknown')
                related_news = middle_cat.get('relatedNews', [])
                print(f"   â”œâ”€ ì¤‘ë¶„ë¥˜ {middle_idx}: {middle_name} ({len(related_news)}ê°œ ë‰´ìŠ¤)")
        
        if other_news:
            print(f"   â””â”€ ê¸°íƒ€ ë‰´ìŠ¤: {len(other_news)}ê°œ")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    test_fasttext_clusterer()
