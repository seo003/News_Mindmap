#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ë¹ˆë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë“ˆ

ë³µì¡í•œ TF-IDF, HDBSCAN, K-Means ëŒ€ì‹ 
ìˆœìˆ˜ ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ë¶„ë¥˜/ì¤‘ë¶„ë¥˜ë¥¼ êµ¬í•˜ëŠ” ê°„ë‹¨í•œ ë°©ë²•
"""

import logging
from collections import Counter, defaultdict
from konlpy.tag import Okt
import re
import os

logger = logging.getLogger(__name__)

class SimpleClusterer:
    """
    ê°„ë‹¨í•œ ë¹ˆë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ í´ë˜ìŠ¤
    ê¸°ì¡´ NewsAnalyzerì˜ ëŒ€í•™êµ ë¶„ë¥˜ ë¡œì§ì„ ì¬ì‚¬ìš©
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.okt = Okt()
        
        # ê¸°ì¡´ NewsAnalyzerì™€ ë™ì¼í•œ ì„¤ì •
        self.MIN_WORD_LENGTH = 2
        self.MIN_TITLE_LENGTH = 10
        
        # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        self.bracket_pattern = re.compile(r'\[.*?\]')
        self.parenthesis_pattern = re.compile(r'\(.*?\)')
        self.html_tag_pattern = re.compile(r'<[^>]+>')
        self.special_char_pattern = re.compile(r'[^\w\sê°€-í£]')
        self.whitespace_pattern = re.compile(r'\s+')
        
        # ëŒ€í•™êµ íŒ¨í„´ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        self.uni_pattern = re.compile(r'.*ëŒ€(í•™êµ|í•™ì›)?$')
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì • (backend/config í´ë”)
        # __file__: backend/analysis/simple_clusterer.py
        # dirname(__file__): backend/analysis
        # dirname(dirname(__file__)): backend
        backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        config_dir = os.path.join(backend_dir, "config")
        STOPWORDS_PATH = os.path.join(config_dir, "stopwords.txt")
        NON_UNIV_WORD_PATH = os.path.join(config_dir, "non_university_words.txt")
        
        # ë¶ˆìš©ì–´ì™€ ì œì™¸ ë‹¨ì–´ ë¡œë“œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        self.stopwords = self._load_text_file_as_set(STOPWORDS_PATH, "ë¶ˆìš©ì–´")
        self.exclude_words = self._load_text_file_as_set(NON_UNIV_WORD_PATH, "ì œì™¸ ë‹¨ì–´")
        
        logger.info("âœ… SimpleClusterer ì´ˆê¸°í™” ì™„ë£Œ (ê¸°ì¡´ NewsAnalyzer ë¡œì§ ì¬ì‚¬ìš©)")
    
    def _load_text_file_as_set(self, file_path, file_description):
        """
        í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ setìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ê³µí†µ ë©”ì„œë“œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return {line.strip() for line in f if line.strip()}
        except FileNotFoundError:
            logger.error(f"{file_description} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return set()
        except PermissionError:
            logger.error(f"{file_description} íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return set()
        except UnicodeDecodeError as e:
            logger.error(f"{file_description} íŒŒì¼ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
            return set()
        except Exception as e:
            logger.error(f"{file_description} ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return set()
    
    def extract_nouns(self, text):
        """
        KoNLPyì˜ Oktë¥¼ ì‚¬ìš©í•˜ì—¬ í˜•íƒœì†Œ ë¶„ì„ í›„ ëª…ì‚¬ ì¶”ì¶œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        """
        tokens = self.okt.pos(text, stem=True)
        nouns = [word for word, tag in tokens 
                if tag == "Noun" and word not in self.stopwords and len(word) >= self.MIN_WORD_LENGTH]
        return nouns
    
    def _extract_university_keyword(self, nouns):
        """
        ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ëŒ€í•™êµ í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        """
        university_keyword = next(
            (kw for kw in nouns if self.uni_pattern.match(kw) and kw not in self.exclude_words), 
            None
        )
        
        if not university_keyword and "KAIST" in nouns:
            return "KAIST"
        
        return university_keyword
    
    def preprocess_titles(self, news_data):
        """
        ë‰´ìŠ¤ ì œëª© ì „ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        """
        processed_titles = []
        
        for item in news_data:
            title = item["title"]
            
            title = self.bracket_pattern.sub('', title)
            title = self.parenthesis_pattern.sub('', title)
            title = self.html_tag_pattern.sub('', title)
            title = self.special_char_pattern.sub(' ', title)
            title = self.whitespace_pattern.sub(' ', title).strip()
            
            if len(title) > self.MIN_TITLE_LENGTH:
                processed_titles.append({
                    "original": item,
                    "cleaned_title": title
                })
        
        unique_titles = []
        seen_titles = set()
        
        for item in processed_titles:
            title = item["cleaned_title"]
            if title not in seen_titles:
                seen_titles.add(title)
                unique_titles.append(item)
        
        return unique_titles
    
    def split_news_by_uni_name(self, processed_data):
        """
        ëŒ€í•™êµ ì´ë¦„ìœ¼ë¡œ ë‰´ìŠ¤ ë¶„ë¥˜ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        """
        university_news = defaultdict(list)
        other_news = []
        
        for item in processed_data:
            title = item["cleaned_title"]
            nouns = self.extract_nouns(title)
            university_keyword = self._extract_university_keyword(nouns)
            
            news_info = {
                "original": item["original"],
                "cleaned_title": title,
                "nouns": nouns
            }
            
            if university_keyword:
                university_news[university_keyword].append(news_info)
            else:
                other_news.append(news_info)
        
        return university_news, other_news
    
    def extract_keywords(self, text, min_length=2):
        """
        í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ëª…ì‚¬ë§Œ) - ê¸°ì¡´ NewsAnalyzer ë¡œì§ ì‚¬ìš©
        """
        return self.extract_nouns(text)
    
    def cluster_by_keyword_frequency(self, news_data, min_cluster_size=3):
        """
        í‚¤ì›Œë“œ ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§
        
        Args:
            news_data (list): ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            min_cluster_size (int): ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°
            
        Returns:
            dict: í´ëŸ¬ìŠ¤í„° ê²°ê³¼
        """
        logger.info(f"ğŸ”¢ í‚¤ì›Œë“œ ë¹ˆë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {len(news_data)}ê°œ ë‰´ìŠ¤")
        
        # 1ë‹¨ê³„: ëª¨ë“  ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        all_keywords = []
        news_keywords = []
        
        for item in news_data:
            title = item.get("title", "")
            keywords = self.extract_keywords(title)
            news_keywords.append(keywords)
            all_keywords.extend(keywords)
        
        logger.info(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: ì´ {len(all_keywords)}ê°œ í‚¤ì›Œë“œ")
        
        # 2ë‹¨ê³„: í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keyword_counts = Counter(all_keywords)
        logger.info(f"ğŸ“Š ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜: {len(keyword_counts)}ê°œ")
        
        # 3ë‹¨ê³„: ìƒìœ„ í‚¤ì›Œë“œë¡œ í´ëŸ¬ìŠ¤í„° ìƒì„±
        clusters = {}
        cluster_id = 0
        used_news_indices = set()
        
        # ë¹ˆë„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ í‚¤ì›Œë“œë“¤
        top_keywords = keyword_counts.most_common(50)  # ìƒìœ„ 50ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
        
        for keyword, count in top_keywords:
            if count < min_cluster_size:
                break  # ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¯¸ë§Œì´ë©´ ì¤‘ë‹¨
            
            # ì´ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë‰´ìŠ¤ë“¤ ì°¾ê¸°
            cluster_news = []
            for i, keywords in enumerate(news_keywords):
                if i in used_news_indices:
                    continue
                
                if keyword in keywords:
                    cluster_news.append(news_data[i])
                    used_news_indices.add(i)
            
            # í´ëŸ¬ìŠ¤í„° í¬ê¸°ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ ì €ì¥
            if len(cluster_news) >= min_cluster_size:
                clusters[cluster_id] = {
                    "keyword": keyword,
                    "news": cluster_news,
                    "size": len(cluster_news)
                }
                cluster_id += 1
                logger.info(f"   í´ëŸ¬ìŠ¤í„° {cluster_id-1}: '{keyword}' ({len(cluster_news)}ê°œ ë‰´ìŠ¤)")
        
        # 4ë‹¨ê³„: ì‚¬ìš©ë˜ì§€ ì•Šì€ ë‰´ìŠ¤ë“¤ì„ 'ê¸°íƒ€' í´ëŸ¬ìŠ¤í„°ë¡œ
        unused_news = [news_data[i] for i in range(len(news_data)) if i not in used_news_indices]
        
        if unused_news:
            clusters[cluster_id] = {
                "keyword": "ê¸°íƒ€",
                "news": unused_news,
                "size": len(unused_news)
            }
            logger.info(f"   í´ëŸ¬ìŠ¤í„° {cluster_id}: 'ê¸°íƒ€' ({len(unused_news)}ê°œ ë‰´ìŠ¤)")
        
        logger.info(f"ğŸ‰ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
        
        return clusters
    
    def create_subcategories(self, cluster_news, max_subcategories=5):
        """
        í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ì¤‘ë¶„ë¥˜ ìƒì„±
        
        Args:
            cluster_news (list): í´ëŸ¬ìŠ¤í„° ë‚´ ë‰´ìŠ¤ë“¤
            max_subcategories (int): ìµœëŒ€ ì¤‘ë¶„ë¥˜ ê°œìˆ˜
            
        Returns:
            list: ì¤‘ë¶„ë¥˜ ë¦¬ìŠ¤íŠ¸
        """
        if len(cluster_news) < 6:  # ë‰´ìŠ¤ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì¤‘ë¶„ë¥˜ ìƒì„± ì•ˆí•¨
            return []
        
        # í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘
        all_keywords = []
        for item in cluster_news:
            title = item.get("title", "")
            keywords = self.extract_keywords(title)
            all_keywords.extend(keywords)
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keyword_counts = Counter(all_keywords)
        
        # ìƒìœ„ í‚¤ì›Œë“œë¡œ ì¤‘ë¶„ë¥˜ ìƒì„±
        subcategories = []
        used_news_indices = set()
        
        for keyword, count in keyword_counts.most_common(max_subcategories):
            if count < 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë‰´ìŠ¤ê°€ ìˆì–´ì•¼ ì¤‘ë¶„ë¥˜ë¡œ ì¸ì •
                break
            
            # ì´ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë‰´ìŠ¤ë“¤ ì°¾ê¸°
            subcategory_news = []
            for i, item in enumerate(cluster_news):
                if i in used_news_indices:
                    continue
                
                title = item.get("title", "")
                keywords = self.extract_keywords(title)
                
                if keyword in keywords:
                    subcategory_news.append(item)
                    used_news_indices.add(i)
            
            if len(subcategory_news) >= 2:
                subcategories.append({
                    "keyword": keyword,
                    "news": subcategory_news,
                    "size": len(subcategory_news)
                })
        
        # ì‚¬ìš©ë˜ì§€ ì•Šì€ ë‰´ìŠ¤ë“¤ì„ 'ê¸°íƒ€' ì¤‘ë¶„ë¥˜ë¡œ
        unused_news = [cluster_news[i] for i in range(len(cluster_news)) if i not in used_news_indices]
        if unused_news:
            subcategories.append({
                "keyword": "ê¸°íƒ€",
                "news": unused_news,
                "size": len(unused_news)
            })
        
        return subcategories
    
    def analyze_news(self, news_data):
        """
        ë‰´ìŠ¤ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜ (ê¸°ì¡´ NewsAnalyzer ë¡œì§ ì¬ì‚¬ìš©)
        
        Args:
            news_data (list): ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"ğŸš€ ê°„ë‹¨í•œ ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘: {len(news_data)}ê°œ ë‰´ìŠ¤")
        
        # 1ë‹¨ê³„: ì „ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        processed_data = self.preprocess_titles(news_data)
        
        if len(processed_data) < 10:  # ìµœì†Œ ë‰´ìŠ¤ ìˆ˜ ì²´í¬
            logger.warning("ë¶„ì„ ê°€ëŠ¥í•œ ë‰´ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")
            return None
        
        # 2ë‹¨ê³„: ëŒ€í•™êµ ë‰´ìŠ¤ ë¶„ë¦¬ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        university_news, other_news = self.split_news_by_uni_name(processed_data)
        
        logger.info(f"ğŸ“Š ë¶„ë¥˜ ì™„ë£Œ: ëŒ€í•™êµ {len(university_news)}ê°œ ê·¸ë£¹, ê¸°íƒ€ {len(other_news)}ê°œ")
        
        # 3ë‹¨ê³„: ê¸°íƒ€ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ (ê°„ë‹¨í•œ ë¹ˆë„ ê¸°ë°˜)
        clusters = self.cluster_by_keyword_frequency(other_news)
        
        # 4ë‹¨ê³„: í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        result = []
        
        # ëŒ€í•™êµ ë‰´ìŠ¤ ì¶”ê°€ (í•˜ë‚˜ì˜ "ëŒ€í•™êµ" ëŒ€ë¶„ë¥˜ë¡œ í†µí•©)
        if university_news:
            # ëª¨ë“  ëŒ€í•™êµ ë‰´ìŠ¤ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
            all_university_news = []
            for uni_name, uni_news_list in university_news.items():
                all_university_news.extend(uni_news_list)
            
            # "ëŒ€í•™êµ" ëŒ€ë¶„ë¥˜ë¡œ ì¶”ê°€
            result.append({
                "majorKeyword": "ëŒ€í•™êµ",
                "middleKeywords": [],
                "otherNews": all_university_news
            })
        
        # í´ëŸ¬ìŠ¤í„°ë“¤ ì¶”ê°€
        for cluster_id, cluster_data in clusters.items():
            keyword = cluster_data["keyword"]
            news = cluster_data["news"]
            
            # ì¤‘ë¶„ë¥˜ ìƒì„±
            subcategories = self.create_subcategories(news)
            
            # ì¤‘ë¶„ë¥˜ê°€ ìˆìœ¼ë©´ ì¤‘ë¶„ë¥˜ë¡œ, ì—†ìœ¼ë©´ ê¸°íƒ€ ë‰´ìŠ¤ë¡œ
            if subcategories:
                middle_keywords = []
                other_news_in_cluster = []
                
                for subcat in subcategories:
                    if subcat["keyword"] == "ê¸°íƒ€":
                        other_news_in_cluster = subcat["news"]
                    else:
                        middle_keywords.append({
                            "middleKeyword": subcat["keyword"],
                            "relatedNews": subcat["news"]
                        })
                
                result.append({
                    "majorKeyword": keyword,
                    "middleKeywords": middle_keywords,
                    "otherNews": other_news_in_cluster
                })
            else:
                result.append({
                    "majorKeyword": keyword,
                    "middleKeywords": [],
                    "otherNews": news
                })
        
        logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {len(result)}ê°œ ëŒ€ë¶„ë¥˜ ìƒì„±")
        
        return result


def test_simple_clusterer():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    import json
    import os
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    json_file_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "test_news_1000.json")
    
    if not os.path.exists(json_file_path):
        print(f"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file_path}")
        return
    
    with open(json_file_path, 'r', encoding='utf-8') as f:
        json_data = json.load(f)
    
    news_data = json_data.get('news_data', [])[:100]  # ì²˜ìŒ 100ê°œë§Œ í…ŒìŠ¤íŠ¸
    
    # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
    clusterer = SimpleClusterer()
    result = clusterer.analyze_news(news_data)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")
    print("=" * 80)
    
    for major_idx, major_cat in enumerate(result, 1):
        major_name = major_cat.get('majorKeyword', 'Unknown')
        middle_keywords = major_cat.get('middleKeywords', [])
        other_news = major_cat.get('otherNews', [])
        
        total_news = sum(len(mid.get('relatedNews', [])) for mid in middle_keywords) + len(other_news)
        print(f"\nğŸ“ ëŒ€ë¶„ë¥˜ {major_idx}: {major_name} (ì´ {total_news}ê°œ ë‰´ìŠ¤)")
        
        if middle_keywords:
            for middle_idx, middle_cat in enumerate(middle_keywords, 1):
                middle_name = middle_cat.get('middleKeyword', 'Unknown')
                related_news = middle_cat.get('relatedNews', [])
                print(f"   â”œâ”€ ì¤‘ë¶„ë¥˜ {middle_idx}: {middle_name} ({len(related_news)}ê°œ ë‰´ìŠ¤)")
        
        if other_news:
            print(f"   â””â”€ ê¸°íƒ€ ë‰´ìŠ¤: {len(other_news)}ê°œ")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    test_simple_clusterer()
