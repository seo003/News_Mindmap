import os
import re
import logging
from collections import defaultdict
import numpy as np
from sentence_transformers import SentenceTransformer
import hdbscan
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from konlpy.tag import Okt
from keybert import KeyBERT
from config.config import STOPWORDS_PATH, NON_UNIV_WORD_PATH

logger = logging.getLogger(__name__)


class NewsAnalyzer:
    """
    ë‰´ìŠ¤ ë¶„ì„ í´ë˜ìŠ¤
    
    ë‰´ìŠ¤ ì „ì²˜ë¦¬, í´ëŸ¬ìŠ¤í„°ë§, í‚¤ì›Œë“œ ì¶”ì¶œ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    # ========== ìƒìˆ˜ ì •ì˜ ==========
    # í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê´€ë ¨
    MIN_TITLE_LENGTH = 10          # ìµœì†Œ ì œëª© ê¸¸ì´
    MIN_WORD_LENGTH = 2            # ìµœì†Œ ë‹¨ì–´(ëª…ì‚¬) ê¸¸ì´
    MIN_NEWS_COUNT = 5             # ë¶„ì„ì— í•„ìš”í•œ ìµœì†Œ ë‰´ìŠ¤ ê°œìˆ˜
    
    # í•„í„°ë§ ê¸°ì¤€
    MIN_UNIV_NEWS_COUNT = 2        # ëŒ€í•™êµë¡œ ë¶„ë¥˜ë˜ê¸° ìœ„í•œ ìµœì†Œ ë‰´ìŠ¤ ê°œìˆ˜
    MIN_CLUSTER_NEWS_COUNT = 3     # í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„ë¥˜ë˜ê¸° ìœ„í•œ ìµœì†Œ ë‰´ìŠ¤ ê°œìˆ˜ (8â†’3ìœ¼ë¡œ ì™„í™”)
    MIN_MINOR_NEWS_COUNT = 2       # ì¤‘ë¶„ë¥˜ë¡œ ë¶„ë¥˜ë˜ê¸° ìœ„í•œ ìµœì†Œ ë‰´ìŠ¤ ê°œìˆ˜ (4â†’2ë¡œ ì™„í™”)
    MAX_UNIV_DISPLAY = 5           # í‘œì‹œí•  ìµœëŒ€ ëŒ€í•™êµ ê°œìˆ˜
    MIN_MIDDLE_KEYWORDS_COUNT = 1  # ëŒ€ë¶„ë¥˜ë¡œ í‘œì‹œë˜ê¸° ìœ„í•œ ìµœì†Œ ì¤‘ë¶„ë¥˜ ê°œìˆ˜ (2â†’1ë¡œ ì™„í™”)
    
    # í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²• ì„¤ì • ('graph_based', 'frequency_based', 'advanced')
    CLUSTERING_METHOD = 'frequency_based'   # ë©”ì¸ ë§ˆì¸ë“œë§µ ê¸°ë³¸ê°’: ë¹ˆë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
    
    # í´ëŸ¬ìŠ¤í„°ë§ ê´€ë ¨ (ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ìœ„í•œ ì¡°ì •)
    HDBSCAN_MIN_CLUSTER_SIZE = 20          # HDBSCAN ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° (25â†’20ìœ¼ë¡œ ì™„í™”)
    HDBSCAN_MIN_SAMPLES = 12               # HDBSCAN ìµœì†Œ ìƒ˜í”Œ ìˆ˜ (15â†’12ë¡œ ì™„í™”)
    HDBSCAN_EPSILON = 0.3                 # HDBSCAN í´ëŸ¬ìŠ¤í„° ì„ íƒ ì—¡ì‹¤ë¡  (0.15â†’0.3ìœ¼ë¡œ ì™„í™”)
    CLUSTER_DUPLICATE_THRESHOLD = 0.5      # í´ëŸ¬ìŠ¤í„° ì¤‘ë³µ ë¹„ìœ¨ ì„ê³„ê°’
    MIN_NOISE_FOR_RECLUSTERING = 5         # ì¬í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ìµœì†Œ ë…¸ì´ì¦ˆ ê°œìˆ˜
    
    # K-Means í´ëŸ¬ìŠ¤í„°ë§ íŒŒë¼ë¯¸í„° (ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ìœ„í•œ ì¡°ì •)
    KMEANS_SMALL_DATA_THRESHOLD = 20       # ì†Œê·œëª¨ ë°ì´í„° ì„ê³„ê°’
    KMEANS_MEDIUM_DATA_THRESHOLD = 100     # ì¤‘ê·œëª¨ ë°ì´í„° ì„ê³„ê°’
    KMEANS_SMALL_MAX_CLUSTERS = 3          # ì†Œê·œëª¨ ë°ì´í„° ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ (2â†’3)
    KMEANS_SMALL_DIVISOR = 8               # ì†Œê·œëª¨ ë°ì´í„° í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚° ì œìˆ˜ (10â†’8)
    KMEANS_MEDIUM_MAX_CLUSTERS = 6         # ì¤‘ê·œëª¨ ë°ì´í„° ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ (3â†’6)
    KMEANS_MEDIUM_DIVISOR = 20             # ì¤‘ê·œëª¨ ë°ì´í„° í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚° ì œìˆ˜ (30â†’20)
    KMEANS_LARGE_MAX_CLUSTERS = 10         # ëŒ€ê·œëª¨ ë°ì´í„° ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ (5â†’10)
    KMEANS_LARGE_DIVISOR = 30              # ëŒ€ê·œëª¨ ë°ì´í„° í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚° ì œìˆ˜ (50â†’30)
    KMEANS_RANDOM_STATE = 42               # K-Means ëœë¤ ì‹œë“œ
    KMEANS_N_INIT = 10                     # K-Means ì´ˆê¸°í™” íšŸìˆ˜
    
    # TF-IDF íŒŒë¼ë¯¸í„° (ë”ìš± ì™„í™”ëœ ì„¤ì •)
    TFIDF_NGRAM_MIN = 1            # TF-IDF n-gram ìµœì†Œê°’
    TFIDF_NGRAM_MAX = 3            # TF-IDF n-gram ìµœëŒ€ê°’
    TFIDF_MAX_FEATURES = 3000      # TF-IDF ìµœëŒ€ íŠ¹ì„± ìˆ˜ (5000â†’3000ìœ¼ë¡œ ê°ì†Œ)
    TFIDF_MIN_DF = 1               # TF-IDF ìµœì†Œ ë¬¸ì„œ ë¹ˆë„ (1ë¡œ ìœ ì§€)
    TFIDF_MAX_DF = 0.99            # TF-IDF ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„ (0.95â†’0.99ë¡œ ë” ì™„í™”)
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ ê´€ë ¨
    TOP_KEYWORDS_COUNT = 5         # ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
    KEYBERT_TOP_K = 3              # KeyBERT í‚¤ì›Œë“œ ê°œìˆ˜
    KEYBERT_NGRAM_MIN = 1          # KeyBERT n-gram ìµœì†Œê°’
    KEYBERT_NGRAM_MAX = 3          # KeyBERT n-gram ìµœëŒ€ê°’
    KEYBERT_DIVERSITY = 0.5        # KeyBERT MMR ë‹¤ì–‘ì„± íŒŒë¼ë¯¸í„°
    TFIDF_CATEGORY_KEYWORDS = 20   # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ìš© TF-IDF í‚¤ì›Œë“œ ê°œìˆ˜
    
    def __init__(self):
        """ë‰´ìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.kw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')
        self.okt = Okt()
        
        self.stopwords = self.load_stopwords()
        self.exclude_words = self.load_exclude_words()
        
        self.uni_pattern = re.compile(r".+ëŒ€$")
        self.bracket_pattern = re.compile(r'\[.*?\]')
        self.parenthesis_pattern = re.compile(r'\(.*?\)')
        self.html_tag_pattern = re.compile(r'<.*?>')
        self.special_char_pattern = re.compile(r'[^\w\sê°€-í£]')
        self.whitespace_pattern = re.compile(r'\s+')
        
    def _load_text_file_as_set(self, file_path, file_description):
        """
        í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ setìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ê³µí†µ ë©”ì„œë“œ
        
        Args:
            file_path (str): íŒŒì¼ ê²½ë¡œ
            file_description (str): íŒŒì¼ ì„¤ëª… (ë¡œê¹…ìš©)
            
        Returns:
            set: íŒŒì¼ ë‚´ìš©ì„ setìœ¼ë¡œ ë³€í™˜í•œ ê²°ê³¼
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return {line.strip() for line in f if line.strip()}
        except FileNotFoundError:
            logger.error(f"{file_description} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return set()
        except PermissionError:
            logger.error(f"{file_description} íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return set()
        except UnicodeDecodeError as e:
            logger.error(f"{file_description} íŒŒì¼ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
            return set()
        except Exception as e:
            logger.error(f"{file_description} ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return set()
    
    def load_stopwords(self):
        """
        ë¶ˆìš©ì–´ íŒŒì¼ ë¡œë“œ
        
        Returns:
            set: ë¶ˆìš©ì–´ ì§‘í•©
        """
        return self._load_text_file_as_set(STOPWORDS_PATH, "ë¶ˆìš©ì–´")
    
    def load_exclude_words(self):
        """
        ëŒ€í•™êµê°€ ì•„ë‹Œ ì œì™¸ ë‹¨ì–´ ëª©ë¡ ë¡œë“œ
        
        Returns:
            set: ì œì™¸ ë‹¨ì–´ ì§‘í•©
        """
        return self._load_text_file_as_set(NON_UNIV_WORD_PATH, "ì œì™¸ ë‹¨ì–´")
    
    def extract_nouns(self, text):
        """
        KoNLPyì˜ Oktë¥¼ ì‚¬ìš©í•˜ì—¬ í˜•íƒœì†Œ ë¶„ì„ í›„ ëª…ì‚¬ ì¶”ì¶œ
        
        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            list: ì¶”ì¶œëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        tokens = self.okt.pos(text, stem=True)
        nouns = [word for word, tag in tokens 
                if tag == "Noun" and word not in self.stopwords and len(word) >= self.MIN_WORD_LENGTH]
        return nouns
    
    def _extract_university_keyword(self, nouns):
        """
        ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ëŒ€í•™êµ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            nouns (list): ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            str or None: ëŒ€í•™êµ í‚¤ì›Œë“œ (ì—†ìœ¼ë©´ None)
        """
        university_keyword = next(
            (kw for kw in nouns if self.uni_pattern.match(kw) and kw not in self.exclude_words), 
            None
        )
        
        if not university_keyword and "KAIST" in nouns:
            return "KAIST"
        
        return university_keyword
    
    def split_news_by_uni_name(self, processed_data):
        """
        ëŒ€í•™êµ ì´ë¦„ìœ¼ë¡œ ë‰´ìŠ¤ ë¶„ë¥˜
        
        Args:
            processed_data (list): ì „ì²˜ë¦¬ëœ ë‰´ìŠ¤ ë°ì´í„°
            
        Returns:
            tuple: (ëŒ€í•™êµë³„ ë‰´ìŠ¤ ë”•ì…”ë„ˆë¦¬, ê¸°íƒ€ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸)
        """
        university_news = defaultdict(list)
        other_news = []
        
        for item in processed_data:
            title = item["cleaned_title"]
            nouns = self.extract_nouns(title)
            university_keyword = self._extract_university_keyword(nouns)
            
            news_info = {
                "original": item["original"],
                "cleaned_title": title,
                "nouns": nouns
            }
            
            if university_keyword:
                university_news[university_keyword].append(news_info)
            else:
                other_news.append(news_info)
        
        return university_news, other_news
    
    def preprocess_titles(self, news_data):
        """
        ë‰´ìŠ¤ ì œëª© ì „ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì¤‘ë³µ ì œê±°)
        
        Args:
            news_data (list): ì›ë³¸ ë‰´ìŠ¤ ë°ì´í„°
            
        Returns:
            list: ì „ì²˜ë¦¬ëœ ë‰´ìŠ¤ ë°ì´í„°
        """
        processed_titles = []
        
        for item in news_data:
            title = item["title"]
            
            title = self.bracket_pattern.sub('', title)
            title = self.parenthesis_pattern.sub('', title)
            title = self.html_tag_pattern.sub('', title)
            title = self.special_char_pattern.sub(' ', title)
            title = self.whitespace_pattern.sub(' ', title).strip()
            
            if len(title) > self.MIN_TITLE_LENGTH:
                processed_titles.append({
                    "original": item,
                    "cleaned_title": title
                })
        
        unique_titles = []
        seen_titles = set()
        
        for item in processed_titles:
            title = item["cleaned_title"]
            if title not in seen_titles:
                seen_titles.add(title)
                unique_titles.append(item)
        
        return unique_titles
    
    def extract_keywords_with_konlpy_tfidf(self, texts, topn=5):
        """
        KoNLPy + TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ì›ë³¸ ë³µì›)
        
        Args:
            texts (list): í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            topn (int): ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            list: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        if not texts or len(texts) == 0:
            return []
        
        noun_texts = [" ".join(self.extract_nouns(text)) for text in texts]
        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        noun_texts = [text for text in noun_texts if text.strip()]
        
        if not noun_texts:
            return []
        
        # ë‰´ìŠ¤ ìˆ˜ê°€ ì ìœ¼ë©´ (5ê°œ ì´í•˜) TF-IDF ëŒ€ì‹  ë¹ˆë„ ê¸°ë°˜ ì‚¬ìš©
        if len(noun_texts) <= 5:
            from collections import Counter
            all_nouns = []
            for text in noun_texts:
                if text.strip():
                    all_nouns.extend(text.split())
            
            if not all_nouns:
                return []
            
            noun_counts = Counter(all_nouns)
            top_keywords = [word for word, count in noun_counts.most_common(topn)]
            return top_keywords
        
        vectorizer = TfidfVectorizer(
            ngram_range=(self.TFIDF_NGRAM_MIN, self.TFIDF_NGRAM_MAX),
            max_features=self.TFIDF_MAX_FEATURES,
            min_df=self.TFIDF_MIN_DF,
            max_df=self.TFIDF_MAX_DF
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(noun_texts)
            feature_names = vectorizer.get_feature_names_out()
            
            if len(feature_names) == 0:
                # ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ fallback
                return self._extract_keywords_by_frequency(noun_texts, topn)
            
            mean_scores = np.asarray(tfidf_matrix.mean(axis=0)).ravel()
            top_indices = mean_scores.argsort()[-topn:][::-1]
            
            return [feature_names[i] for i in top_indices]
        except ValueError as e:
            # TF-IDF ì‹¤íŒ¨ ì‹œ ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ fallback (ê²½ê³  ë¡œê·¸ ì œê±°)
            return self._extract_keywords_by_frequency(noun_texts, topn)
    
    def _extract_keywords_by_frequency(self, noun_texts, topn=5):
        """
        ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (fallback ë©”ì„œë“œ)
        
        Args:
            noun_texts (list): ëª…ì‚¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            topn (int): ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            list: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        from collections import Counter
        all_nouns = []
        for text in noun_texts:
            if text.strip():
                all_nouns.extend(text.split())
        
        if not all_nouns:
            return []
        
        noun_counts = Counter(all_nouns)
        top_keywords = [word for word, count in noun_counts.most_common(topn)]
        return top_keywords
    
    def calculate_kmeans_clusters(self, n_data):
        """
        ë°ì´í„° ê°œìˆ˜ì— ë”°ë¼ ì ì ˆí•œ K-Means í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°
        
        Args:
            n_data (int): ë°ì´í„° ê°œìˆ˜
            
        Returns:
            int: ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
        """
        if n_data < self.KMEANS_SMALL_DATA_THRESHOLD:
            return min(self.KMEANS_SMALL_MAX_CLUSTERS, n_data // self.KMEANS_SMALL_DIVISOR)
        elif n_data < self.KMEANS_MEDIUM_DATA_THRESHOLD:
            return min(self.KMEANS_MEDIUM_MAX_CLUSTERS, n_data // self.KMEANS_MEDIUM_DIVISOR)
        else:
            return min(self.KMEANS_LARGE_MAX_CLUSTERS, n_data // self.KMEANS_LARGE_DIVISOR)
    
    def _perform_hdbscan_clustering(self, embeddings):
        """
        HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        
        Args:
            embeddings: ë‰´ìŠ¤ ì„ë² ë”© ë²¡í„°
            
        Returns:
            array: í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”
        """
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=self.HDBSCAN_MIN_CLUSTER_SIZE,
            min_samples=self.HDBSCAN_MIN_SAMPLES,
            metric="euclidean",  # cosine â†’ euclideanìœ¼ë¡œ ë˜ëŒë¦¼ (HDBSCANì´ cosineì„ ì§€ì›í•˜ì§€ ì•ŠìŒ)
            cluster_selection_method='eom',  # ë” ê´€ëŒ€í•œ í´ëŸ¬ìŠ¤í„° ì„ íƒ
            cluster_selection_epsilon=self.HDBSCAN_EPSILON,
            prediction_data=True,
            alpha=1.0,  # í´ëŸ¬ìŠ¤í„° ì„ íƒì„ ë” ê´€ëŒ€í•˜ê²Œ
            allow_single_cluster=True  # ë‹¨ì¼ í´ëŸ¬ìŠ¤í„° í—ˆìš©
        )
        return clusterer.fit_predict(embeddings)
    
    def _is_meaningful_cluster(self, cluster_news):
        """
        í´ëŸ¬ìŠ¤í„°ê°€ ì˜ë¯¸ìˆëŠ”ì§€ íŒë‹¨
        
        Args:
            cluster_news (list): í´ëŸ¬ìŠ¤í„° ë‚´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            tuple: (ì˜ë¯¸ìˆëŠ”ì§€ ì—¬ë¶€, ì‚¬ìœ )
        """
        if len(cluster_news) < self.MIN_CLUSTER_NEWS_COUNT:
            return False, f"í¬ê¸° ë¶€ì¡± ({len(cluster_news)}ê°œ)"
        
        titles = [news['cleaned_title'] for news in cluster_news]
        unique_titles = set(titles)
        duplicate_ratio = (len(titles) - len(unique_titles)) / len(titles)
        
        if duplicate_ratio >= self.CLUSTER_DUPLICATE_THRESHOLD:
            return False, f"ì¤‘ë³µ ë¹„ìœ¨ ë†’ìŒ ({duplicate_ratio:.1%})"
        
        return True, "ì •ìƒ"
    
    def _filter_meaningful_clusters(self, hdbscan_clusters, titles_data, embeddings):
        """
        ì˜ë¯¸ìˆëŠ” í´ëŸ¬ìŠ¤í„°ë§Œ í•„í„°ë§í•˜ê³  ì˜ë¯¸ì—†ëŠ” ê²ƒë“¤ì€ ë…¸ì´ì¦ˆë¡œ ì´ë™
        
        Args:
            hdbscan_clusters (dict): HDBSCAN í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬
            titles_data (list): ì „ì²´ ë‰´ìŠ¤ ë°ì´í„°
            embeddings: ì „ì²´ ì„ë² ë”© ë²¡í„°
            
        Returns:
            tuple: (ì˜ë¯¸ìˆëŠ” í´ëŸ¬ìŠ¤í„°, ë…¸ì´ì¦ˆ ë°ì´í„°, ë…¸ì´ì¦ˆ ì„ë² ë”©)
        """
        meaningful_clusters = {}
        noise_data = []
        noise_embeddings = []
        
        item_to_idx = {id(item): idx for idx, item in enumerate(titles_data)}
        
        for cluster_id, cluster_news in hdbscan_clusters.items():
            is_meaningful, reason = self._is_meaningful_cluster(cluster_news)
            
            if is_meaningful:
                meaningful_clusters[cluster_id] = cluster_news
            else:
                noise_data.extend(cluster_news)
                for item in cluster_news:
                    item_idx = item_to_idx.get(id(item))
                    if item_idx is not None:
                        noise_embeddings.append(embeddings[item_idx])
        
        return meaningful_clusters, noise_data, noise_embeddings
    
    def _perform_kmeans_clustering(self, data, embeddings):
        """
        K-Means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        
        Args:
            data (list): í´ëŸ¬ìŠ¤í„°ë§í•  ë°ì´í„°
            embeddings: ì„ë² ë”© ë²¡í„°
            
        Returns:
            dict: í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬
        """
        n_clusters = self.calculate_kmeans_clusters(len(data))
        
        if n_clusters < 2:
            return {}
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=self.KMEANS_RANDOM_STATE, n_init=self.KMEANS_N_INIT)
        kmeans_labels = kmeans.fit_predict(embeddings)
        
        clusters = defaultdict(list)
        for label, item in zip(kmeans_labels, data):
            clusters[label].append(item)
        
        return dict(clusters)
    
    def _recluster_noise_with_kmeans(self, noise_data, noise_embeddings, existing_clusters):
        """
        ë…¸ì´ì¦ˆ ë°ì´í„°ë¥¼ K-Meansë¡œ ì¬í´ëŸ¬ìŠ¤í„°ë§
        
        Args:
            noise_data (list): ë…¸ì´ì¦ˆ ë°ì´í„°
            noise_embeddings: ë…¸ì´ì¦ˆ ì„ë² ë”©
            existing_clusters (dict): ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°
            
        Returns:
            dict: ì—…ë°ì´íŠ¸ëœ í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬
        """
        if len(noise_data) <= self.MIN_NOISE_FOR_RECLUSTERING:
            logger.info(f"ë…¸ì´ì¦ˆ ë°ì´í„°ê°€ ì ì–´ K-Means ì¬ë¶„ë¥˜ ìƒëµ (ë°ì´í„° ê°œìˆ˜: {len(noise_data)})")
            return existing_clusters
        
        n_clusters = self.calculate_kmeans_clusters(len(noise_data))
        
        if n_clusters < 2:
            logger.info(f"ë…¸ì´ì¦ˆ ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ K-Means ì¬ë¶„ë¥˜ ìƒëµ (í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters})")
            return existing_clusters
        
        logger.info(f"ğŸ”„ K-Means ì¬ë¶„ë¥˜ ì‹œì‘: {len(noise_data)}ê°œ ë…¸ì´ì¦ˆ â†’ {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=self.KMEANS_RANDOM_STATE, n_init=self.KMEANS_N_INIT)
        kmeans_labels = kmeans.fit_predict(noise_embeddings)
        
        max_cluster_id = max(existing_clusters.keys()) if existing_clusters else -1
        
        updated_clusters = existing_clusters.copy()
        new_clusters = defaultdict(list)
        
        for label, item in zip(kmeans_labels, noise_data):
            new_cluster_id = max_cluster_id + 1 + label
            if new_cluster_id not in updated_clusters:
                updated_clusters[new_cluster_id] = []
            updated_clusters[new_cluster_id].append(item)
            new_clusters[label].append(item)
        
        # ìƒˆë¡œ ìƒì„±ëœ í´ëŸ¬ìŠ¤í„°ë³„ ë‰´ìŠ¤ ì œëª© ë¡œê·¸
        logger.info("ğŸ†• K-Meansë¡œ ìƒˆë¡œ ìƒì„±ëœ í´ëŸ¬ìŠ¤í„°:")
        for label, cluster_items in new_clusters.items():
            new_cluster_id = max_cluster_id + 1 + label
            logger.info(f"   ğŸ“ í´ëŸ¬ìŠ¤í„° {new_cluster_id}: {len(cluster_items)}ê°œ ë‰´ìŠ¤")
            for i, item in enumerate(cluster_items[:3]):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                title = item.get('cleaned_title', 'Unknown')[:50] + "..." if len(item.get('cleaned_title', '')) > 50 else item.get('cleaned_title', 'Unknown')
                logger.info(f"      {i+1}. {title}")
            if len(cluster_items) > 3:
                logger.info(f"      ... ì™¸ {len(cluster_items) - 3}ê°œ")
        
        logger.info(f"âœ… K-Means ì¬ë¶„ë¥˜ ì™„ë£Œ: {len(new_clusters)}ê°œ ìƒˆ í´ëŸ¬ìŠ¤í„° ìƒì„±")
        return updated_clusters
    
    def enhanced_cluster_news(self, titles_data):
        """
        HDBSCAN + K-Means í•˜ì´ë¸Œë¦¬ë“œ í´ëŸ¬ìŠ¤í„°ë§
        
        1ë‹¨ê³„ì—ì„œ HDBSCANìœ¼ë¡œ ë°€ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ê³ ,
        2ë‹¨ê³„ì—ì„œ ë…¸ì´ì¦ˆ ë°ì´í„°ë¥¼ K-Meansë¡œ ì¬ë¶„ë¥˜í•©ë‹ˆë‹¤.
        
        Args:
            titles_data (list): í´ëŸ¬ìŠ¤í„°ë§í•  ë‰´ìŠ¤ ë°ì´í„°
            
        Returns:
            tuple: (í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬, ë…¸ì´ì¦ˆ ë¦¬ìŠ¤íŠ¸)
        """
        titles = [item["cleaned_title"] for item in titles_data]
        n_data = len(titles_data)
        
        logger.info(f"ğŸ” í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {n_data}ê°œ ë‰´ìŠ¤")
        
        embeddings = self.embedding_model.encode(titles, normalize_embeddings=True)
        
        logger.info("ğŸ“Š 1ë‹¨ê³„: HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘...")
        hdbscan_labels = self._perform_hdbscan_clustering(embeddings)
        
        hdbscan_clusters = defaultdict(list)
        noise_data = []
        noise_embeddings = []
        
        if hdbscan_labels is not None:
            for label, item, embedding in zip(hdbscan_labels, titles_data, embeddings):
                if label == -1:
                    noise_data.append(item)
                    noise_embeddings.append(embedding)
                else:
                    hdbscan_clusters[label].append(item)
            
            # HDBSCAN ê²°ê³¼ ë¡œê·¸
            n_hdbscan_clusters = len(hdbscan_clusters)
            n_hdbscan_noise = len(noise_data)
            logger.info(f"âœ… HDBSCAN ì™„ë£Œ: {n_hdbscan_clusters}ê°œ í´ëŸ¬ìŠ¤í„°, {n_hdbscan_noise}ê°œ ë…¸ì´ì¦ˆ")
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ë‰´ìŠ¤ ì œëª© ë¡œê·¸
            for cluster_id, cluster_items in hdbscan_clusters.items():
                logger.info(f"   ğŸ“ í´ëŸ¬ìŠ¤í„° {cluster_id}: {len(cluster_items)}ê°œ ë‰´ìŠ¤")
                for i, item in enumerate(cluster_items[:3]):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                    title = item.get('cleaned_title', 'Unknown')[:50] + "..." if len(item.get('cleaned_title', '')) > 50 else item.get('cleaned_title', 'Unknown')
                    logger.info(f"      {i+1}. {title}")
                if len(cluster_items) > 3:
                    logger.info(f"      ... ì™¸ {len(cluster_items) - 3}ê°œ")
            
            meaningful_clusters, filtered_noise, filtered_noise_emb = self._filter_meaningful_clusters(
                hdbscan_clusters, titles_data, embeddings
            )
            noise_data.extend(filtered_noise)
            noise_embeddings.extend(filtered_noise_emb)
            hdbscan_clusters = meaningful_clusters
            
            logger.info(f"ğŸ” ì˜ë¯¸ìˆëŠ” í´ëŸ¬ìŠ¤í„° í•„í„°ë§ í›„: {len(hdbscan_clusters)}ê°œ í´ëŸ¬ìŠ¤í„°, {len(noise_data)}ê°œ ë…¸ì´ì¦ˆ")
        else:
            noise_data = list(titles_data)
            noise_embeddings = list(embeddings)
            logger.warning(f"HDBSCAN ì‹¤íŒ¨: ëª¨ë“  {len(noise_data)}ê°œ ë°ì´í„°ë¥¼ ë…¸ì´ì¦ˆë¡œ ì²˜ë¦¬")
        
        if len(hdbscan_clusters) == 0:
            logger.info("HDBSCANì´ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. K-Meansë¡œ ëŒ€ì²´ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            hdbscan_clusters = self._perform_kmeans_clustering(titles_data, embeddings)
            
            if not hdbscan_clusters:
                logger.warning(f"ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë°ì´í„° ê°œìˆ˜: {len(titles_data)})")
        else:
            logger.info("ğŸ“Š 2ë‹¨ê³„: K-Meansë¡œ ë…¸ì´ì¦ˆ ì¬ë¶„ë¥˜ ìˆ˜í–‰ ì¤‘...")
            hdbscan_clusters = self._recluster_noise_with_kmeans(
                noise_data, noise_embeddings, hdbscan_clusters
            )
            
            # K-Means ì¬ë¶„ë¥˜ ê²°ê³¼ ë¡œê·¸
            n_final_clusters = len(hdbscan_clusters)
            n_final_noise = len(noise_data)
            logger.info(f"âœ… K-Means ì¬ë¶„ë¥˜ ì™„ë£Œ: ì´ {n_final_clusters}ê°œ í´ëŸ¬ìŠ¤í„°, {n_final_noise}ê°œ ë…¸ì´ì¦ˆ")
            
            # ìµœì¢… í´ëŸ¬ìŠ¤í„°ë³„ ë‰´ìŠ¤ ì œëª© ë¡œê·¸
            logger.info("ğŸ“‹ ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
            for cluster_id, cluster_items in hdbscan_clusters.items():
                logger.info(f"   ğŸ“ í´ëŸ¬ìŠ¤í„° {cluster_id}: {len(cluster_items)}ê°œ ë‰´ìŠ¤")
                for i, item in enumerate(cluster_items[:3]):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                    title = item.get('cleaned_title', 'Unknown')[:50] + "..." if len(item.get('cleaned_title', '')) > 50 else item.get('cleaned_title', 'Unknown')
                    logger.info(f"      {i+1}. {title}")
                if len(cluster_items) > 3:
                    logger.info(f"      ... ì™¸ {len(cluster_items) - 3}ê°œ")
        
        logger.info(f"ğŸ‰ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(hdbscan_clusters)}ê°œ í´ëŸ¬ìŠ¤í„°, {len(noise_data)}ê°œ ë…¸ì´ì¦ˆ")
        return hdbscan_clusters, []
    
    def kmeans_only_cluster_news(self, titles_data):
        """
        K-Meansë§Œ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§ (HDBSCAN ëŒ€ì‹ )
        
        Args:
            titles_data (list): í´ëŸ¬ìŠ¤í„°ë§í•  ë‰´ìŠ¤ ë°ì´í„°
            
        Returns:
            tuple: (í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬, ë…¸ì´ì¦ˆ ë¦¬ìŠ¤íŠ¸)
        """
        titles = [item["cleaned_title"] for item in titles_data]
        n_data = len(titles_data)
        
        logger.info(f"ğŸ” K-Means í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {n_data}ê°œ ë‰´ìŠ¤")
        
        embeddings = self.embedding_model.encode(titles, normalize_embeddings=True)
        
        # K-Means í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°
        n_clusters = self.calculate_kmeans_clusters(n_data)
        
        if n_clusters < 2:
            logger.warning(f"ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters})")
            return {}, titles_data
        
        logger.info(f"ğŸ“Š K-Means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰: {n_data}ê°œ ë‰´ìŠ¤ â†’ {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
        
        # K-Means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        kmeans = KMeans(n_clusters=n_clusters, random_state=self.KMEANS_RANDOM_STATE, n_init=self.KMEANS_N_INIT)
        kmeans_labels = kmeans.fit_predict(embeddings)
        
        # í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±
        clusters = defaultdict(list)
        for label, item in zip(kmeans_labels, titles_data):
            clusters[label].append(item)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë‰´ìŠ¤ ì œëª© ë¡œê·¸
        logger.info("ğŸ“‹ K-Means í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
        for cluster_id, cluster_items in clusters.items():
            logger.info(f"   ğŸ“ í´ëŸ¬ìŠ¤í„° {cluster_id}: {len(cluster_items)}ê°œ ë‰´ìŠ¤")
            for i, item in enumerate(cluster_items[:3]):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                title = item.get('cleaned_title', 'Unknown')[:50] + "..." if len(item.get('cleaned_title', '')) > 50 else item.get('cleaned_title', 'Unknown')
                logger.info(f"      {i+1}. {title}")
            if len(cluster_items) > 3:
                logger.info(f"      ... ì™¸ {len(cluster_items) - 3}ê°œ")
        
        logger.info(f"ğŸ‰ K-Means í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
        return clusters, []
    
    def _reduce_dimensions_with_umap(self, embeddings, n_components=15, n_neighbors=25, metric='cosine'):
        """
        UMAPì„ ì‚¬ìš©í•œ ì°¨ì› ì¶•ì†Œ
        
        Args:
            embeddings: ì›ë³¸ ì„ë² ë”© (384ì°¨ì›)
            n_components: ì¶•ì†Œí•  ì°¨ì› ìˆ˜
            n_neighbors: UMAP ì´ì›ƒ ìˆ˜
            metric: ê±°ë¦¬ ë©”íŠ¸ë¦­
            
        Returns:
            ì¶•ì†Œëœ ì„ë² ë”©
        """
        try:
            import umap
            logger.info(f"ğŸ”½ UMAP ì°¨ì› ì¶•ì†Œ ì‹œì‘: {embeddings.shape[1]}D â†’ {n_components}D")
            logger.info(f"   íŒŒë¼ë¯¸í„°: n_neighbors={n_neighbors}, metric={metric}")
            
            reducer = umap.UMAP(
                n_components=n_components,
                n_neighbors=n_neighbors,
                metric=metric,
                n_jobs=1,  # ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”ë¡œ ê²½ê³  ì œê±°
                verbose=False
            )
            
            reduced_embeddings = reducer.fit_transform(embeddings)
            logger.info(f"âœ… UMAP ì°¨ì› ì¶•ì†Œ ì™„ë£Œ: {reduced_embeddings.shape}")
            
            return reduced_embeddings
            
        except ImportError:
            logger.warning("âš ï¸ UMAPì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ì›ë³¸ ì„ë² ë”© ì‚¬ìš©")
            return embeddings
        except Exception as e:
            logger.error(f"âŒ UMAP ì°¨ì› ì¶•ì†Œ ì‹¤íŒ¨: {e}. ì›ë³¸ ì„ë² ë”© ì‚¬ìš©")
            return embeddings
    
    def _tuned_hdbscan_clustering(self, embeddings, min_cluster_size=30, min_samples=12, 
                                 cluster_selection_epsilon=0.6, probability_threshold=0.2):
        """
        íŠœë‹ëœ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ (í™•ë¥  ê¸°ë°˜ í•„í„° í¬í•¨)
        
        Args:
            embeddings: ì°¨ì› ì¶•ì†Œëœ ì„ë² ë”©
            min_cluster_size: ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°
            min_samples: ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            cluster_selection_epsilon: í´ëŸ¬ìŠ¤í„° ì„ íƒ ì—¡ì‹¤ë¡ 
            probability_threshold: í™•ë¥  ì„ê³„ê°’ (ì´í•˜ ë…¸ì´ì¦ˆ ì²˜ë¦¬)
            
        Returns:
            tuple: (í´ëŸ¬ìŠ¤í„° ë¼ë²¨, í™•ë¥ , í´ëŸ¬ìŠ¤í„°ëŸ¬ ê°ì²´)
        """
        try:
            import hdbscan
            logger.info(f"ğŸ” íŠœë‹ëœ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")
            logger.info(f"   íŒŒë¼ë¯¸í„°: min_cluster_size={min_cluster_size}, min_samples={min_samples}")
            logger.info(f"   cluster_selection_epsilon={cluster_selection_epsilon}, probability_threshold={probability_threshold}")
            
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                min_samples=min_samples,
                metric="euclidean",  # UMAP ì¶œë ¥ì€ ìœ í´ë¦¬ë“œë¡œ ì²˜ë¦¬
                cluster_selection_method='eom',
                cluster_selection_epsilon=cluster_selection_epsilon,
                allow_single_cluster=False,  # ë‹¨ì¼ ëŒ€í´ëŸ¬ìŠ¤í„° ë°©ì§€
                prediction_data=True
            )
            
            cluster_labels = clusterer.fit_predict(embeddings)
            probabilities = clusterer.probabilities_
            
            # í™•ë¥  ê¸°ë°˜ í•„í„° ì ìš©
            original_labels = cluster_labels.copy()
            low_prob_mask = probabilities < probability_threshold
            cluster_labels[low_prob_mask] = -1
            
            n_original_clusters = len(set(original_labels)) - (1 if -1 in original_labels else 0)
            n_filtered_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
            n_noise = list(cluster_labels).count(-1)
            
            logger.info(f"âœ… HDBSCAN ì™„ë£Œ: {n_original_clusters}ê°œ â†’ {n_filtered_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
            logger.info(f"   ë…¸ì´ì¦ˆ: {n_noise}ê°œ (í™•ë¥  ì„ê³„ê°’ {probability_threshold} ì ìš©)")
            
            return cluster_labels, probabilities, clusterer
            
        except ImportError:
            logger.warning("âš ï¸ HDBSCANì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. K-Means ì‚¬ìš©")
            return None, None, None
        except Exception as e:
            logger.error(f"âŒ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return None, None, None
    
    def _merge_similar_clusters(self, clusters, embeddings, similarity_threshold=0.8):
        """
        ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„° ë³‘í•© (ì„¼íŠ¸ë¡œì´ë“œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)
        
        Args:
            clusters: í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬
            embeddings: ì›ë³¸ ì„ë² ë”©
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ë³‘í•©ëœ í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬
        """
        if len(clusters) <= 1:
            return clusters
        
        logger.info(f"ğŸ”— í´ëŸ¬ìŠ¤í„° ë³‘í•© ì‹œì‘: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
        logger.info(f"   ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}")
        
        # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚°
        cluster_centroids = {}
        for cluster_id, cluster_items in clusters.items():
            cluster_indices = [i for i, item in enumerate(cluster_items)]
            if cluster_indices:
                centroid = np.mean(embeddings[cluster_indices], axis=0)
                cluster_centroids[cluster_id] = centroid
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        cluster_ids = list(cluster_centroids.keys())
        centroids_matrix = np.array([cluster_centroids[cid] for cid in cluster_ids])
        similarity_matrix = cosine_similarity(centroids_matrix)
        
        # ë³‘í•©í•  í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
        merged_clusters = clusters.copy()
        merge_count = 0
        
        for i, cluster_id1 in enumerate(cluster_ids):
            if cluster_id1 not in merged_clusters:
                continue
                
            for j, cluster_id2 in enumerate(cluster_ids[i+1:], i+1):
                if cluster_id2 not in merged_clusters:
                    continue
                    
                similarity = similarity_matrix[i, j]
                if similarity >= similarity_threshold:
                    # í´ëŸ¬ìŠ¤í„° ë³‘í•©
                    merged_clusters[cluster_id1].extend(merged_clusters[cluster_id2])
                    del merged_clusters[cluster_id2]
                    merge_count += 1
                    logger.info(f"   ë³‘í•©: í´ëŸ¬ìŠ¤í„° {cluster_id1} + {cluster_id2} (ìœ ì‚¬ë„: {similarity:.3f})")
                    break  # í•œ ë²ˆì— í•˜ë‚˜ì”©ë§Œ ë³‘í•©
        
        logger.info(f"âœ… í´ëŸ¬ìŠ¤í„° ë³‘í•© ì™„ë£Œ: {len(clusters)}ê°œ â†’ {len(merged_clusters)}ê°œ ({merge_count}ë²ˆ ë³‘í•©)")
        return merged_clusters
    
    def _assign_noise_to_clusters(self, clusters, noise_data, noise_embeddings, similarity_threshold=0.75):
        """
        ë…¸ì´ì¦ˆ ë°ì´í„°ë¥¼ ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ì— í¸ìŠ¹
        
        Args:
            clusters: ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°
            noise_data: ë…¸ì´ì¦ˆ ë°ì´í„°
            noise_embeddings: ë…¸ì´ì¦ˆ ì„ë² ë”©
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            tuple: (ì—…ë°ì´íŠ¸ëœ í´ëŸ¬ìŠ¤í„°, ë‚¨ì€ ë…¸ì´ì¦ˆ)
        """
        if not noise_data or not clusters:
            return clusters, noise_data
        
        logger.info(f"ğŸ¯ ë…¸ì´ì¦ˆ í¸ìŠ¹ ì‹œì‘: {len(noise_data)}ê°œ ë…¸ì´ì¦ˆ")
        logger.info(f"   ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}")
        
        # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚°
        cluster_centroids = {}
        for cluster_id, cluster_items in clusters.items():
            if cluster_items:
                # í´ëŸ¬ìŠ¤í„° ì•„ì´í…œì˜ ì„ë² ë”© ì¸ë±ìŠ¤ ì°¾ê¸° (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •í™•í•œ ë°©ë²• í•„ìš”)
                centroid = np.mean([noise_embeddings[0]], axis=0)  # ì„ì‹œ êµ¬í˜„
                cluster_centroids[cluster_id] = centroid
        
        updated_clusters = clusters.copy()
        remaining_noise = []
        assigned_count = 0
        
        for i, noise_item in enumerate(noise_data):
            noise_embedding = noise_embeddings[i]
            best_similarity = 0
            best_cluster_id = None
            
            # ê°€ì¥ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
            for cluster_id, centroid in cluster_centroids.items():
                similarity = cosine_similarity([noise_embedding], [centroid])[0][0]
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_cluster_id = cluster_id
            
            # ì„ê³„ê°’ ì´ìƒì´ë©´ í¸ìŠ¹
            if best_similarity >= similarity_threshold:
                updated_clusters[best_cluster_id].append(noise_item)
                assigned_count += 1
            else:
                remaining_noise.append(noise_item)
        
        logger.info(f"âœ… ë…¸ì´ì¦ˆ í¸ìŠ¹ ì™„ë£Œ: {assigned_count}ê°œ í¸ìŠ¹, {len(remaining_noise)}ê°œ ë‚¨ìŒ")
        return updated_clusters, remaining_noise
    
    def _spherical_kmeans_clustering(self, noise_data, noise_embeddings, n_clusters=None):
        """
        Spherical K-Means í´ëŸ¬ìŠ¤í„°ë§ (L2 ì •ê·œí™” + ì¼ë°˜ K-Means ê·¼ì‚¬)
        
        Args:
            noise_data: ë…¸ì´ì¦ˆ ë°ì´í„°
            noise_embeddings: ë…¸ì´ì¦ˆ ì„ë² ë”©
            n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜ (Noneì´ë©´ ìë™ ê³„ì‚°)
            
        Returns:
            í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬
        """
        if not noise_data or len(noise_data) < 2:
            return {}
        
        if n_clusters is None:
            n_clusters = self.calculate_kmeans_clusters(len(noise_data))
        
        if n_clusters < 2:
            logger.warning(f"âš ï¸ ë…¸ì´ì¦ˆ ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ Spherical K-Means ìƒëµ: {len(noise_data)}ê°œ")
            return {}
        
        logger.info(f"ğŸŒ Spherical K-Means ì‹œì‘: {len(noise_data)}ê°œ ë…¸ì´ì¦ˆ â†’ {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
        
        # L2 ì •ê·œí™”
        from sklearn.preprocessing import normalize
        normalized_embeddings = normalize(noise_embeddings, norm='l2')
        
        # ì¼ë°˜ K-Means ì‚¬ìš© (MiniBatchKMeansë¡œ ì†ë„ í–¥ìƒ)
        from sklearn.cluster import MiniBatchKMeans
        kmeans = MiniBatchKMeans(
            n_clusters=n_clusters,
            random_state=self.KMEANS_RANDOM_STATE,
            n_init='auto',
            batch_size=min(100, len(noise_data))
        )
        
        kmeans_labels = kmeans.fit_predict(normalized_embeddings)
        
        # í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±
        clusters = defaultdict(list)
        for label, item in zip(kmeans_labels, noise_data):
            clusters[label].append(item)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë‰´ìŠ¤ ì œëª© ë¡œê·¸
        logger.info("ğŸ“‹ Spherical K-Means ê²°ê³¼:")
        for cluster_id, cluster_items in clusters.items():
            logger.info(f"   ğŸ“ í´ëŸ¬ìŠ¤í„° {cluster_id}: {len(cluster_items)}ê°œ ë‰´ìŠ¤")
            for i, item in enumerate(cluster_items[:3]):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                title = item.get('cleaned_title', 'Unknown')[:50] + "..." if len(item.get('cleaned_title', '')) > 50 else item.get('cleaned_title', 'Unknown')
                logger.info(f"      {i+1}. {title}")
            if len(cluster_items) > 3:
                logger.info(f"      ... ì™¸ {len(cluster_items) - 3}ê°œ")
        
        logger.info(f"âœ… Spherical K-Means ì™„ë£Œ: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
        return clusters
    
    def graph_based_cluster_news(self, titles_data):
        """
        ê·¸ë˜í”„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§: TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ + ì—°ê²°ìš”ì†Œë¡œ í† í”½ ê·¸ë£¹í™”
        
        Args:
            titles_data (list): í´ëŸ¬ìŠ¤í„°ë§í•  ë‰´ìŠ¤ ë°ì´í„°
            
        Returns:
            tuple: (í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬, ë…¸ì´ì¦ˆ ë¦¬ìŠ¤íŠ¸)
        """
        titles = [item["cleaned_title"] for item in titles_data]
        n_data = len(titles_data)
        
        logger.info(f"ğŸ”— ê·¸ë˜í”„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {n_data}ê°œ ë‰´ìŠ¤")
        
        # 1ë‹¨ê³„: ë¬¸ì n-gram TF-IDF ë²¡í„°í™” (ë„ì–´ì“°ê¸°/í˜•íƒœì†Œ ë³€ë™ì— ê°•í•¨)
        logger.info("ğŸ“Š ë¬¸ì n-gram TF-IDF ë²¡í„°í™” ì¤‘...")
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity
            import numpy as np
            
            # í•œê¸€ ì œëª©ì— ìµœì í™”ëœ TF-IDF ì„¤ì •
            vectorizer = TfidfVectorizer(
                analyzer='char',           # ë¬¸ì ë‹¨ìœ„ ë¶„ì„
                ngram_range=(3, 5),        # 3~5ê¸€ì n-gram
                min_df=3,                  # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„
                max_df=0.6,                # ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„
                sublinear_tf=True,         # ë¡œê·¸ ìŠ¤ì¼€ì¼ë§
                norm='l2'                  # L2 ì •ê·œí™”
            )
            
            tfidf_matrix = vectorizer.fit_transform(titles)
            logger.info(f"âœ… TF-IDF ë²¡í„°í™” ì™„ë£Œ: {tfidf_matrix.shape}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ TF-IDF ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            # Fallback: ë‹¨ìˆœ í‚¤ì›Œë“œ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
            return self._fallback_keyword_clustering(titles_data)
        
        # 2ë‹¨ê³„: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ë° ê°„ì„  ìƒì„±
        logger.info("ğŸ”— ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ë° ê°„ì„  ìƒì„± ì¤‘...")
        similarity_threshold = 0.22  # Ï„=0.22Â±0.05
        
        # ìƒìœ„ k-NNë§Œ ê³„ì‚° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        k_neighbors = min(15, n_data - 1)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        cosine_sim = cosine_similarity(tfidf_matrix)
        
        # ê°„ì„  ìƒì„±: ìƒìœ„ kê°œ + ì„ê³„ê°’ ì´ìƒ
        edges = []
        for i in range(n_data):
            # ìƒìœ„ kê°œ ìœ ì‚¬ë„ ì¸ë±ìŠ¤
            top_k_indices = np.argsort(cosine_sim[i])[-k_neighbors-1:-1]  # ìê¸° ìì‹  ì œì™¸
            
            for j in top_k_indices:
                if cosine_sim[i][j] >= similarity_threshold:
                    edges.append((i, j, cosine_sim[i][j]))
        
        logger.info(f"âœ… ê°„ì„  ìƒì„± ì™„ë£Œ: {len(edges)}ê°œ ê°„ì„  (ì„ê³„ê°’: {similarity_threshold})")
        
        # 3ë‹¨ê³„: ì—°ê²°ìš”ì†Œ(Connected Components) ì°¾ê¸°
        logger.info("ğŸ” ì—°ê²°ìš”ì†Œ íƒìƒ‰ ì¤‘...")
        clusters = self._find_connected_components(n_data, edges)
        
        # 4ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° í›„ì²˜ë¦¬ ë° ë¼ë²¨ë§
        logger.info("ğŸ·ï¸ í´ëŸ¬ìŠ¤í„° í›„ì²˜ë¦¬ ë° ë¼ë²¨ë§ ì¤‘...")
        processed_clusters = {}
        cluster_id = 0
        
        for component in clusters:
            if len(component) >= self.MIN_CLUSTER_NEWS_COUNT:
                cluster_items = [titles_data[idx] for idx in component]
                processed_clusters[cluster_id] = cluster_items
                cluster_id += 1
                logger.info(f"   í´ëŸ¬ìŠ¤í„° {cluster_id-1}: {len(component)}ê°œ ë‰´ìŠ¤")
        
        # 5ë‹¨ê³„: ë…¸ì´ì¦ˆ ì²˜ë¦¬
        used_indices = set()
        for cluster in clusters:
            used_indices.update(cluster)
        noise_news = [titles_data[i] for i in range(n_data) if i not in used_indices]
        
        logger.info(f"ğŸ‰ ê·¸ë˜í”„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(processed_clusters)}ê°œ í´ëŸ¬ìŠ¤í„°, {len(noise_news)}ê°œ ë…¸ì´ì¦ˆ")
        
        return processed_clusters, noise_news
    
    def _find_connected_components(self, n_nodes, edges):
        """
        ì—°ê²°ìš”ì†Œ ì°¾ê¸° (Union-Find ì•Œê³ ë¦¬ì¦˜)
        """
        parent = list(range(n_nodes))
        
        def find(x):
            if parent[x] != x:
                parent[x] = find(parent[x])
            return parent[x]
        
        def union(x, y):
            px, py = find(x), find(y)
            if px != py:
                parent[px] = py
        
        # ê°„ì„ ìœ¼ë¡œ ë…¸ë“œë“¤ ì—°ê²°
        for i, j, _ in edges:
            union(i, j)
        
        # ì—°ê²°ìš”ì†Œë³„ë¡œ ê·¸ë£¹í™”
        components = {}
        for i in range(n_nodes):
            root = find(i)
            if root not in components:
                components[root] = []
            components[root].append(i)
        
        return list(components.values())
    
    def _fallback_keyword_clustering(self, titles_data):
        """
        TF-IDF ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ í‚¤ì›Œë“œ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
        """
        logger.info("ğŸ”„ Fallback: ë‹¨ìˆœ í‚¤ì›Œë“œ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§")
        
        titles = [item["cleaned_title"] for item in titles_data]
        n_data = len(titles_data)
        
        # ë‹¨ìˆœ í‚¤ì›Œë“œ ì¶”ì¶œ (ëª…ì‚¬ë§Œ)
        news_keywords = []
        for title in titles:
            try:
                nouns = self.okt.nouns(title)
                # ê¸¸ì´ 2 ì´ìƒì¸ ëª…ì‚¬ë§Œ
                keywords = [noun for noun in nouns if len(noun) >= 2]
                news_keywords.append(keywords[:5])  # ìƒìœ„ 5ê°œë§Œ
            except:
                news_keywords.append([])
        
        # Jaccard ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
        clusters = {}
        cluster_id = 0
        used_indices = set()
        
        for i, keywords_i in enumerate(news_keywords):
            if i in used_indices or not keywords_i:
                continue
                
            similar_news = [i]
            used_indices.add(i)
            
            for j, keywords_j in enumerate(news_keywords):
                if j in used_indices or not keywords_j:
                    continue
                    
                # Jaccard ìœ ì‚¬ë„
                intersection = len(set(keywords_i) & set(keywords_j))
                union = len(set(keywords_i) | set(keywords_j))
                similarity = intersection / union if union > 0 else 0
                
                if similarity >= 0.3:  # 30% ì´ìƒ ìœ ì‚¬
                    similar_news.append(j)
                    used_indices.add(j)
            
            if len(similar_news) >= self.MIN_CLUSTER_NEWS_COUNT:
                clusters[cluster_id] = [titles_data[idx] for idx in similar_news]
                cluster_id += 1
        
        noise_news = [titles_data[i] for i in range(n_data) if i not in used_indices]
        
        logger.info(f"âœ… Fallback í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°, {len(noise_news)}ê°œ ë…¸ì´ì¦ˆ")
        
        return clusters, noise_news
    
    def frequency_based_cluster_news(self, titles_data):
        """
        ë¹ˆë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§: TF-IDF í‚¤ì›Œë“œ ìœ ì‚¬ë„ë¡œ ë‰´ìŠ¤ ê·¸ë£¹í™”
        
        Args:
            titles_data (list): í´ëŸ¬ìŠ¤í„°ë§í•  ë‰´ìŠ¤ ë°ì´í„°
            
        Returns:
            tuple: (í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬, ë…¸ì´ì¦ˆ ë¦¬ìŠ¤íŠ¸)
        """
        titles = [item["cleaned_title"] for item in titles_data]
        n_data = len(titles_data)
        
        logger.info(f"ğŸ”¢ ë¹ˆë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {n_data}ê°œ ë‰´ìŠ¤")
        
        # 1ë‹¨ê³„: TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ
        logger.info("ğŸ“Š TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        all_keywords = []
        news_keywords = []
        
        for i, title in enumerate(titles):
            keywords = self.extract_keywords_with_konlpy_tfidf([title], topn=5)
            news_keywords.append(keywords)
            all_keywords.extend(keywords)
        
        logger.info(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: í‰ê·  {sum(len(kw) for kw in news_keywords) / len(news_keywords):.1f}ê°œ/ë‰´ìŠ¤")
        
        # 2ë‹¨ê³„: í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
        logger.info("ğŸ”— í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
        clusters = {}
        cluster_id = 0
        used_indices = set()
        
        # ê° ë‰´ìŠ¤ì— ëŒ€í•´ ìœ ì‚¬í•œ ë‰´ìŠ¤ ì°¾ê¸°
        for i, keywords_i in enumerate(news_keywords):
            if i in used_indices:
                continue
                
            # í˜„ì¬ ë‰´ìŠ¤ì™€ ìœ ì‚¬í•œ ë‰´ìŠ¤ë“¤ ì°¾ê¸°
            similar_news = [i]
            used_indices.add(i)
            
            for j, keywords_j in enumerate(news_keywords):
                if j in used_indices:
                    continue
                    
                # í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„)
                if keywords_i and keywords_j:
                    intersection = len(set(keywords_i) & set(keywords_j))
                    union = len(set(keywords_i) | set(keywords_j))
                    similarity = intersection / union if union > 0 else 0
                    
                    # ìœ ì‚¬ë„ ì„ê³„ê°’ (30% ì´ìƒ)
                    if similarity >= 0.3:
                        similar_news.append(j)
                        used_indices.add(j)
            
            # í´ëŸ¬ìŠ¤í„° í¬ê¸°ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ ì €ì¥
            if len(similar_news) >= self.MIN_CLUSTER_NEWS_COUNT:
                clusters[cluster_id] = [titles_data[idx] for idx in similar_news]
                cluster_id += 1
                logger.info(f"   í´ëŸ¬ìŠ¤í„° {cluster_id-1}: {len(similar_news)}ê°œ ë‰´ìŠ¤ (í‚¤ì›Œë“œ: {keywords_i[:3]})")
        
        # 3ë‹¨ê³„: ë…¸ì´ì¦ˆ ì²˜ë¦¬
        noise_news = [titles_data[i] for i in range(n_data) if i not in used_indices]
        
        logger.info(f"ğŸ‰ ë¹ˆë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°, {len(noise_news)}ê°œ ë…¸ì´ì¦ˆ")
        
        return clusters, noise_news
    
    def advanced_cluster_news(self, titles_data, embeddings=None):
        """
        ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸: UMAP â†’ íŠœë‹ëœ HDBSCAN â†’ ë³‘í•© â†’ ë…¸ì´ì¦ˆ í¸ìŠ¹ â†’ Spherical K-Means
        
        Args:
            titles_data (list): í´ëŸ¬ìŠ¤í„°ë§í•  ë‰´ìŠ¤ ë°ì´í„°
            embeddings: ë¯¸ë¦¬ ìƒì„±ëœ ì„ë² ë”© (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
            
        Returns:
            tuple: (í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬, ë…¸ì´ì¦ˆ ë¦¬ìŠ¤íŠ¸)
        """
        titles = [item["cleaned_title"] for item in titles_data]
        n_data = len(titles_data)
        
        logger.info(f"ğŸš€ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {n_data}ê°œ ë‰´ìŠ¤")
        
        # 1ë‹¨ê³„: ì„ë² ë”© ìƒì„± (ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        if embeddings is None:
            logger.info("ğŸ¤– ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = self.embedding_model.encode(titles, normalize_embeddings=True)
            logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape}")
        else:
            logger.info(f"â™»ï¸ ê¸°ì¡´ ì„ë² ë”© ì¬ì‚¬ìš©: {embeddings.shape}")
        
        # 2ë‹¨ê³„: UMAP ì°¨ì› ì¶•ì†Œ
        reduced_embeddings = self._reduce_dimensions_with_umap(
            embeddings, 
            n_components=15, 
            n_neighbors=25, 
            metric='cosine'
        )
        
        # 3ë‹¨ê³„: íŠœë‹ëœ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
        cluster_labels, probabilities, clusterer = self._tuned_hdbscan_clustering(
            reduced_embeddings,
            min_cluster_size=30,
            min_samples=12,
            cluster_selection_epsilon=0.6,
            probability_threshold=0.2
        )
        
        if cluster_labels is None:
            logger.warning("âš ï¸ HDBSCAN ì‹¤íŒ¨. K-Meansë¡œ ëŒ€ì²´")
            return self.kmeans_only_cluster_news(titles_data)
        
        # 4ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±
        clusters = defaultdict(list)
        noise_data = []
        noise_embeddings = []
        
        for label, item, embedding in zip(cluster_labels, titles_data, embeddings):
            if label == -1:
                noise_data.append(item)
                noise_embeddings.append(embedding)
            else:
                clusters[label].append(item)
        
        logger.info(f"ğŸ“Š HDBSCAN ê²°ê³¼: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°, {len(noise_data)}ê°œ ë…¸ì´ì¦ˆ")
        
        # 5ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° ë³‘í•©
        if len(clusters) > 1:
            clusters = self._merge_similar_clusters(clusters, embeddings, similarity_threshold=0.8)
        
        # 6ë‹¨ê³„: ë…¸ì´ì¦ˆ í¸ìŠ¹
        if noise_data and clusters:
            clusters, remaining_noise_data = self._assign_noise_to_clusters(
                clusters, noise_data, noise_embeddings, similarity_threshold=0.75
            )
            noise_data = remaining_noise_data
        
        # 7ë‹¨ê³„: ë‚¨ì€ ë…¸ì´ì¦ˆë¥¼ Spherical K-Meansë¡œ ì²˜ë¦¬
        final_noise = []
        if noise_data:
            spherical_clusters = self._spherical_kmeans_clustering(noise_data, noise_embeddings)
            
            # Spherical K-Means ê²°ê³¼ë¥¼ ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ì— ì¶”ê°€
            max_cluster_id = max(clusters.keys()) if clusters else -1
            for label, cluster_items in spherical_clusters.items():
                new_cluster_id = max_cluster_id + 1 + label
                clusters[new_cluster_id] = cluster_items
            
            logger.info(f"ğŸ”„ Spherical K-Meansë¡œ {len(spherical_clusters)}ê°œ ì¶”ê°€ í´ëŸ¬ìŠ¤í„° ìƒì„±")
        
        logger.info(f"ğŸ‰ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: ì´ {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°, {len(final_noise)}ê°œ ìµœì¢… ë…¸ì´ì¦ˆ")
        
        # ìµœì¢… í´ëŸ¬ìŠ¤í„°ë³„ ë‰´ìŠ¤ ì œëª© ë¡œê·¸
        logger.info("ğŸ“‹ ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
        for cluster_id, cluster_items in clusters.items():
            logger.info(f"   ğŸ“ í´ëŸ¬ìŠ¤í„° {cluster_id}: {len(cluster_items)}ê°œ ë‰´ìŠ¤")
            for i, item in enumerate(cluster_items[:3]):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                title = item.get('cleaned_title', 'Unknown')[:50] + "..." if len(item.get('cleaned_title', '')) > 50 else item.get('cleaned_title', 'Unknown')
                logger.info(f"      {i+1}. {title}")
            if len(cluster_items) > 3:
                logger.info(f"      ... ì™¸ {len(cluster_items) - 3}ê°œ")
        
        return clusters, final_noise
    
    def extract_keywords_with_keybert(self, text, top_k=3):
        """
        KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            text (str): í…ìŠ¤íŠ¸
            top_k (int): ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            list: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            keybert_keywords = self.kw_model.extract_keywords(
                text, 
                keyphrase_ngram_range=(self.KEYBERT_NGRAM_MIN, self.KEYBERT_NGRAM_MAX),
                stop_words=list(self.stopwords),
                top_n=top_k,
                use_mmr=True,
                diversity=self.KEYBERT_DIVERSITY
            )
            
            return [kw for kw, score in keybert_keywords]
        except ValueError as e:
            logger.warning(f"KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ (ë°ì´í„° ë¶€ì¡± ë˜ëŠ” í˜•ì‹ ì˜¤ë¥˜): {e}")
            return []
        except Exception as e:
            logger.error(f"KeyBERT ê³„ì‚° ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return []

    def generate_cluster_labels(self, clusters):
        """
        í´ëŸ¬ìŠ¤í„°ë³„ í‚¤ì›Œë“œ ë¼ë²¨ ìƒì„± (ì›ë³¸ ë³µì›)
        
        Args:
            clusters (dict): í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            dict: í´ëŸ¬ìŠ¤í„°ë³„ ë¼ë²¨ ì •ë³´ (major_category, keywords í¬í•¨)
        """
        cluster_labels = {}
        
        for cluster_id, news_list in clusters.items():
            titles = [item["cleaned_title"] for item in news_list]
            combined_text = " ".join(titles)
            
            tfidf_keywords = self.extract_keywords_with_konlpy_tfidf(
                titles, topn=self.TOP_KEYWORDS_COUNT
            )
            
            keybert_keywords = self.extract_keywords_with_keybert(
                combined_text, top_k=self.KEYBERT_TOP_K
            )
            
            combined_keywords = tfidf_keywords.copy()
            for kw in keybert_keywords:
                if kw not in combined_keywords:
                    combined_keywords.append(kw)
            
            major_category = self.determine_major_category(combined_keywords, titles)
            
            cluster_labels[cluster_id] = {
                "major_category": major_category,
                "keywords": combined_keywords[:self.TOP_KEYWORDS_COUNT],
                "tfidf_keywords": tfidf_keywords,
                "keybert_keywords": keybert_keywords
            }
        
        return cluster_labels
    
    def determine_major_category(self, keywords, titles):
        """
        í‚¤ì›Œë“œì™€ ì œëª©ì„ ë¶„ì„í•˜ì—¬ ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ê²°ì •
        
        Args:
            keywords (list): í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            titles (list): ì œëª© ë¦¬ìŠ¤íŠ¸
            
        Returns:
            str: ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ì´ë¦„
        """
        category_mapping = {
            "ì •ì¹˜": ["ëŒ€í†µë ¹", "ì •ë¶€", "êµ­íšŒ", "ì •ì¹˜", "ì„ ê±°", "ì—¬ì•¼", "ì •ì±…", "êµ­ì •", "ì •ë‹¹"],
            "ê²½ì œ": ["ê²½ì œ", "íˆ¬ì", "ê¸°ì—…", "ê¸ˆìœµ", "ì£¼ì‹", "ì‹œì¥", "ìˆ˜ì¶œ", "ìˆ˜ì…", "GDP", "ê¸ˆë¦¬"],
            "ì‚¬íšŒ": ["ì‚¬íšŒ", "êµìœ¡", "ë³µì§€", "ë³´ê±´", "í™˜ê²½", "êµí†µ", "ì£¼íƒ", "ë…¸ë™", "ê³ ìš©"],
            "êµ­ì œ": ["êµ­ì œ", "ì™¸êµ", "ë¯¸êµ­", "ì¤‘êµ­", "ì¼ë³¸", "ëŸ¬ì‹œì•„", "ìœ ëŸ½", "íŠ¸ëŸ¼í”„", "í‘¸í‹´"],
            "ë²•ë¬´": ["ë²•ë¬´", "ë²•ì›", "ê²€ì°°", "ê²½ì°°", "ì¬íŒ", "í˜•ì‚¬", "ë¯¼ì‚¬", "ë²•ë¥ ", "ì‚¬ë²•"],
            "ë¬¸í™”": ["ë¬¸í™”", "ì˜ˆìˆ ", "ìŠ¤í¬ì¸ ", "ì—°ì˜ˆ", "ì˜í™”", "ìŒì•…", "ì¶•ì œ", "ì „ì‹œ"],
            "ê¸°ìˆ ": ["ê¸°ìˆ ", "AI", "ì¸ê³µì§€ëŠ¥", "ë””ì§€í„¸", "ìŠ¤ë§ˆíŠ¸", "IT", "ì†Œí”„íŠ¸ì›¨ì–´", "í•˜ë“œì›¨ì–´"],
            "êµìœ¡": ["êµìœ¡", "ëŒ€í•™", "í•™êµ", "í•™ìƒ", "êµìˆ˜", "ì—°êµ¬", "í•™ìˆ ", "ì…ì‹œ"],
            "ì˜ë£Œ": ["ì˜ë£Œ", "ë³‘ì›", "ì˜ì‚¬", "ì¹˜ë£Œ", "ê±´ê°•", "ì§ˆë³‘", "ì˜ì•½", "ë³´ê±´"],
            "í™˜ê²½": ["í™˜ê²½", "ê¸°í›„", "ì—ë„ˆì§€", "ì¬ìƒ", "ì¹œí™˜ê²½", "ëŒ€ê¸°", "ìˆ˜ì§ˆ", "íê¸°ë¬¼"]
        }
        
        category_scores = {}
        all_texts = titles + [f"{' '.join(keywords)}"]
        tfidf_keywords = self.extract_keywords_with_konlpy_tfidf(all_texts, topn=self.TFIDF_CATEGORY_KEYWORDS)
        
        for category, words in category_mapping.items():
            score = 0
            for word in words:
                if word in tfidf_keywords:
                    try:
                        rank = tfidf_keywords.index(word)
                        weight = 1.0 / (rank + 1)
                        score += weight
                    except ValueError:
                        pass
            category_scores[category] = score
        
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            if category_scores[best_category] > 0:
                return best_category
        
        return tfidf_keywords[0] if tfidf_keywords else "ê¸°íƒ€"
    
    def normalize_keyword(self, keyword):
        """
        í‚¤ì›Œë“œì˜ ê³µë°±ì„ í•˜ì´í”ˆìœ¼ë¡œ ë³€ê²½ (ë…¸ë“œ ID ë§¤ì¹­ìš©)
        
        Args:
            keyword (str): í‚¤ì›Œë“œ
            
        Returns:
            str: ì •ê·œí™”ëœ í‚¤ì›Œë“œ
        """
        return keyword.replace(" ", "-")
    
    def _format_news_item(self, news):
        """
        ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            news (dict): ë‰´ìŠ¤ ë°ì´í„°
            
        Returns:
            dict: {"title": ..., "link": ...} í˜•ì‹ì˜ ë‰´ìŠ¤ ë°ì´í„°
        """
        return {
            "title": news['cleaned_title'], 
            "link": news["original"].get("link", "")
        }
    
    def _filter_and_sort_universities(self, univ_news):
        """
        ëŒ€í•™êµ ë°ì´í„° í•„í„°ë§ ë° ì •ë ¬
        
        Args:
            univ_news (dict): ëŒ€í•™êµë³„ ë‰´ìŠ¤
            
        Returns:
            dict: í•„í„°ë§ ë° ì •ë ¬ëœ ëŒ€í•™êµ ë°ì´í„°
        """
        filtered_universities = {
            university_name: news_list 
            for university_name, news_list in univ_news.items() 
            if len(news_list) >= self.MIN_UNIV_NEWS_COUNT
        }
        
        sorted_items = sorted(filtered_universities.items(), key=lambda x: len(x[1]), reverse=True)
        return dict(sorted_items[:self.MAX_UNIV_DISPLAY])
    
    def _assign_news_to_minor_categories(self, news_list, keywords):
        """
        ë‰´ìŠ¤ë¥¼ ì¤‘ë¶„ë¥˜ í‚¤ì›Œë“œì— í• ë‹¹ (ê° ë‰´ìŠ¤ëŠ” í•˜ë‚˜ì˜ ì¤‘ë¶„ë¥˜ì—ë§Œ í• ë‹¹)
        
        Args:
            news_list (list): í´ëŸ¬ìŠ¤í„° ë‚´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            keywords (list): í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            dict: í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ë”•ì…”ë„ˆë¦¬
        """
        minor_category_news = {keyword: [] for keyword in keywords[:self.TOP_KEYWORDS_COUNT]}
        assigned_news = set()
        
        for keyword in keywords[:self.TOP_KEYWORDS_COUNT]:
            for news in news_list:
                news_link = news["original"].get("link", "")
                
                if news_link in assigned_news:
                    continue
                
                if keyword in news['cleaned_title']:
                    minor_category_news[keyword].append(news)
                    assigned_news.add(news_link)
        
        return minor_category_news
    
    def _build_minor_categories_data(self, minor_category_news):
        """
        ì¤‘ë¶„ë¥˜ ë°ì´í„° êµ¬ì¡° ìƒì„±
        
        Args:
            minor_category_news (dict): í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            list: ì¤‘ë¶„ë¥˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        minor_categories_data = []
        
        for minor_cat, minor_news in minor_category_news.items():
            if len(minor_news) < self.MIN_MINOR_NEWS_COUNT:
                continue
            
            minor_categories_data.append({
                "name": minor_cat,
                "news_count": len(minor_news),
                "news": [self._format_news_item(news) for news in minor_news]
            })
        
        return minor_categories_data
    
    def _extract_cluster_info(self, cluster_info, cluster_id):
        """
        í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì •ë³´ì—ì„œ ì¹´í…Œê³ ë¦¬ì™€ í‚¤ì›Œë“œ ì¶”ì¶œ (ì›ë³¸ ë³µì›)
        
        Args:
            cluster_info: í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì •ë³´ (dict ë˜ëŠ” list)
            cluster_id (int): í´ëŸ¬ìŠ¤í„° ID
            
        Returns:
            tuple: (ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬, í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸)
        """
        if isinstance(cluster_info, dict):
            major_category = cluster_info.get("major_category", f"í´ëŸ¬ìŠ¤í„° {cluster_id}")
            keywords = cluster_info.get("keywords", [])
            return major_category, keywords
        
        major_category = f"í´ëŸ¬ìŠ¤í„° {cluster_id}"
        keywords = cluster_info if isinstance(cluster_info, list) else []
        return major_category, keywords
    
    def _build_university_data(self, univ_news):
        """
        ëŒ€í•™êµ ë°ì´í„° êµ¬ì¡° ìƒì„±
        
        Args:
            univ_news (dict): ëŒ€í•™êµë³„ ë‰´ìŠ¤
            
        Returns:
            list: ëŒ€í•™êµ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        if not univ_news:
            return []
        
        sorted_universities = self._filter_and_sort_universities(univ_news)
        universities = []
        
        for university_name, news_list in sorted_universities.items():
            universities.append({
                "name": university_name,
                "news_count": len(news_list),
                "news": [self._format_news_item(news) for news in news_list]
            })
        
        return universities
    
    def _build_cluster_data(self, clusters, cluster_labels):
        """
        í´ëŸ¬ìŠ¤í„° ë°ì´í„° êµ¬ì¡° ìƒì„±
        
        Args:
            clusters (dict): í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬
            cluster_labels (dict): í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì •ë³´
            
        Returns:
            list: í´ëŸ¬ìŠ¤í„° ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        clusters_data = []
        
        for cluster_id, news_list in clusters.items():
            if len(news_list) < self.MIN_CLUSTER_NEWS_COUNT:
                continue
            
            cluster_info = cluster_labels.get(cluster_id, {})
            major_category, keywords = self._extract_cluster_info(cluster_info, cluster_id)
            
            minor_category_news = self._assign_news_to_minor_categories(news_list, keywords)
            minor_categories_data = self._build_minor_categories_data(minor_category_news)
            
            clusters_data.append({
                "cluster_id": cluster_id,
                "major_category": major_category,
                "news_count": len(news_list),
                "minor_categories": minor_categories_data
            })
        
        return clusters_data
    
    def _convert_to_major_keyword_format(self, universities, clusters):
        """
        ëŒ€í•™êµì™€ í´ëŸ¬ìŠ¤í„° ë°ì´í„°ë¥¼ majorKeyword í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            universities (list): ëŒ€í•™êµ ë°ì´í„°
            clusters (list): í´ëŸ¬ìŠ¤í„° ë°ì´í„°
            
        Returns:
            list: ë³€í™˜ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        converted_data = []
        
        if universities:
            univ_middle_keywords = [
                {
                    "middleKeyword": self.normalize_keyword(univ['name']),
                    "relatedNews": univ['news']
                }
                for univ in universities
            ]
            
            if len(univ_middle_keywords) >= self.MIN_MIDDLE_KEYWORDS_COUNT:
                converted_data.append({
                    "majorKeyword": self.normalize_keyword("ëŒ€í•™êµ"),
                    "middleKeywords": univ_middle_keywords,
                    "otherNews": []
                })
        
        for cluster in clusters:
            cluster_middle_keywords = [
                {
                    "middleKeyword": self.normalize_keyword(minor_cat['name']),
                    "relatedNews": minor_cat['news']
                }
                for minor_cat in cluster['minor_categories']
            ]
            
            if len(cluster_middle_keywords) < self.MIN_MIDDLE_KEYWORDS_COUNT:
                continue
            
            converted_data.append({
                "majorKeyword": self.normalize_keyword(cluster['major_category']),
                "middleKeywords": cluster_middle_keywords,
                "otherNews": []
            })
        
        return converted_data
    
    def create_frontend_data(self, univ_news, clusters, cluster_labels, noise_news):
        """
        í”„ë¡ íŠ¸ì—”ë“œìš© ë°ì´í„° êµ¬ì¡° ìƒì„±
        
        Args:
            univ_news (dict): ëŒ€í•™êµë³„ ë‰´ìŠ¤
            clusters (dict): í´ëŸ¬ìŠ¤í„° ë”•ì…”ë„ˆë¦¬
            cluster_labels (dict): í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì •ë³´
            noise_news (list): ë…¸ì´ì¦ˆ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼
        """
        universities = self._build_university_data(univ_news)
        clusters_data = self._build_cluster_data(clusters, cluster_labels)
        return self._convert_to_major_keyword_format(universities, clusters_data)
    
    def analyze_from_db(self, news_data, embeddings=None, clustering_method=None):
        """
        ë‰´ìŠ¤ ì œëª© ë¶„ì„ íŒŒì´í”„ë¼ì¸
        
        ì „ì²˜ë¦¬, ëŒ€í•™êµ ë¶„ë¥˜, í´ëŸ¬ìŠ¤í„°ë§, í‚¤ì›Œë“œ ì¶”ì¶œì„ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            news_data (list): ì›ë³¸ ë‰´ìŠ¤ ë°ì´í„°
            embeddings: ë¯¸ë¦¬ ìƒì„±ëœ ì„ë² ë”© (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
            clustering_method (str): í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²• ('graph_based', 'frequency_based', 'advanced')
                                     Noneì´ë©´ self.CLUSTERING_METHOD ì‚¬ìš©
            
        Returns:
            list: í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼
        """
        processed_data = self.preprocess_titles(news_data)
        
        if len(processed_data) < self.MIN_NEWS_COUNT:
            return None
        
        university_news, other_news = self.split_news_by_uni_name(processed_data)
        
        if other_news:
            # í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²• ì„ íƒ (íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            if clustering_method is None:
                clustering_method = self.CLUSTERING_METHOD
            
            if clustering_method == 'graph_based':
                logger.info("ğŸ”— ê·¸ë˜í”„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‚¬ìš© (TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ + ì—°ê²°ìš”ì†Œ)")
                clusters, noise_news = self.graph_based_cluster_news(other_news)
            elif clustering_method == 'frequency_based':
                logger.info("ğŸ”¢ ë¹ˆë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‚¬ìš© (TF-IDF í‚¤ì›Œë“œ ìœ ì‚¬ë„)")
                clusters, noise_news = self.frequency_based_cluster_news(other_news)
            elif clustering_method == 'advanced':
                logger.info("ğŸš€ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ì‚¬ìš© (UMAP + HDBSCAN + K-Means)")
                clusters, noise_news = self.advanced_cluster_news(other_news, embeddings)
            else:
                logger.info("ğŸ”¢ ê¸°ë³¸ ë¹ˆë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‚¬ìš©")
                clusters, noise_news = self.frequency_based_cluster_news(other_news)
        else:
            clusters, noise_news = {}, []
        
        cluster_labels = self.generate_cluster_labels(clusters)
        
        frontend_data = self.create_frontend_data(university_news, clusters, cluster_labels, noise_news)
        
        return frontend_data

