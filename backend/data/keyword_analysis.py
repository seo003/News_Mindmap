import os
from gensim import corpora
from gensim.models import LdaModel, FastText
from sklearn.cluster import KMeans
import numpy as np
from config.config import FASTTEXT_MODEL_PATH, EXCLUDED_UNIVERSITY_PATH
import re
from collections import defaultdict

def load_excluded_universities(filepath):
    if not os.path.exists(filepath):
        return set()
    with open(filepath, 'r', encoding='utf-8') as f:
        return set(line.strip() for line in f if line.strip())

def get_topic_vector(w2v_model, keywords):
    vectors = [w2v_model.wv[word] for word in keywords if word in w2v_model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)

def extract_university_keywords(tokenized_titles):
    uni_keywords = defaultdict(list)
    other_titles = []

    # ì œì™¸í•  ëŒ€í•™êµ ìœ ì‚¬ í‚¤ì›Œë“œ ë¶ˆëŸ¬ì˜¤ê¸°
    excluded_unis = load_excluded_universities(EXCLUDED_UNIVERSITY_PATH)

    for tokens in tokenized_titles:
        unis = [word for word in tokens if re.match(r'.+ëŒ€$', word) and word not in excluded_unis]
        if unis:
            for uni in unis:
                related_keywords = [t for t in tokens if t != uni]
                uni_keywords[uni].extend(related_keywords)
        else:
            other_titles.append(tokens)

    return uni_keywords, other_titles


def analyze_keywords(titles_with_links, tokenized_titles):
    result = defaultdict(lambda: {"ì†Œë¶„ë¥˜": set(), "ë‰´ìŠ¤": []})
    uni_pattern = re.compile(r".+ëŒ€$")

    remaining_tokens = []
    remaining_title_info = []

    for idx, (item, tokens) in enumerate(zip(titles_with_links, tokenized_titles)):
        title = item["title"]
        link = item["link"]

        uni_kw = next((kw for kw in tokens if uni_pattern.match(kw)), None)

        if uni_kw:
            main_kw = uni_kw
        elif tokens:
            main_kw = tokens[0]
        else:
            continue  # í‚¤ì›Œë“œ ì—†ëŠ” ë‰´ìŠ¤ëŠ” ê±´ë„ˆëœ€

        sub_keywords = [kw for kw in tokens if kw != main_kw]

        result[main_kw]["ì†Œë¶„ë¥˜"].update(sub_keywords)
        result[main_kw]["ë‰´ìŠ¤"].append({"title": title, "link": link})

        # ëŒ€í•™êµ í‚¤ì›Œë“œ ì—†ì„ ê²½ìš° LDA ìš©ìœ¼ë¡œ ë”°ë¡œ ì €ì¥
        if not uni_kw:
            remaining_tokens.append(tokens)
            remaining_title_info.append({"title": title, "link": link, "tokens": tokens})

    # === LDA + KMeans ë¶„ì„ ===
    if remaining_tokens:
        dictionary = corpora.Dictionary(remaining_tokens)
        corpus = [dictionary.doc2bow(text) for text in remaining_tokens]
        lda_model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)

        topics = []
        for topic_id in range(lda_model.num_topics):
            topic = [word for word, _ in lda_model.show_topic(topic_id, topn=5)]
            if topic:
                topics.append(topic)

        w2v_model = FastText.load(FASTTEXT_MODEL_PATH)
        topic_vectors = [get_topic_vector(w2v_model, topic) for topic in topics]
        kmeans = KMeans(n_clusters=3, random_state=42)
        labels = kmeans.fit_predict(topic_vectors)

        clustered = defaultdict(list)
        for i, label in enumerate(labels):
            main = topics[i][0]
            subs = topics[i][1:]
            clustered[main].extend(subs)

        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë“±ë¡
        for main, subs in clustered.items():
            result[main]["ì†Œë¶„ë¥˜"].update(subs)

        # ë‰´ìŠ¤ ë¶„ë°°
        for item in remaining_title_info:
            title, link, tokens = item["title"], item["link"], item["tokens"]
            matched_main = None

            for main in clustered:
                if any(tok in clustered[main] or tok == main for tok in tokens):
                    matched_main = main
                    break

            if matched_main:
                result[matched_main]["ë‰´ìŠ¤"].append({"title": title, "link": link})

    # set â†’ list ë³€í™˜
    for val in result.values():
        val["ì†Œë¶„ë¥˜"] = list(val["ì†Œë¶„ë¥˜"])

    for main_category, data in result.items():
        print(f"\nğŸ“Œ ëŒ€ë¶„ë¥˜: {main_category}")
        print(f"   â”” ì†Œë¶„ë¥˜: {', '.join(data['ì†Œë¶„ë¥˜'])}")
        print("   â”” ê´€ë ¨ ë‰´ìŠ¤:")
        for news in data["ë‰´ìŠ¤"]:
            print(f"      - ğŸ“° {news['title']}")
            print(f"        ğŸ”— {news['link']}")
    return result